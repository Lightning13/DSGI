# Large-sample inference

```{r, echo=FALSE}
knitr::opts_chunk$set(cache = T)
```

Up to now, we've answered every question about statistical uncertainty by running a Monte Carlo simulation, of which the bootstrap is a special case.  This depends, obviously, on having a pretty fast computer, so that we can `do()` things repeatedly and not wait too long.  So you might ask: what on earth did people do before there were computers fast enough to make this possible?  How did they ever measure their statistical uncertainty?  

The answer is: people did __math.__  Lots and lots of math, spanning at least three centuries (18th to 20th).  And in this lesson, we're going to summarize what people learned from all that math.  


#### Did someone say "math"? {-}

If you enjoy math, feel free to skip this section.  But if you don't enjoy math, you might have some concerns.  Let me try to address those concerns head-on, with the following FAQ.

(1) __"Is this math hard?"__  Some of it is accessible to anyone with a high-school math education, and that's the part that gets taught in a high-school AP Statistics course.  But yes, [certain _other_ parts](https://www.cs.toronto.edu/~yuvalf/CLT.pdf) of this [math](http://cameron.econ.ucdavis.edu/e240a/asymptotic.pdf) actually are pretty hard.  And if you don't dig into this math, there will _always_ be parts of the high-school version that seem [mystifying.](https://shoichimidorikawa.github.io/Lec/ProbDistr/t-e.pdf)  I'd say that overall, this math is harder than a college course on differential equations, but not as hard as, for example, a PhD course on quantum mechanics.^[For those that know this math, I'm thinking of, for example, the proof of the Central Limit Theorem involving characteristic functions, or the proof of the asymptotic normality of the OLS estimator.]


(2) __"Holy smokes.  That sounds terrifying.  Am I going to have to learn do this math myself?"__ Emphatically not, unless you decide to take your studies a lot further than these lessons can carry you.  

(3) __"Whew.  OK, so am I going to have to _understand_ this math?"__  It depends what you mean by "understand."   I _will not_ ask you to actually stare at mathematical derivations and understand what's going on.  I _will_ summarize the practical consequences of this math and ask that you understand them.  

(4) __"Do I even need this math to do statistical inference?"__  Usually not, at least for the problems we consider in this book.  Remember, not every data-science situation you'll encounter even requires statistical inference.  (See "[The truth about statistical uncertainty].") And when statistical inference _is_ required, you typically don't need any math at all---just what we've learned so far (mainly the bootstrap).    

(5) __"Seriously?  So you're telling me we're going to talk about some hard math from a long time ago, rendered largely unnecessary by computers.  Why am I still here?"__  Let me congratulate you on your keen sense of the opportunity cost of your time.  (I wish you were in charge of most meetings I attend.)  Yours is such an excellent question that it deserves a much longer answer.  


#### Why bother? {-}

There's one overwhelmingly important reason why it's important, even for beginners, to understand this historically older, more mathematical approach to statistical inference: _because a lot of people out there still use it._  

Why, you ask, do they still use this older approach?  (I mean, haven't they read my FAQ?!)  I suppose that's an interesting question, but it's a question for another day.  The fact is, folks _do_ use this older approach.  And when they communicate their results to you, using terms like "z test" or "t interval," they'll expect you to understand them.  As a basic matter of statistical literacy, it is important that you do.

And you will!  If you understood the lesson on "[The bootstrap]", these folks describing their "z test" or "t interval" won't be talking about anything fundamentally unfamiliar to you, despite their exotic vocabulary.  In fact, you stand a decent chance of understanding what they're saying better than they understand it themselves.  It does take a bit of time and effort to learn a few new terms for a few old ideas (hence this lesson), but at the end of the day, everyone's just talking about [Sampling distributions].  

To convince you of this, let me show you the following table.  In the left column are a bunch of phrases associated with the older, more mathematical way of approaching statistical inference.  In the right column are some corresponding translations.  If you understand what the right column is referring to, you're going to find this lesson a breeze.  It'll be just like memorizing how to say "please," "thank you," and "where's the bathroom?" in the local language of a foreign country you're about to visit.  


<br>

Table: (\#tab:translation-table) How to translate between two different approaches to statistical inference.

| If someone uses a...  | That's basically the same thing as...|
|:-------------------|:--------------------------------------|
| one-sample t-test (or "t inverval")  | Bootstrapping a sample mean, making a confidence interval, and possibly checking whether that confidence interval includes some specific value (like 0).        |
| one-sample test of proportion (or "z interval")  | Bootstrapping a proportion, making a confidence interval, and possibly checking whether that confidence interval includes some specific value (like 0.5).                                     |
| Two-sample t-test (or z-test)       | Bootstrapping the difference in means between two groups (`diffmean`), making a confidence interval for the difference, and possibly checking whether that confidence interval contains zero.    |
| Two-sample test of proportions | Bootstrapping the difference in proportions between two groups (`diffprop`), making a confidence interval for the difference, and possibly checking whether that confidence interval contains 0. |
| Regression summary table       | Bootstrapping a regression model (`lm`), making confidence intervals for the regression coefficients, and possibly checking whether those confidence intervals contain zero.    |

<br>

The rest of this lesson is about understanding Table \@ref(tab:translation-table).  

Let me also add four other reasons why you'd actually _want_ to understand this table, quite apart from the practical need to understand what other people are talking about when they use the terms in the left column.  

(1) __It's a time-saver.__  Hey, life is short.  I don't know about you, but sometimes I get bored waiting for my computer to `do()` something 10,000 times in order to produce a confidence interval.  It turns out that, if you understand the correspondences in the table above, you can get a perfectly fine confidence interval in no time at all, without actually bootstrapping.  This is pretty useful, especially if you're just exploring a data set and want a quick answer without having to twiddle your thumbs while R is thinking.  

(2) __It works on bigger data sets.__  A related problem is that for some very large data sets, bootstrapping might take _so_ much time that it's no longer just a question of boredom, but of whether you'll get an answer _at all_ within some reasonable time frame.  

(3) __It offers insight.__  To give one example: the math behind Table \@ref(tab:translation-table) helps you understand a very fundamental relationship between sample size and statistical uncertainty, called _de Moivre's equation_.  Knowing this relationship protects you against all kinds of unfortunate logical errors, and we'll talk about some of these below.  

(4) __It prepares you to do more.__  We've already talked about how [Bootstrapping usually, but not always, works].  If you decide to pursue further studies---in statistics, machine learning, computer science, economics, or any number of fields---eventually you will encounter the limits of the simple bootstrapping approach, and you'll need to rely on some math (or, at the very least, on the results of some other people's math).   This lesson will give you the proper foundation to understand what's going on if and when that happens.  Of course, my primary goal here is to teach you some practical data-science skills, not to run you through some mathematical boot camp for advanced studies in statistics.  Future-proofing your stats education is mainly just a nice side benefit.  

Let's dive right in.  


## The Central Limit Theorem  

### The normal distribution {-}

The first piece of the mathematical puzzle here is something called the normal distribution.  It helps us give a formal mathematical description of a phenomenon that you may have noticed yourself: that all sampling distributions basically look the same.

To dramatize this point, here's a cut-and-paste montage of nine different sampling distributions from eight different problems we've seen so far in these lessons:

```{r bunch-of-sampling-distributions, echo=FALSE, out.width="100%", fig.cap="Nine sampling distributions that all look pretty much the same."}
knitr::include_graphics("images/bunch_of_sampling_distributions.png")
```

I mean, just look at them.  It's like that famous first line of _Anna Karenina_, where all happy families are the same.  Without getting out your magnifying glass to inspect the scale of the $x$ axis in each plot, can you tell the difference between these sampling distributions?  I sure can't.  They all look the same!  More to the point, they all look like this:

<br>

```{r, echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("images/normal_cartoon.png")
```

<br>

Distributions that look like this have a name: we call them __normal distributions__ (or more colloquially, "bell curves," or more verbosely, ["Gaussian distributions"](https://en.wikipedia.org/wiki/List_of_things_named_after_Carl_Friedrich_Gauss)).  Normal distributions are "normal" in the same sense that blue jeans are normal, which is to say they're ubiquitous---as evidenced by the fact that all the sampling distributions in Figure \@ref(fig:bunch-of-sampling-distributions) look normal.  This is emphatically _not_ a coincidence, but rather a consequence of a very important result in mathematics called the Central Limit Theorem, or CLT for short.  We'll see a more precise statement of this theorem soon, but here's the basic idea.

> __Central Limit Theorem, simplified version__: sampling distributions based on averages from a large number of independent samples basically all look the same: like a normal distribution.

Since the normal distribution seems so... well... _central_ to all this business, let's try to understand it a bit more.  A normal distribution, written as $N(\mu, \sigma)$, can be described using two numbers: 

- the mean $\mu$, which determines where the peak of the distribution is centered.  
- the standard deviation $\sigma$, which determines how spread out the distribution is.  

The nine sampling distributions shown in Figure \@ref(fig:bunch-of-sampling-distributions) all have different means and standard deviations, but it's hard to tell because they're all shown on different scales (i.e. different sets of $x$ and $y$ axes).  Below are three different normal distributions, all shown on the _same_ scale, so you can appreciate their variety.

<!-- ``` -->
<!-- curve(dnorm(x, 0, 1), n= 10001, from=-10, to = 10, -->
<!--       xlab='', ylab='', axes=FALSE, ylim=c(0, 0.8)) -->
<!-- curve(dnorm(x, 2, 3), n= 10001, add=TRUE, col='red') -->
<!-- curve(dnorm(x, -3, 0.5), n= 10001, add=TRUE, col='blue') -->
<!-- axis(1, at = seq(-10, 10, by=1), cex.axis=0.8) -->
<!-- ``` -->

```{r, echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("images/normal_examples.png")
```

As you can see, a normal distribution can be centered anywhere (depending on $\mu$).  And it can be tall and skinny, short and broad, or anywhere in between (depending on $\sigma$).  If we think that some random quantity $X$ follows a normal distribution, we write $X \sim N(\mu, \sigma)$ as shorthand, where the tilde sign ($\sim$) means "is distributed as."  

Here are some useful facts about normal random variables---or more specifically, about the central areas under the curve, between the extremes of the left and right tail.  If $X$ has a $\mbox{N}(\mu, \sigma)$ distribution, then the chance that $X$ will be within $1 \sigma$ of its mean is about $68\%$; the chance that it will be within $2\sigma$ of its mean is about $95\%$; and the chance that it will be within $3\sigma$ of its mean is about $99.7\%$.  Said in equations:

\begin{eqnarray*}
P(\mu - 1\sigma < X < \mu + 1\sigma) &\approx& 0.68 \\
P(\mu - 2\sigma < X < \mu + 2\sigma) &\approx& 0.95 \\ 
P(\mu - 3\sigma < X < \mu + 3\sigma) &\approx& 0.997 \, .
\end{eqnarray*}

And said in a picture, with fastidiously accurate numbers:

<br>

```{r normal-6895, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap="A useful fact worth remembering about the normal distribution."}
knitr::include_graphics("images/normal6895.png")
```

<br>

### de Moivre's equation {-}

But our discussion of the Central Limit Theorem so far is incomplete.  Suppose we take a bunch of samples $X_1, \ldots, X_N$ from some wider population whose mean is $\mu$, and we calculate $\bar{X}_N$, the mean of the sample.  Can we say anything a bit more specific about the statistical fluctuations in $\bar{X}_N$, beyond the fact that those fluctuations will be approximately normal?  Specifically, can we say how spread out those fluctuations might be around the population mean of $\mu$?

It turns out that the answer is: yes, quite precisely.

This leads us to the second big piece of our mathematical puzzle: something called __de Moivre's equation__, also known as the "square root rule."^[To be clear, this is not the same thing as de Moivre's [formula for complex exponentials.](https://en.wikipedia.org/wiki/De_Moivre%27s_formula)]   The Central Limit Theorem tells us that, with enough data points, the distribution of an average looks approximately normal; de Moivre's equation tells us precisely how narrow or wide that normal distribution will be, as a function of two things:

(1) The variability of a single data point.  
(2) The number of data points you're averaging.  

This equation was discovered by a Swiss mathematician named [Abraham de Moivre](https://en.wikipedia.org/wiki/Abraham_de_Moivre) in 1730.  In my opinion, it represents one of the most under-rated triumphs of human reasoning in history.  Most educated people, for example, have heard of Einstein's equation: $e = mc^2$.  De Moivre's equation is just as profound; it represents an equally universally truth, and is equally useful for making accurate scientific predictions.  Yet very few people outside statistics and data science know it. 

De Moivre's equation concerns a situation we've approached before, using the bootstrap: one where we take an average of many samples, and we want to understand [how much statistical uncertainty we have about that average](#bootstrap-sample-mean).  Specifically, the equation establishes an inverse relationship between the variability of the sample average and the square root of the sample size.  It goes like this:

$$
\mbox{Variability of an average} = \frac{\mbox{Variability of a single data point}}{\sqrt{\mbox{Sample size}}}
$$

Data scientists usually use the Greek letter $\sigma$ (sigma) to denote the variability of a single data point, and the letter N to denote the sample size.  Also, you should remember our more technical term for the variability (or statistical fluctuations) of an average: the "standard error."  This terminology allows us to express de Moivre's equation a little more compactly:

$$
\mbox{Standard error of the mean} = \frac{\sigma}{\sqrt{N}} 
$$

This equation looks simple, but its consequences are profound.

First, de Moivre's equation places a fundamental limit on how fast we can learn about something subject to statistical fluctuations.  Naively, you might think that statistical uncertainty goes down in proportion to how much data you have, i.e. your sample size.  But de Moivre's equation tells us otherwise: it tells us that statistical uncertainty goes down in proportion to the __square root__ of your sample size.  So, for example:

- if you collect 4 times as much data as you had before, you're not 4 times as certain as you were before: de Moivre's equation says that your standard error will go down by a factor of $1/\sqrt{4} = 1/2$.  That is, you'll have half as much uncertainty as you had before.
- if you collect 100 times as much data as you had before, you're not 100 times as certain as you were before: de Moivre's equation says that your standard error will go down by a factor of $1/\sqrt{100} = 1/10$.  That is, you'll have 10% as much uncertainty as you had before.  

Second, de Moivre's equation also allows us to state a more precise version of the Central Limit Theorem, as follows.

```{theorem, name = "Central Limit Theorem, more precise version"}
Suppose we take $N$ independent samples from some wider population, and we compute the average of the samples, denoted $\bar{X}_N$.  Let $\mu$ be the mean of the population, and let $\sigma$ be the standard deviation of a single observation from the population.  If $N$ is sufficiently large, then the statistical fluctuations in $\bar{X}_N$ can be well approximated by a normal distribution, with mean $\mu$ and standard deviation $\sigma/\sqrt{N}$: 

$$
\bar{X}_N \approx N \left( \mu, \frac{\sigma}{\sqrt{N}} \right)
$$
```

<br>

Three notes here:  

(1) The Central Limit Theorem holds regardless of the shape of the original population distribution.^[OK, not _entirely_ regardless.  It is easy for mathematicians to find counterexamples, because that's what mathematicians do.  A more accurate statement is that the "basic" Central Limit Theorem holds whenever your population distribution has a finite mean and variance.  This covers the vast majority of all common data-science situations.]  In particular, the population itself _does not have to be normally distributed,_ and indeed can be crazily non-normal.  It doesn't matter.  As long as the sample size is large enough, the statistical fluctuations in the sample mean will look approximately normal.  We'll see examples of this below.  
(2) The requirement that $N$ be "sufficiently large" is what gives this lesson (and this whole approach to statistics) its name: __large-sample inference.__  The basic idea behind all this math is that, by studying what happens when we collect lots of data, we can learn some things that allow us to make inferential statements that hold __approximately__ across a wide variety of situations, even if we don't necessarily have lots of data ourselves.
(3) You might notice two remaining weasel words in this supposedly more precise version of the Central Limit Theorem: "sufficiently large" and "well approximated."  What, precisely, do those mean?  Alas, it's hard to say without going into some very detailed math, specifically a result called the [Berry--Esseen theorem](https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem).  But here's a rough guideline: if the population you're sampling from isn't too skewed or crazy looking, then 30 samples is generally "sufficiently large" for the approximation to be really quite good.  

### The CLT in action {-}

The Central Limit Theorem and de Moivre's equation can feel a bit abstract.  In my opinion, the best way to make them feel concrete is to mess around with some Monte Carlo simulations until they both start to make sense.  

### Some simple uses of CLT {-}

Let's see some examples with actual numbers attached, so that we can understand how the Central Limit Theorem, combined with de Moivre's equation, can be useful in practice for helping us reason about uncertainty.   


__Example 1: FedEx packages.__


__Example 2: etc.__


## Confidence intervals for a mean 

Let's now turn to the main question: how can we use the Central Limit Theorem, combined with de Moivre's equation, to get confidence interval without bootstrapping? 

You might also remember the "confidence interval rule of thumb" from our lesson on [The bootstrap]: 

> __Confidence interval rule of thumb:__ a 95% confidence interval tends to be about two standard errors to either side of your best guess.  A 68% confidence interval tends to be about one standard error to either side of your best guess.

These numbers of 68% and 95% actually come from the normal distribution (recall Figure \@ref(fig:normal-6895)), and the underlying justification for this rule of thumb _is_ the Central Limit Theorem.  The logic here, roughly is:

- The Central Limit Theorem says that statistical uncertainty in the sample mean can be described by a normal distribution.  
- de Moivre's equation tells us the standard deviation of that sampling distribution: $\sigma/\sqrt{n}$
- A normally distributed random variable is within 2 standard deviations of its mean 95% of the time, so $\bar{x}_n$ should be within 2 standard errors of $\mu$ (the right answer) 95% of the time.  
- Thus if we quote the confidence interval $\bar{x}_n \pm 2 \cdot \sigma/\sqrt{n}$, we should capture the true value in our interval 95% of the time.  


### Example 1: sleep, revisited  {-}

```{r, echo=FALSE, message=FALSE}
NHANES_sleep = read.csv('data/NHANES_sleep.csv', header=T)
```

For our first example, let's revisit the [NHANES_sleep](data/NHANES_sleep.csv) data we saw in the lesson on bootstrapping.   Please import this data set into RStudio, and then load the `tidyverse` and `mosaic` libraries:

```{r, message=FALSE}
library(tidyverse)
library(mosaic)
```

You'll recall that, in our original analysis of this data, we looked at Americans' average number of sleep hours per night, based on the following distribution of results from the NHANES survey:  

```{r, eval=FALSE}
ggplot(NHANES_sleep) + 
  geom_histogram(aes(x = SleepHrsNight), binwidth=1)
```

```{r, echo=FALSE}
ggplot(NHANES_sleep) + 
  geom_histogram(aes(x = SleepHrsNight), binwidth=1) + 
  scale_x_continuous(breaks=1:13)
```

We took the mean of this data distribution, and we found that on average, Americans sleep 6.88 hours per night.  

```{r}
mean(~SleepHrsNight, data=NHANES_sleep)
```

Now let's turn to the question of statistical uncertainty.  de Moivre's equation makes a specific mathematical prediction: it says that the standard error of this sample mean is equal to the standard deviation of a single measurement ($\sigma$), divided by the square root of the sample size.  So as long as you know $N$ and can calculate $\sigma$ from the data, we can use de Moivre's equation to quantify our statistical uncertainty, all without ever running the bootstrap---which, of course, de Moivre couldn't feasibly do, since he sadly had never encountered a computer.  

Let's churn through the math.  Our sample size $N$ is...

```{r}
nrow(NHANES_sleep)
```

...1,991 survey respondents.  And we estimate that standard deviation $\sigma$ of the `SleepHrsNight` variable is:

```{r}
sd(~SleepHrsNight, data=NHANES_sleep)
```

... about 1.32 hours.  Therefore de Moivre's equation says that the standard error of our sample mean should be...

```{r}
1.32/sqrt(1991)
```

...about 0.0296 hours.

Remember, this number represents a specific mathematical prediction for the statistical uncertainty of the sample mean---that is, the typical error of the mean across many repeated samples.  Let's compare this prediction with the standard error we get when we bootstrap:  

```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r, message=FALSE}
boot_sleep = do(10000)*mean(~SleepHrsNight, data=resample(NHANES_sleep))

# calculate bootstrapped standard error
boot_sleep %>%
  summarize(std_err_sleep = sd(mean))
```

About 0.0295.  Clearly the answer from de Moivre's equation was really, really similar.  In fact, here's $\pm$ 1 bootstrap standard error to either side of the sample mean,  superimposed on the bootstrap sampling distribution:

```{r, echo=FALSE, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_sleep) + 
  geom_histogram(aes(x=mean)) + 
  geom_vline(xintercept=c(6.878955 - 0.0296, 6.878955 + 0.0296), color='red', size=2)
```

And here's $\pm$ 1 standard error calculated from de Moivre's equation, superimposed on the same sampling distribution:

```{r, echo=FALSE, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_sleep) + 
  geom_histogram(aes(x=mean)) + 
  geom_vline(xintercept=c(6.878955 - 0.0295, 6.878955 + 0.0295), color='blue', size=2)
```

I don't know about you but I definitely can't tell the difference.  

What about a confidence interval?  Let's compare our bootstrapped confidence interval with what we get using de Moivre's equation---that is, going out two "de Moivre" standard errors from the sample mean.  Our bootstrapped confidence interval goes from about 6.82 to 6.94:

```{r}
confint(boot_sleep, level = 0.95)
```

And if we go use de Moivre's equation to go out two standard errors to either side of the sample mean of 6.88, we get a confidence interval of...

```{r}
6.88 - 2 * 1.32/sqrt(1991)
6.88 + 2 * 1.32/sqrt(1991)
```

... also 6.82 to 6.94.  Voila---a confidence interval that's basically identical to what you get when you bootstrap, except using math rather than computational muscle.  


### `t.test` shortcut {-}

In this particular case, it is both trivial and fast to compute a bootstrapped confidence interval.  However, I hope you think it's at least somewhat cool that de Moivre could have used his equation back in 1730 to tell you what bootstrap confidence interval you could expect to get 300 years later, with your fancy modern computer.  

However, it's also a little bit tedious to have to do these calculations "by hand," i.e. treating R as a calculator and manually type out the formula for de Moivre's equation, like this:

```{r}
6.88 - 2 * 1.32/sqrt(1991)
6.88 + 2 * 1.32/sqrt(1991)
```

Luckily, there's a shortcut, using a built-in R command called `t.test`:  It works like this:

```{r}
t.test(~SleepHrsNight, data=NHANES_sleep)
```

This command prints out a _bunch_ of extraneous information to the console.  You can safely ignore just about everything, except the part labeled "95% confidence interval," which is indeed the interval calculated using de Moivre's equation.^[You might ask, why is it really close to not _exactly identical_ to the confidence interval we calculated by hand?  Well, that's because going out 2 standard errors is really close to but not _exactly identical_ to a 95% confidence interval.  Instead of 2, `t.test` is using a number that's really, really close to 2, but is more exact.]

#### Summary: inference for a mean {-}

If you want to get a 95% confidence interval for a mean without bootstrapping, use `t.test`, and ignore everything in the output except for the part that says "confidence interval":

### Example 2: cheese {-}


Let's see a second example, using the [data on weekly cheese sales](data/kroger.csv) across 68 weeks at 11 different Kroger grocery stores that we examined back in the lesson on [Plots].  

```{r, echo=FALSE, message=FALSE}
kroger = read.csv('data/kroger.csv')
```

We'll start by filtering down this data set to consider only sales for the Dallas store: 

```{r}
kroger_dfw = filter(kroger, city == 'Dallas')
```

Here's a quick histogram of the data distribution for the `vol` variable, representing weekly sales of [American cheese](https://www.google.com/search?q=kraft+singles&hl=en&tbm=isch):

```{r, message=FALSE}
ggplot(kroger_dfw) + 
  geom_histogram(aes(x=vol))
```

For the Dallas store, it looks like the mean weekly sales volume is...

```{r}
# mean for the sample...
mean(~vol, data=kroger_dfw)
```

... about 4356 packages of cheese.

But what about statistical uncertainty?  Weekly sales of pretty much any consumer good are an inherently variable thing, even such a staple product as American cheese.  Maybe a couple weeks were a bit colder than average, depressing demand for patio fare like nachos.  Or maybe the Dallas area had a big power outage one week, implying that nobody could microwave any cheese.  These things happen.  And what if this particular store had a weekly target of 4500 packages?  Does this data imply that they're really, truly under-performing that target, or might they have been the victim of some unlucky weekly fluctuations?

Let's use de Moivre's equation to calculate the standard error of this sample mean.  Our sample size is...

```{r}
nrow(kroger_dfw)
```

...61 weeks.  And our sample standard deviation is...

```{r}
sd(~vol, data=kroger_dfw)
```

... about 2354.  So de Moivre's equation says is that our standard error is $\sigma/\sqrt{n}$, or:

```{r}
2353.645/sqrt(61)
```

...about 300 packages of cheese.  Let's compare this with our bootstrapped standard error, which is...

```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r, message=FALSE}
boot_cheese = do(10000)*mean(~vol, data=resample(kroger_dfw))

# calculate bootstrapped standard error
boot_cheese %>%
  summarize(std_err_vol = sd(mean))
```

... about 300.  Again, that's nearly identical to the answer from de Moivre's equation.  For reference, here's $\pm$ 1 bootstrap standard error to either side of the sample mean, superimposed on the sampling distribution:  

```{r, echo=FALSE, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_cheese) + 
  geom_histogram(aes(x=mean)) + 
  geom_vline(xintercept=c(4356.508 - 301.3534, 4356.508 + 301.3534), color='red', size=2)
```

And here's $\pm$ 1 standard error calculated from de Moivre's equation, superimposed on the same sampling distribution:

```{r, echo=FALSE, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_cheese) + 
  geom_histogram(aes(x=mean)) + 
  geom_vline(xintercept=c(4356.508 - 298.181, 4356.508 + 298.181), color='blue', size=2)
```

Again, pretty hard to tell apart.  No matter which standard error we use, it's totally plausible that the _real_ long run average sales figure for this store exceeds its hypothetical target of 4500 weekly units, and that the low sample mean just reflects natural variation.  This is a good example of where our statistical uncertainty comes not from sampling (like in a political poll), but rather from the intrinsic variability of some phenomenon---in this case, the economic and culinary phenomenon of how much cheese the people of Dallas happen to want to buy in any given week.  

de Moivre presumably never encountered American cheese.  I like to think that, being Swiss, he would have appreciated its similarity, at least in nacho-cheese form, to fondue.  (Although being _French_-Swiss, he might instead have found it [bizarre](https://www.youtube.com/watch?v=8xjfdGErgUc&ab_channel=BuzzFeedVideo)).  Either way, his equation is pretty good for describing statistical uncertainty surrounding American cheese sales.  






## Beyond de Moivre's equation

### Differences of means

### Proportions

### Differences of proportions

### Regression coefficients



## Misunderstanding de Moivre's equation

