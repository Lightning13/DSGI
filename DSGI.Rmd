--- 
title: "Data Science in R: A Gentle Introduction"
author: "James Scott"
date: "August 2020"
site: bookdown::bookdown_site
output:
  bookdown::gitbook
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: jgscott/DSGI
description: "A gentle introduction to data science in R."
always_allow_html: true 
---

# Welcome! {-}

Hello and welcome!  This online book is structured as a series of walk-through lessons in R that will have you doing real data science in no time.   It covers both the core ideas of data science as well as the concrete software skills that will help you translate those ideas into practice.

Many of these lessons operate on the premise of "mimic first, understand later."  That is, I'll introduce bits of R code that do something interesting and ask you to mimic them word for word to see what they do, without necessarily understanding the details at first.   The theory here is that if you see the "what" first, you'll be more motivated to understand the "how" and "why".  

This book also assumes you've never seen R before.  If that's you, head straight to Lesson 1: [Getting started in R].  If you've used R before, you might be better off skipping around using the Table of Contents in the sidebar.  Either way, I hope you enjoy these lessons.  And feel free to [drop me a line](mailto:james.scott@mccombs.utexas.edu) if you catch any errors or have any feedback!

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# (PART\*) Part I: Data exploration {-}

# Getting started in R

In this very first lesson, you'll learn how to:  

- get R up and running on your computer.  
- interact with R by typing statements into the console and seeing the results of those statements.    
- define your own objects and use those objects in subsequent statements.    
- create a new script and run some statements from that script.  
- get help with R.  
- install and load libraries.  


## Download R and RStudio 

To begin using R, you'll need to download two pieces of software:

1. Head to [cloud.r-project.org](http://cloud.r-project.org) to download and install the version of R appropriate for your computer (Windows, Mac, or Linux).  
2. Head to [www.rstudio.com](http://www.rstudio.com) and download RStudio.  If the `Download` link there asks you to choose your version, you'll want "RStudio Desktop," which is free and, like R, available for Windows, Mac, and Linux.

Why two programs?  Well, R is the program that does most of the raw computational work, while RStudio is a graphical front end for R (called an "integrated development environment," or IDE) that offers a lot of creature comforts.  You'll work entirely within RStudio; meanwhile, RStudio will interface with R behind the scenes, without your ever needing to open the R program itself.  Loosely speaking, if you imagine that analyzing data is like driving a car, then RStudio is the steering wheel and the pedals, while R is the engine.^[This car analogy isn't quite right.  Strictly speaking, RStudio is optional, and you _could_ opt to use base R (i.e. only the "engine").  But RStudio is both free and great. I _highly_ recommend using it; it's really helpful for beginners, but it also has many wonderful features that cater to advanced users and that you might explore as your R skills mature.  From here on out, I'll assume you're using RStudio rather than base R.  On a practical level, this means that you'll start your R sessions by double-clicking the RStudio icon on your computer, rather than the R icon.]  That's why you need to install R in order for RStudio to work: if you try to drive a car without an engine, you're not going to get very far.  


## First steps

Go ahead and open up RStudio for the first time.  On a Mac or Windows machine, this is as simple as finding where you've installed the RStudio app (__not__ the R app) and double-clicking its icon.  You should see something like this (with possible minor aesthetic differences, depending on your computer's operating system):

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/rstudio_console.png")
```

There's a lot here, and it might seem overwhelming at first.  Here's a brief summary of what you're looking at:

- The left panel, with the blinking cursor and the `>` symbol, is the __console__.  This is where R statements get evaluated and where the results of those statements get printed.  
- The top right panel shows your __workspace__.  In these lessons, you'll mainly use this panel to import data sets.  But this panel allows you to do some other handy things, too, like examine a history of prior statements that you've asked R to evaluate in the console.
- Finally, the bottom-right panel is mainly used for __plots, packages, and help__.  We'll see this in action soon.    

### Interacting with R {-}


For now, just focus on the left-hand panel (the console).  More specifically, focus on the blinking cursor in the console, right next to the angle bracket (`>`).  That `>` symbol is a prompt; it's R's way of saying, "Tell me what to do by typing statements right here."  Below I've circled it in red:

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/rstudio_console_circle.png")
```

Where see you that prompt (`>`), type the following statement:

```
2+3
```

__Then hit Enter__ to run the statement.  R should print out `5` right underneath where you typed `2+3`, and then it should give you a new prompt (`>` + blinking cursor) immediately below that.  In other words, your screen should now look like this:

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/rstudio_console_eval.png")
```

Congratulations!  You've just run your first line of R code --- in this case, a statement that tells R to do some simple arithmetic.  In the console, R is telling you that the result of evaluating the statement `2+3` is `5`.

You might be confused by the little `[1]` in front of the `5`.  That's R's way of telling you that it has only 1 number to report, and that this number is `5`.  This habit of R's might initially seem like oversharing, but it will make a lot more sense later, when we start doing calculations that produce multiple numbers as outputs and we might want some handy way of referencing, say, the 104th number.

This example, while simple, illustrates the basic idea of interacting with R:  

- You write __statements__ (also known as __commands__ or __lines of code__), and __run__ those statements in the console.  
- R __evaluates__ those statements and does something---for example, printing out the result of a calculation or making a plot.  

Some statements, like `2 + 3`, are simple.  Other statements, like `log10(1000) + 4^2 - 15.85841`, are a bit more complex.  (Try that one in the console.)^[You can probably guess what this statement does, but to be super explicit: it takes the base-10 logarithm of 1000, to that adds the quantity $4^2$ (i.e. $4 \cdot 4$), and from that subtracts 15.85841.  You might recognize the result.]  Soon, you'll start to chain multiple statements together to form a __program.__  But in the very beginning, learning R mostly means learning which statements do which things!  

### How you'll get feedback {-}

Every lesson in this book depends on getting __immediate feedback__: I'll show you what R statements you need to write in order to accomplish a given task, together with what you should expect to see as a result of executing those statements.  In my experience, this is the best way learn R (or any programming language).  

Above, I provided this feedback via screen shot.  This worked OK.  But screen shots aren't really the best way to give you the feedback you need.  Not only are they tedious for me, but they're also inefficient for you: they show a bunch of extraneous information, with the relevant feedback buried in a corner somewhere, perhaps with a silly red circle drawn around it.

So from here on, we'll adopt the following convention.   Whenever you're supposed to evaluate a statement (like `2+3`) and to compare your result with mine, you'll see it written in two boxes, like this:

```{r}
2+3
```

The first box shows what you're supposed to type.  Immediately below that, the second box shows what the result should be, whether it's something printed to console (like we see here) or a plot (like you'll see below).  Note that you won't actually see the little `##` symbols printed in your console.  Those `##` symbols are there so that you can distinguish _intended input_ (first box, no `##`) from _expected output_ (second box, with `##`) at a quick glance.

Let's see one more explicit example of this convention in action.  We'll add 1 and 4, and then multiply the result by 3:

```{r}
3*(1+4)
```

The two blocks above are telling you: 1) that you should type in `3*(1+4)` and then hit Enter (first block); and 2) that you should see `[1] 15` printed out to the console as a result (second block).  Again, the `[1] 15` means that R has 1 number to report as a result of what you asked it to do, and that this number is 15.

I'll mostly stick with this convention for providing feedback, saving screen shots only for when they're necessary.

### R as a calculator {-}

You probably got the sense from the examples above you can treat R as a simple calculator.  Indeed, R works exactly as you'd expect it to in this regard: it obeys the standard order of operations that you learned in grade school, and it also knows all the important "fancy" functions like logarithms and cosines and exponentials that you learned in high school.  Here we'll calculate the base-10 logarithm of 1000:

```{r}
log10(1000)
```

You can also treat R like a graphing calculator, using the `curve` function.  Try, for example, typing in the following statement:

```{r}
curve(x^2 - 3*x + 1, from=0, to=5)
```

This statement plots the curve $f(x) = x^2 - 3x + 1$ over the domain $0 \leq x \leq 5$.  The three inputs to `curve` are called __arguments__:  

- `x^2 - 3*x + 1` is the curve you want to plot.  
- `from=0` and `to=5` says that you want to start the curve at $x=0$ and end it at $x=5$.  

When you evaluate the statement by hitting Enter, nothing gets printed to the console, but you should see a graph pop up automatically in the `Plots` tab, in the lower-right panel on your screen.  (Displaying plots is one of the main uses of that lower-right panel, although we'll cover a couple of other uses later.)  

If you want to get some practice, try any of the following examples, or else just make up your own.  

```
30/10
5^2
sqrt(9)
log(7.4)
exp(2)
curve(cos(x), from=0, to=4*pi)
```

Note that to R:

- the four basic arithmetic operations are `+`, `-`, `*`, and `/`.   
- `log` means natural log, while `log10` means the base-10 log.  
- `x^a` means "raise x to the power a."  
- But if you want to raise the mathematical constant $e = 2.718...$ to some power $a$, use `exp(a)`.  
- For trigonometric functions (like `cos`, `sin`, `tan`, etc.), angles are assumed to be in radians. 

### R is case sensitive {-}

Statements in R are case-sensitive.  If you use upper case where lower case is expected, or vice versa, you'll get an error.    So if, for example, you typed `Curve` rather than `curve`, you'd get an error telling you that there is no such thing as a function called `Curve`:

```{r, error=TRUE}
Curve(x^2 - 3*x + 1, from=0, to=5)
```

Be aware of this case-sensitivity.  It's a common source of coding errors for beginners.  

## Objects {#sec_objects}

We've seen that using R as a (graphing) calculator is pretty straightforward.  But to do anything more interesting than basic arithmetic, we need to learn how to assign __values__ to __objects__.^[Objects are sometimes called __variables__ in computer programming.  But we'll use the term __object__ to avoid confusion with the statistical concept of a variable.]

In computer programming, an object is analogous to an envelope or a file folder.  It's a place where information can be stored, given a label, and accessed later.  Creating objects helps us break complex tasks down into a series of simpler tasks.

Let's create your first object.  Run the following statement in the console:
```{r}
foo = 3
```

Nothing gets printed to the console when you run this statement.  But under the hood, R has created an object called `foo` and stored the value `3` in that object.

Let's unpack the statement `foo = 3` piece by piece.  All assignment statements in R have the same basic structure:  

- `3` is the value of the object.  In our file-folder analogy, this is like the contents of the folder.  
- `foo` is the name of the object.  In our file-folder analogy, this is like the label on the folder.  Here we called the object `foo`, but you can call objects in R pretty much anything you want, with a [handful of exceptions](https://stat.ethz.ch/R-manual/R-devel/library/base/html/Reserved.html).  
- `=` is called the "assignment operator."  It tells R to assign the value on the right (3) to the object on the left (`foo`).   

If we now ask R what `foo` is, it will tell us:  
```{r}
foo
```

The reason we assign values to objects is so that we can use those objects in subsequent calculations.  It's like a souped-up version of the "memory" function on your calculator.   To illustrate this, let's create an object called `bar` that stores the results of the computation $4 + \log_{10}(100)$.  Since $\log_{10}(100) = 2$, this calculation should give us 6.    
```{r}
bar = 4 + log10(100)
```

Again, nothing gets printed to the console when we create an object and assign it a value.  But, remember, once we've created the object, we can type its name into the console, and R will tell us what value is stored there:  
```{r}
bar
```

More importantly, now we can use this object in a subsequent computation, just like we'd use any number:  
```{r}
bar + 4
```

Creating objects---that is, storing intermediate results in user-defined objects, and re-using those results in subsequent calculations---may not seem like a big deal now.  After all, we're just messing around with basic arithmetic.  But as you'll soon learn, the ability to create your own objects is a source of great power.  That's because it allows us to write a complex data analysis as a sequence of many smaller, simpler steps.  And that's the way we accomplish pretty much _anything_ complex in life, whether it's running a data analysis, auditing a bank, or building a house: 

- break the complex task down into simpler tasks.  
- accomplish each task in isolation, using the products of earlier tasks to help us with the next task. 
- stitch the tasks together in the proper order to accomplish the overall goal.  

We could summarize this "basic mantra of data science" as follows.

> __The basic mantra of data science__: manage complexity by breaking complex tasks down into simple tasks, and then stitching the simple tasks together.  

In the next section, we'll learn how __scripts__ can make this process a lot more manageable.  

<!-- ### `=` versus `==` -->

<!-- In a statement `foo = 3`, it's important to recognize that the `=` sign means __assignment__, not the __mathematical equals sign.__    If you want to test whether two things are equal to each other, R expects you to use a double-equals sign (`==`).  For example, suppose we've assign the value 3 to the object called `foo`:   -->

<!-- ```{r} -->
<!-- foo = 3 -->
<!-- ``` -->

<!-- Now we can  -->


## Scripts

In our previous examples, you learned to interact with R by typing statements like `2+3` or `sqrt(9)` directly into the console, where you see the `>` prompt.  This works OK for simple statements, but it's actually not the best way to interact with R---especially when we start chaining together complex statements to analyze data.

Instead, you should learn to work with __scripts.__   A script is a file that collects multiple statements (i.e. lines of R code) in a single document, which always ends in a `.R` suffix. 

Your basic workflow in RStudio should look like this. 

- __Create__ a script with the goal of solving some specific tasks.  
- __Write__ statements in your script, saving them for subsequent modification or re-use.  
- __Run__ those statements in the console to produce the desired output or behavior.  
- __View__ the results, usually either in the console or the `Plots` tab of RStudio's lower-right panel.   

There are many advantages to working with scripts, which we'll discuss below.  For now let's focus on the "how" and "what" rather than the "why."

### Creating and running scripts {-}

To create a new R script, go to the `File` Menu and choose `New File > R Script`.  Your screen should now look something like this:

```{r, echo=FALSE}
knitr::include_graphics("images/rstudio_open.png")
```

Before there were three panels; now there are four.  The bottom left panel is your old friend, the __console.__   But now there's a (new) top left panel called the __code editor__.   RStudio's code editor allows you to create, open, and edit R scripts.  When you opened the File menu and chose `New File > R script`, you conjured into existence a new, blank script whose default name is probably something like `Untitled1.R`.  In the lessons to follow, this panel is where you'll do most of your actual work, by creating and editing scripts (.R files) that encode the steps in a data analysis. 

Go ahead and type the following two statements in the new script you just created.  __Do not__ type them directly in the console.  Put each statement on its own line in your script, and then save the script, giving it whatever name you want (e.g. `my_first_script.R`):  

```
foo = 1 + 2
foo + 7
```

These statements: 1) create an object called `foo` that is assigned a specific value (i.e. 3, the result of `1 + 2`); and then 2) add 7 to `foo`.  We should get 10 as a result, right?   But when you type these statements in your script, nothing actually happens.  That's because you need to __run__ these statements in the console in order to get R to evaluate them.   

The easiest way to run statements from your script is to <span style="color:blue">highlight</span> those statements with your mouse, and then use the keyboard shortcut `Control-Enter` (`Command-Enter` works too if you're on a Mac).   When you do this, you should see the statements themselves, followed by their result, appear in your console, like this:

```{r}
foo = 1 + 2
foo + 7
```

And that's it---your first R script.  From here on, when I give you R commands to run, you should first write them in a script, and then run them in the console.  It might feel clunky at first, but you should practice this way of doing things, because it will indispensable when things get more complex.  

Two further notes on running statements from a script:  

- If you just want to run a single line from your script, you don't have to highlight it.  You can just click anywhere on that line and then hit `Control-Enter`.  
- You can also run statements (either individually or as a block) using the `Run` button at the top of the code editor.  I personally find this less user-friendly, but your mileage may vary.  

### A slightly more interesting script {-}

OK, so that first script was a bit... basic.  Below, I've given you one that's slightly more interesting.  You can safely include the little notes after the `#` symbol, which are called "comments."  R ignores anything in a script the right of the `#` symbol, allowing you to write little explanations (for yourself or others) about what your code is doing.

```
# Load one of R's built-in data sets about cars
data(mtcars)

# Fit a straight line for mpg vs hp and plot the result.
mpg_model = lm(mpg ~ hp, data=mtcars)
plot(mtcars$hp, mtcars$mpg)
abline(mpg_model)
coef(mpg_model)
```

Create a new blank script, and then type^[You could just copy-paste, but I recommend actually typing it, to build "muscle memory" for what these commands are doing.] this entire chunk of R code, word for word, into your script.   Don't worry too much about the details of the individual statements; we'll cover those in later lessons.

Now highlight everything in the script, and hit `Control-Enter` to run it.  You should see the plot below:

```{r, echo=FALSE}
data(mtcars)
mpg_model = lm(mpg ~ hp, data=mtcars)
plot(mtcars$hp, mtcars$mpg)
abline(mpg_model)
```

And you should also see the following information printed in your console:

```{r, echo=FALSE}
coef(mpg_model)
```

Each dot in the plot represents a car.  The x coordinate of the dot represents the horsepower (`hp`) of that car's engine.  The y coordinate represents that car's gas mileage (`mpg`).  The line you see is the result of fitting a _linear regression model_ to estimate the systematic relationship between mileage and horsepower.   The two numbers printed in your console tell you the $y$-intercept and slope of the trend line.  Unsurprisingly, cars with more powerful engines tend to get lower gas mileage (hence the negative slope). 

This code chunk exhibits two important ideas we've covered: 1) the use of `=` to __assign__ values to objects; and 2) the __re-use__ of those objects in subsequent statements.  For example, the statement `mpg_model = lm(mpg ~ hp, data=mtcars)` fits a straight line to mpg versus hp, storing the result in an object that I called `mpg_model`.  This `mpg_model` object is then re-used in two subsequent statements, `abline(mpg_model)` and `coef(mpg_model)`, which draw the line through the point cloud and print the intercept/slope to the console, respectively.

So there you have it: you've run and visualized your first data analysis in R! I hope you're beginning to get a feel for the basic RStudio workflow.  You create a script organized around some specific task.  Then you:

- __write__ statements in your script, saving them for subsequent modification or re-use.  
- __run__ those statements in the console to produce the desired output or behavior.  
- __view__ the results, usually either in the console or the `Plots` panel  

In more complex data analyses, you will typically iterate these three steps, gradually building up complexity until you've accomplished what you set out to do.  

Organizing your data analyses around scripts is probably the __single most important__ "best practice" of using R.  It's perfectly fine to type statements directly into the console every now and again, especially if you're in more of an "exploratory" mode.  But if you find yourself doing this repeatedly, _especially_ with complex statements that build on previous statements, you should probably stop and ask yourself: "Would I be better off writing these statements in a script instead, so that I can save, re-use, and modify them later?"  Usually the answer is yes!


### Why can't I just point and click? {-}

This way of interacting with R---writing statements in a script and then running those statements in the console---is called a "command-line interface" or a "REPL" (pronounced "reeple", for Read-Evaluate-Print Loop).  It may seem unfamiliar or even intimidating at first.  It's certainly different from popular programs you might be used to, where you do a lot of pointing and clicking.  To R beginners, the command-line interface can even feel like a step backward in time---sort of like you're interacting with a "dumb" computer from the 1970s rather than something "smart" from the 21st century, with a mouse or a touch screen.

```{r, echo=FALSE, out.width="75%", fig.align='center'}
knitr::include_graphics("images/old_terminal.png")
```

"Why do I have to type literally everything?" you might wonder.  "Why can't I click on menus and buttons that do this stuff for me?"

I understand this reaction.  But let me try to convince you that the command-line interface is actually a __huge advantage__ for doing data science!  It's true that the learning curve is steeper than with more familiar "point and click" software packages.   But with R, the results of running a complex analysis don't require that you remember a long, detailed succession of clicks and menu options.  Instead, those results rely upon a series of written commands that _do exactly what they say._  So, for example, if you want to re-run your analysis on a new data set---perhaps because you collected some more data---you don't need have to remember which buttons you clicked or which menu options you chose in order to get your new results.  You just have to load the new data set and re-run your script from beginning to end---possibly tweaking it here and there, as required.  

Here are five other advantages to this way of interacting with R, via scripts and the console:  

1. Scripts make it simple to save your work and pick up where you left off, without having to remember what you've accomplished already.  
2. Scripts make it easy to modify a complex analysis by adding or changing steps in the middle of a long chain of statements.  (No `Control-Z` required!)     
3. Scripts make your analysis shareable: just save the .R file and post it to a site like [GitHub](http://www.github.com).  
4. Scripts make your analysis reproducible, since anyone---including a future version of yourself---can read the script and see/repeat _exactly_ what you've done.  
5. Scripts make it much easier to diagnose and correct errors in your analysis, because---unlike, say, in other [widely used data-analysis programs](https://www.businessinsider.com/reinhart-and-rogoff-admit-excel-blunder-2013-4)---an error __always__ arises from some specific statement that's written down in black and white for anyone to see.^[Rather than, for example, hidden in a formula associated with some specific cell of a spreadsheet.]  

As I hope you'll come to appreciate over the lessons to come, these advantages far outweigh the familiar comforts of mice and menus.


## Getting help {#sec_getting_help}

Everyone needs help with R at some point or another.  In fact, the more you use R, the more you'll find yourself relying on various help resources.  In my life as a teacher and researcher, __I look for R help all the time,__ and you will, too.

Here are your best bets.  

1. Search the web.  There's a __huge__ and very active community of R users out there, and many of them like to post about their R problems, or their solutions to other people's R problems, online.  If in doubt, just Google your question, e.g. "[How do I find the maximum of two numbers in R?](https://www.google.com/search?q=how+do+I+find+the+max+of+two+numbers+in+R)"  Chances are very, very good that your question has been asked and answered before.    
2. Search R's own help files.  For example, suppose you wanted to figure out how to calculate the median of a bunch of numbers in R.  You could go to the `Help` tab in the lower-right panel, and type `median` into the search bar.  You'd see several options pop up, one of which is the function called `median`.  Click on it, and the help page for that function will be displayed.  It will show you how to use the function, and even give you examples at the bottom of the help page.  

3. If you want help on a specific function---say, for example, the function `log10`---you can type a question mark, followed by the function's name, into the console.  Try, for example, running this statement:
```{r}
?log10
```

Personally, I use option 1 about 70% of the time, option 3 about 30% of the time, and option 2 about 0% of the time (since all of R's help files are on the web anyway, and will come up in a web search if they're useful).



## Libraries {#sec_installing_library}

R has an [enormous](https://www.google.com/search?q=how+many+R+libraries+are+there) ecosystem of libraries, ranging from the simple to the very sophisticated.  A library is a piece of software that provides additional functionality to R, beyond what's contained in the basic R installation.  If R is like a smart phone, then a library is like an app for the phone.  And just like a phone app, a library is something you need to _install_ only once, but _load_ each time you want to use it.

### Installing a library {-}

Here we'll install two libraries that we'll use a lot in the lessons to follow: `tidyverse` and `mosaic`.

The first minute of [this video](https://www.youtube.com/watch?v=u1r5XTqrCTQ) gives a walk-through of how to install a library. (It's with an older version of RStudio, thought the process is still the same).  But we'll explain the steps here, too.   Conveniently, libraries, also called "packages," are installed from within RStudio itself.

Here are the steps to install `tidyverse`.  The same process works for any library:

1. In the lower right panel of RStudio, you'll see a tab called `Packages`.  Click on it.
2. Under the `Packages` tab, you'll see a button at the top left of the panel called `Install`.  Click on it.
3. In the window that pops up, type in the name of the package you want to install: `tidyverse`.  After a few letters, RStudio will start to auto-suggest options for you.  Just keep typing until you see `tidyverse` as the only option.  Then either click on it or hit `Tab` to auto-complete the full name of the library.
4. Click the `Install` button.

In response, R should print out a very long and seemingly ponderous "progress report" into the console.  This "progress report" provides all kinds of interesting detail for R super-users, but it isn't all that helpful if you're a beginner---and because there's just so much darn red text, it can even seem intimidating!  But not to worry. R is just telling you, in its own long-winded way, that it's downloading and installing a bunch of `tidyverse`-related files behind the scenes.^[`tidyverse` is actually a suite of individual libraries, kind of like Microsoft Office is a suite of individual programs.  So it make take a minute or two to install.]   Eventually it should stop with some kind of `DONE` message in the console and give you another prompt (`>`), at which point the library is installed and ready to be used.

Then repeat the whole process to install `mosaic`.  If you got an error, see [Dealing with installation errors], below.

### Loading a library {-}

Let's practice loading the `tidyverse` library, which we'll lean on heavily in most of these lessons.  In R, you load a library using the `library` command, like this:

```{r}
library(tidyverse)
```

If you see a similar set of messages to what's shown above after the `##` symbols, you're good to go!  Feel free to move on to the next lesson.  

But if you haven't installed the `tidyverse` library, executing this command will give you an error like this:

  ```
Error in library(tidyverse) : there is no package called ‘tidyverse’
```

To avoid the error, you'll first need to install `tidyverse` like we covered above.


### Dealing with installation errors {-}

R might issue a `warning` or tell you that it had a `conflict` when you installed these libraries.  Despite sounding scary, these notices are almost certainly innocuous and can be safely ignored.

On the other hand, if R tells you that it had an `error` when installing, you'll need to address that error before you move on.  Luckily, most library installation errors are a result of having an old version of R, and therefore fixable with two simple steps:

1. Install the latest version of R, from [cloud.r-project.org](http://cloud.r-project.org).
2. Install the latest version of RStudio, from [www.rstudio.com](http://www.rstudio.com).

Usually, it's that simple.

If that doesn't fix the problem, however, your next step is to try Googling the error message.  (If you're taking my class and show me the installation error message, that is precisely what I will do, unless it happens to be some particular error I've seen before.)  Installation errors like this are rare, but they can happen, and they're almost always due to some quirk of your particular computer (and therefore difficult to generalize about).  The good news is whatever error you're experiencing is almost surely not unprecedented in the history of R.  In fact, the chances are good that someone, somewhere, has figured out what the error means and how to fix it.

In my large classes at UT-Austin, the most common (but still quite rare) library installation error I've seen tends to occur on Windows machines, and it looks something like the following:

```
The downloaded source packages are in
?/tmp/Rtmph4YKLX/downloaded_packages?
Updating HTML index of packages in '.Library'
Warning in install.packages :
cannot create file
'/opt/POC/lib64/Revo-7.3/R-3.1.1/lib64/R/doc/html/packages.html', reason
'Permission denied'
```

Yikes!  Basically, what's happening here is that you don't have permission to write files to the directory where R wants to write files. (Hence `'Permission denied'`!)  This can happen if your computer is actually owned by someone else, e.g. your employer, but I've also seen it happen out of the blue to students who own their computers fair and square.

Fixing the error depends on which version of Windows you have, and so your best bet is to Google something like "Give permissions to files and folders in Windows" and follow whatever steps suggested by the collective wisdom of humanity.  



<!--chapter:end:01_getting_started.Rmd-->

# Data {#lesson_data}

R is all about data.  So let's look at some!  In this lesson, you'll learn to:  

- Import and view a data set.  
- Understand the basic vocabulary of data.  
- Conduct a short but complete data analysis by compute simple statistics and making a plot.  

To start, open RStudio and create a new script for this lesson (calling it, e.g., `lesson_data.R`, or whatever you want).  You'll use this script to type out some code chunks and see what results they produce.    

Then place the following line at the top of your R script, and run it in the console.  This will load the tidyverse library, just like we covered in the [Libraries] lesson:

```{r, message=FALSE}
library(tidyverse)
```

This will make all the functionality of the `tidyverse` library available when we need it down below.  

## Importing a data set {#importing_data}

```{r, echo=FALSE, message=FALSE}
tvshows = read.csv('data/tvshows.csv')
```

Let's see how to import a data set into R.  The way I'm going to describe here is very beginner friendly; another (totally optional and more advanced) way is described in "[Importing data from the command line]."

Download the data set [tvshows.csv](data/tvshows.csv), which contains data on 40 shows that aired on major U.S. television networks.  A .csv file, like this one, is a common way of encoding tabular data in a way that can be easily shared across platforms.   

> __Important public-service announcement__: don't open that .csv file with Excel first!  I see folks do this from time to time in my classes at UT-Austin, and it can produce bizarre errors.  Basically, if you open a .csv file with Excel and then hit `Control-S` to save (or let it auto-save), Excel might decide to barf under the rug in your file.  This effect will remain insidiously invisible within Excel itself, but can potentially render the file unreadable to RStudio.  If you've already done this, don't worry --- just delete the file and download a fresh copy.^[In my experience this behavior seems to depend on your platform and version of Excel.  It _is possible_ to safely open and edit .csv files within Excel, but then you have to _re-export_ them as .csv files again if you want to use them in RStudio.]

[This video](https://www.youtube.com/watch?v=DuQSQQa6Ssw&t=431s&ab_channel=JamesScott) shows how import a data set into RStudio.  But we'll cover the steps here, too:    

1. Go to the `Environment` tab in RStudio's top-right panel.  
2. Click the `Import Dataset` button.  You'll see a pop-up menu.  Choose the first option: `From Text (base)...`.  
3. A standard file-browser window should pop up.  Use it to navigate to wherever you saved the `tvshows.csv` file.  When you find it, click `Open`.  

At this point, a new window should pop up that looks something like this.  I've labeled several parts of this screen shot in red, so that I can refer to them below.  

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/import_dataset.png")
```

Let's take those four red boxes in turn:  

1. This is what the raw .csv looks like.  It's just what you'd see if you opened the file in a plain text editor.  
2. This is the name of the __object__ in R that will be created in order to hold the data set.  It's how you'll refer to the data set when you want to do something with it later.  R infers a default name from the file itself; you can change it if you want, but there's usually no good reason to do so.  
3. This tells R whether the data set has a header row---that is, whether the first row contains the _names of the columns._  Usually R is pretty good at inferring whether your data has a header row or not.  But it's always a good idea to double-check: click `Yes` if there's a header row (like there is here), and `No` if there isn't.  
4. This little window shows how the imported data set will appear to R.  It's a useful sanity check.  If you've told R (or R has inferred) that your data set has a header row, the first row will appear in bold here.

Now click `Import`.  The imported data set should pop up in a Viewer window, like this:

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/import_dataset2.png")
```

Note that you can't actually edit the data set in this window---only view it.  You may also notice that `tvshows` is now listed under the `Environment` tab in the top-right panel.  This tab keeps track of all the objects you've created.  

The Viewer window is a perfectly fine way to look at your data.  I find it especially nice for giving me a "birds-eye" view of an unfamiliar data set.  If you've closed the Viewer window, you can always conjure it back into existence using the `view` function, like this:

```
view(tvshows)
```

Another handy "data viewing" command in R is called `head`.  This command will print out the first six of a data set in the console, like this:

```{r}
head(tvshows)
```

I use this one all the time to get a quick peek at the data, but don't want the pop-up viewer window to take me away from my script---like, for example, when I need to remind myself what the columns are called.    

## The vocabulary of data  

Let's talk about the structure of this `tvshows` data set, as a way of introducing some important vocabulary terms.  

- The typical way R stores data is in a tabular format called a __data frame.__
- Each row of the data frame is called an __example__, a __case__, or an __observation__.  These three words can be used interchangeably.  Here, each example is a TV show.  
- A collection of examples is called a __sample__, and the number of examples is called the __sample size__, usually denoted by the letter $n$.  Here our sample size is $n=40$.  
- Each column of the data frame is called a __feature__ (or sometimes a __variable__).  Each feature conveys a specific piece of information about the examples.  
- The first feature, `Show`, tells us which TV show we're talking about.  This type of feature is called a __unique identifier__, or just an __ID__.  
- Some features in the data set are actual numbers; appropriately enough, these are called __numerical features__.  The numerical features in this data set are `Duration`, `GRP` (["gross rating points"](https://marketing-dictionary.org/g/gross-rating-point/), a measure of audience size), and  `PE` (which stands for ["predicted engagement"](https://affect.media.mit.edu/pdfs/13.Hernandez.et_al.pdf) and measures whether the audience is actually paying attention to the show).  
- Other features in the data set can only take on a limited number of possible values, just like the answer to a multiple-choice question.  The categorical features in this data set are __categorical features.__  The categorical features in this data set are `Network` and `Genre`.^[You could also argue that `Duration`, despite being a number, should really be considered a categorical feature, since it only takes on a set of specific values like 30 and 60.]  

So, for example, a data set with 500 examples and 10 features would be stored in a data frame with 500 rows and 10 columns.  

## A short, simple data analysis {#sec_example_analysis}

Once you've imported a data set and comprehended its basic structure (using, e.g., `view` or `head`), the next step is to try to learn something from the data by actually analyzing it.  We'll see many ways to do this in the lessons to come!  But for now, here's a quick preview of the two simplest ways: calculating summary statistics and making plots.  

A __statistic__ is any numerical summary of a data set.  Statistics are answers to questions like: How many reality shows in the sample are 60 minutes long?  Or: What is the average audience size, as measured by GRP, for comedies?  

A __plot__, meanwhile, is exactly what it sounds like: per [Wikipedia](https://en.wikipedia.org/wiki/Plot_(graphics)), a "graphical technique for representing a data set, usually as a graph showing the relationship between two or more variables."  Plots can help us answer questions like: What is the nature of the the relationship between audience size (`GRP`) and audience engagement (`PE`)?  

Let's see some examples, on a "mimic first, understand later" basis.  Just type out the code chunks into your `lesson_data.R` script and run them in the console, making sure that you can reproduce the results you see below.  

Our first question is: how many reality shows in the sample are 60 minutes long, versus 30?  As we'll learn in the upcoming lesson on [Counting](#contingency_tables), the `xtabs` function gets R to count for us:  

```{r}
xtabs(~Genre + Duration, data=tvshows)
```

It looks like 8 reality shows are 60 minutes long (versus 9 that are 30 minutes long).  

Let's answer a second question: what is the average audience size for comedies, as measured by `GRP`?  Calculating this summary actually requires multiple lines of R code chained together, and it relies upon functions defined in `tidyverse`, so it won't work if you haven't already loaded that library.  You might be able to guess what this code chunk is doing, but we'll understand it in detail in the upcoming lesson on [Summaries].    

```{r}
tvshows %>%
  group_by(Genre) %>%
  summarize(mean_GRP = mean(GRP))
```

The result is a summary table with one column, called `mean_GRP`.  It looks like comedies tend to have middle-sized audiences: an average GRP of about 630, versus 401 for reality shows and 1243 for drama/adventure shows.  

Finally, let's make a plot to understand the relationship between audience size (`GRP`) and audience engagement (`PE`).  We'll make this plot using a function called `ggplot`, which is made available by loading the `tidyverse`.  Again, don't sweat the details; we'll cover `ggplot` in detail in the upcoming [Plots] lesson.  Just copy/paste the statements into your script, run them, and observe the result:  

```{r first-example-plot, out.width="100%", fig.align='center'}
ggplot(tvshows) + 
  geom_point(aes(x=GRP, y=PE, color=Genre))
```

We've learned from this plot that shows with _bigger_ audiences (high GRP) also tend to have more _engaged_ audiences (higher PE), a fact that isn't necessarily obvious (at least to me) until you actually look at the data.  If you wanted to save this plot (say, for including in a report or a homework submission), you could click the `Export` button right above the plot itself and save it in a format of your choosing.  You can also click the `Zoom` button to make the plot pop out in a resizeable window.

Note: this plot above actually isn't great, because the red and green can look nearly identical to color-blind viewers.  I'm especially sensitive to that issue, since I'm red-green colorblind myself.

```{r, echo=FALSE, out.width="50%", fig.align="center"}
knitr::include_graphics("images/colorblind.png")
```

We'll clean up that problem in a later lesson, when we learn more about plots.  But for now, focus on your victory!  You've now conducted your first data analysis---a small one, to be sure, but a "complete" one, in the sense that it has all the major elements involved in _any_ data analysis using R:   

- import a data set from an external file.  
- decide what questions you want to ask about the data.  
- write a script that answers those questions by running an analysis (calculating statistics and making plots).  
- run the script to see the results.  

In the lessons to come, we're going to focus heavily on the "analysis" part of this pipeline, addressing questions like: what analyses might I consider running for a given data set?  How do I write statements in R to run those analyses?  And how do I interpret the results?

## Importing data from the command line

This section is totally optional.  If you're an R beginner, or you're happy with the `Import Dataset` button for reading in data sets, you can safely skip this section and move on to the next lesson.

If, on the other hand, you have a bit more advanced knowledge of computers and are familiar with the concepts of [pathnames](https://en.wikipedia.org/wiki/Path_(computing)) and [absolute vs. relative paths](https://www.geeksforgeeks.org/absolute-relative-pathnames-unix/), you might prefer to read in data from the command line, rather than the point-and-click interface of the `Import Dataset` button.  I personally prefer the command-line way of doing things, because it makes every aspect of a script (including the data-import step) fully reproducible.  The downside is that it's less beginner-friendly.

Let's see an example.  To read in the tvshows.csv file from the command line in R, I put the following statement at the top of my script:

```{r}
tvshows = read.csv('data/tvshows.csv', header=TRUE)
```

This statement reads the "tvshows.csv" file off my hard drive and stores it in on object called `tvshows`.  Let's break it down:  

- `read.csv` is the command that reads .csv files.  It expects the path to a file as its first input.  
- `'data/tvshows.csv'` is the path to the "tvshows.csv" file on my computer.  In this case it's a relative path (i.e. relative to the directory on my computer containing the R file from where the command was executed).  But you could provide an absolute path or a url instead.  
- `header=TRUE` tells R that the first row of the input file is a header, i.e. a list of column names.  
- `tvshows` is the name of the object we're creating with the call to `read.csv`. It's how we'll refer to the data set in subsequent statements.  

R has a whole family of `read` functions for various data types, and they all follow more or less this structure.  I won't list them all here; [this overview](https://cran.r-project.org/doc/manuals/r-release/R-data.html) is terse but authoritative, while [this one](https://www.datacamp.com/community/tutorials/r-data-import-tutorial) is a bit friendlier-looking.   

<!--chapter:end:02_data.Rmd-->

# Counting {#contingency_tables}

Counting is one of the simplest you can do with a data set.  But it can you get you surprisingly far.  I've heard it said that data science is mostly plotting and counting things.  After all, "How many?" is the [most basic quantitative question](https://www.youtube.com/watch?v=0Zi8KbgVhFc&ab_channel=SesameStreet) we can ask about the world.  Moreover, from counts, we can get proportions, and from proportions we can estimate probabilities---like, for example, one of the probabilities we'll estimate below: how likely is a band to play at Austin City Limits Music Festival, given that it played at Lollapalooza earlier that same year?  

This lesson is about getting R to do the work of counting for us, so that we can focus on the questions we care about and not the tedious mechanical details of how to answer them.  You'll learn to:  

- make tables of counts with `xtabs`.  
- use `prop.table` to turn tables of counts into tables of proportions.    
- use those tables to estimate simple probabilities (including conditional and joint probabilities).  
- use the pipe operator (`%>%`) as a way of chaining together computations.      

For this lesson, you'll need to download [aclfest.csv](http://jgscott.github.io/teaching/data/aclfest.csv), which contains data on some bands that played at several major U.S. music festivals (including our own ACL Fest here in Austin).  You'll also want to create a new R script and name it something (e.g. `lesson_tables.R`).  That way, you can practice the advice from an earlier lesson on [Scripts]: write statements in scripts, not directly in the console.

## Getting started: ACL Fest

First load the [tidyverse library](https://www.tidyverse.org/), which we'll need for just about every R lesson in this book.  

```{r, message=FALSE}
library(tidyverse)
```

Next, use the `Import Dataset` button to read in `aclfest.csv`.  If you need a reminder on how to accomplish these two key steps, see the previous lessons: [Loading a library] and [Importing a data set].  

```{r, echo=FALSE, message=FALSE}
aclfest = read.csv('data/aclfest.csv')
```

If you've imported the data correctly, you can use the `head` function to get the first six lines of the file.  You should see the following result:

```{r}
head(aclfest)
```

Each entry is either a 1 or a 0, meaning "yes" and "no", respectively.  So, for example, on the 6th line we see an entry of 1 for Radiohead under the `lollapalooza` column, which means that Radiohead played at Lollapalooza that year.  

Let's make a few simple tables with this data.  This will allow us to estimate probabilities like:  

1. How likely is a band in this sample to play Lollapalooza?  
2. How like is a band to play both ACL Fest _and_ Lollapalooza?
3. How likely is a band to play ACL Fest, given that they played Lollapalooza?  


## Simple probabilities {#simple_probabilities}

### Using xtabs alone {-}

What is P(played Lollapalooza) for a randomly selected band in this sample?  To answer this, we'll use R's `xtabs` function to tabulate (i.e. count) the data according to whether a band played Lollapalooza (1) or not (0).  

```{r}
xtabs(~lollapalooza, data=aclfest)
```

You might be curious about the little tilde (`~`) symbol in front of `lollapalooza`.  Roughly speaking, `~` means "by" or "according to"---as in, "cross-tabulate BY the `lollapalooza` variable."

Remember that, in our table, 1 means yes and 0 means no.  So of the 1238 bands (800 + 438) in this sample, 438 of them played Lollapalooza.  We can now use R as a calculator to get this proportion:

```{r}
438/(800 + 438)
```

So about 0.35 (35%).  This is a simple illustration of what you might call the "plug-in principle": if you want to estimate a probability, plug in the corresponding frequency from your data set.

### Using prop.table {-}

Simple enough, right?  But we can also get R to turn those counts into proportions for us, using the `prop.table` function.  In general, the more work we can get R to do for us, the better.  

To do so, we'll make the same table as before, except that now we'll save the result in an object whose name we get to choose.  (Remember our lesson on [Objects].)  We'll call it `t1` in the code below, although you could call it something more imaginative, like `my_great_lollapalooza_table`, if you wanted to.  

```{r}
t1 = xtabs(~lollapalooza, data=aclfest)
```

Notice that nothing gets printed to the screen when you execute this command.  But if you ask R what `t1` is, it will show you the same table as before:
```{r}
t1
```

OK, so why did we bother to store this table in something called `t1`?  Well, remember the core ideas in data science:

> We manage complexity by breaking down complex tasks into simpler tasks, and then linking those simpler tasks together.

That's exactly what we're doing here: we'll take this `t1` object we've created (the first link our chain) and pass it into the `prop.table` function (the second link in our chain).  This function turns a table of counts (like `t1`) into a table of proportions, like this:

```{r}
prop.table(t1)
```

Of course, the answer we get, P(plays Lollapalooza) = 0.354, is the same one we got when we did the division "by hand" (i.e. treating R as a calculator to calculate 438/1238 using numbers we manually peeled off the table created by `xtabs`).  

### Using pipes {-}

The above way of doing things---using `xtabs` creating an "intermediate" object called `t1` and then passing `t1` into the `prop.table` function---works just fine.  Lots of people write R code this way.

But it turns out there's a nicer way to accomplish the same task, using a "pipe" (`%>%`).  Pipes allow us to combine multiple operations in a single "pipeline": that is, a sequential chain of operations, with the result of one operation feeding into the next one.  

Here's an example of a pipeline with just two steps:
```{r}
xtabs(~lollapalooza, data=aclfest) %>%
  prop.table
```

I tend to think of the pipe symbol (`%>%`) as meaning "then."  So in English, this code block says: 

- make a table of counts of the `lollapalooza` variable in the `aclfest` data set...
- _then_ (`%>%`) hand off this table of counts to `prop.table` in order to make a table of proportions.  

Pipes make your code easier to easier to write, read, and modify.  For a simple calculation like this involving only two steps, the difference is minimal.  But for the more complex kinds of calculations we'll see later in the course, the difference can be substantial.

Piping makes it _especially_ easy to add steps in your pipeline.  For example, suppose we wanted to round all numbers to 3 decimal places.  This is easily done by adding one more pipe:
```{r}
xtabs(~lollapalooza, data=aclfest) %>%
  prop.table %>%
  round(3)
```

Or in English:  

- make a table of counts...
- _then_ (`%>%`) turn this into a table of proportions...
- _then_ (`%>%`) round the result to 3 decimal places.  

I hope you agree that the rounded table looks nicer.  (After all, there are only about 1000 bands in the data set, so proportions beyond the thousandths place are [spuriously](http://www.fallacyfiles.org/fakeprec.html) [precise.](https://allthetropes.fandom.com/wiki/Ludicrous_Precision))

We'll use pipes a lot in the lessons to come.  I like them because they give a very transparent view of the logical flow of steps in a data analysis.

Of course, if you take things to an extreme and try to write __everything__ in your scripts as one long sequence of pipes, it can start to be self-defeating, and your code will be _less_ readable as a result.  But for calculations involving somewhere between, say, 2 and 10 steps, pipes are hard to beat from a readability perspective.    

One final nice feature of pipes is that, rather than printing the result of a pipeline directly to the screen, you can store the result in an object.  Suppose, for example, that we wanted to store our rounded table of probabilities in an object called `t2`.  We'd proceed like this:  


```{r}
t2 = xtabs(~lollapalooza, data=aclfest) %>%
  prop.table %>%
  round(3)
```

Nothing gets printed when you run this code chunk. However, just as with any user-defined object, `t2` is sitting there in memory, waiting to be called upon:  

```{r}
t2
```

## Joint probabilities

Let's now look at a second question: how likely is a band to play both ACL Fest _and_ Lollapalooza?  You might recognize that kind of question as asking for a _joint probability_.   

We'll start off by tabulating the bands according to both of the relevant variables: whether they played at ACL (0 or 1), and whether they played at Lollapalooza (0 or 1).  This works as follows:

```{r}
xtabs(~acl + lollapalooza, data=aclfest)
```

Here you should interpret the `+` sign as meaning "and", not in terms of arithmetic.  So in English, the code is saying: "cross-tabulate the aclfest data by the `acl` AND `lollapalooza` variables."  

The result of calling `xtabs` gives us some numbers that represent joint frequencies.  Remember, for the row and column labels, 1 means yes and 0 means no.  So to be specific, the output is telling us that, of the 1238 bands in the data set:  

- 719 played at neither ACL nor Lollapalooza.  
- 81 played at ACL but not Lollapalooza.
- 361 played at Lollapalooza but not ACL.  
- 77 played at both ACL and Lollapalooza.  

From here, we could actually stop and use use a calculator (or use R _as_ a calculator) to work out the relevant joint probability.  But as you'll see, it's less labor-intensive if we get R to do some of that work for us.  

So instead, let's use `prop.table` again, to turn those counts into proportions.  As with the first example above, we'll do this using a pipe:  
```{r}
xtabs(~acl + lollapalooza, data=aclfest) %>%
  prop.table %>%
  round(3)
```

These numbers now represent __proportions__, not counts.  Therefore, all the entries in the table sum to 1, rather than to 1238.  So to be specific, the table is telling us that:

- P(not ACL, not Lollapalooza) = 0.581.   
- P(not ACL, Lollapalooza) = 0.292
- P(ACL, not Lollapalooza) = 0.065
- P(ACL, Lollapalooza) = 0.062

And this last number, of course, is the answer to the question we set out to answer: P(ACL, Lollapalooza) = 0.062.  


## Conditional probabilities

Let turn to our final question: what is P(played ACL | played Lollapalooza)?  Or said in English, what is the conditional probability that a band played ACL, given that they played Lollapalooza?  You might remember the following rule for conditional probabilities:

$$
P(A \mid B) = \frac{P(A,B)}{P(B)}
$$

In frequency terms, we can interpret this formula as saying: how often do $A$ and $B$ happen together, as a fraction of how often $B$ happens overall?  To estimate this probability using the data, we'll again tabulate the bands by both the `acl` and `lollapalooza` variables:

```{r}
xtabs(~acl + lollapalooza, data=aclfest)
```

From this table of counts, we can reason as follows:

- There were 361 + 77 = 438 bands that played at Lollapalooza.  
- Of those 438 bands, 77 played at ACL.  
- Therefore, P(played ACL | played Lollapalooza) = 77/438.

Let's use R as a calculator to express this as a decimal number:  
  
  ```{r}
77/438
```

So about 0.176.

#### Using `margin` {-}

But as with the previous two examples, it's much more satisfying to get R to do the work for us.  We'll do this using `prop.table` again---but this time, with a twist.  Pay close attention to the middle line, where we call `prop.table`:

```{r}
xtabs(~acl + lollapalooza, data=aclfest) %>%
  prop.table(margin=2) %>%
  round(3)
```

Notice how we added `margin=2` inside parentheses in the `prop.table()` step.  What `margin=2` does is to tell `prop.table` that it should calculate proportions __conditional on the second variable__ we named, which was `lollapalooza`.  (Hence `margin=2`; if you wanted to condition on the first variable you named, which here is `acl`, you'd type `margin=1` instead.)  

So having specified that we want to condition on the `lollapalooza` variable, now we can read off the relevant conditional probabilities directly from the table---no calculator required.  Let's focus on the `lollapalooza = 1` column, since this is what we want to condition on (i.e. that a band played Lollapalooza).  The numbers in that column tell us that:  

- P(didn't play ACL | played Lollapalooza) = 0.824  
- P(played ACL | played Lollapalooza) = 0.176  

And that second number is just the answer we were going for.   


## Study questions {-}

1. Use the "aclfest.csv" data set to answer the following questions.  

    A. What is the joint probability that a band played both Outside Lands _and_ Bonnaroo?   
    B. What is the conditional probability that a band played Coachella, given that they played ACL Fest?  Use pipes (`%>%`), and write out in English what each line of your code chunk is doing.   

2. Download [plays_top50.csv](data/plays_top50.csv).  This file has data on 15,000 users of a music streaming service.  The first column is a unique numerical identifier for the user.  The remaining columns are for the top 50 artists most frequently streamed by this particular subset of users.  The entries in the data frame represent "did play" (1) and "did not play" (0), with 1 meaning that a given user streamed a given artist at least once during the data-collection period.   Use this data to answer the following questions:  

   A. For a randomly selected user, what are P(plays Franz Ferdinand) and P(plays Franz Ferdinand | plays the Killers)?    
   B. What are P(plays Bob Dylan) and P(plays Bob Dylan | plays the Beatles)?  
   C. What are P(plays Rihanna) and P(plays Rihanna | plays Kanye West)?  
   D. What is P(plays Queen or plays David Bowie)?  Hint: for this one you'll need to know the addition rule, $P(A \mbox{ or } B) = P(A) + P(B) - P(A,B)$.  

<!--chapter:end:03_counting.Rmd-->

# Plots

If you start reading deeply on the topic of [data visualization](https://en.wikipedia.org/wiki/Data_visualization), you'll encounter dozens, if not hundreds, of [different](https://plotly.com/ggplot2/geom_violin/) [types](https://plotly.com/ggplot2/geom_contour/) of [statistical plots](https://plotly.com/ggplot2/geom_tile/).   But in my opinion, there are only five basic plots that are truly _essential_ for a beginner to know: scatter plots,  line graphs, histograms, boxplots, and bar plots.  Collectively, these five basic plots cover a very broad range of data-science situations. 

In this lesson, you'll learn to:  

- understand the grammar of graphics. 
- create five basic plots:  
    1. Scatter plots, to show relationships among numerical variables.   
    2. Line graphs, to show change over time.  
    3. Histograms, to show data distributions.  
    4. Boxplots, to show between-group and within-group variation.  
    5. Bar plots, to show summary statistics (like counts, means, or proportions).  
 
- use faceting and aesthetic variation (e.g. color) to encode multivariate information.  
- customize plots by exercising finer, more advanced control over their visual details.  


## The grammar of graphics

Plots are like sentences: 

- we use them to tell stories.  
- it's easy to make basic ones, but it takes work to make really good ones.    
- most of the work in making really good ones lies not in creating but in _editing_: sharpening your meaning; adding important details while removing what's extraneous or distracting; and adding those little grace notes that elevate your creation from functional to beautiful.  

And there's one more unexpected way that plots are like sentences: they follow a __grammar__, which you might call the "grammar of graphics."  This grammar forms the foundation of some of the best plotting software out there, including [Plotly](https://plotly.com/), [Tableau](https://www.tableau.com/)---and [ggplot2](https://ggplot2.tidyverse.org/), the R library that we'll use in this lesson (and the lessons to come).  The grammar of graphics is much simpler than the grammar of a natural language, like English, but it's still a grammar.^[Technically the grammar of graphics is what's known to logicians as a [formal grammar.](https://en.wikipedia.org/wiki/Formal_grammar)  While the term "formal grammar" might connote unpleasant images of Teacher Thistlebottom scolding pupils for their split infinitives, formal grammars are typically much simpler than the grammars used by native speakers of natural languages.]

In fact, to make the analogy more explicit, consider a simplified version of English that I'll call "learner English."  Toddlers, for example, often say things like "Squirrel climbs tree" or "Jimmy eats apple" as they're learning to talk.  These sentences may lack the articles, prepositions, and other adornments of textbook English sentences.  But they're still comprehensible because they obey a simple, consistent grammar with these three elements: 

- a __subject__ that performs an action (squirrel, Jimmy).   
- an __object__ that receives the action (tree, apple).  
- a __verb__, i.e. the action itself (climbs, eats).    

Statistical plots also obey a simple, consistent grammar.  Like "learner English," this grammar also has three basic elements:  

- __variables__ in a data set (like the weight or engine size of a vehicle).  These are like the subject of a sentence.  
- __objects__: specifically, geometric objects (like dot or lines or bars).  These are like the object of a sentence.  
- __mappings__ from the data variables to aesthetic properties of the geometric objects (like their size, location, or color).  These are like the verb in a sentence.  

If you swap out different subjects/objects/verbs in our basic "learner English" grammar, you can generate lots of different sentences whose basic structure is familiar to any parent of a toddler ("I throw Cheerios," "Daddy needs sleep," and so on).  Similarly, if you swap out different variables/geometric objects/mappings in the same _graphical_ grammar, you can generate lots of different plots.

#### An example {-}

Here's an example to make this idea explicit.   Below you see the first several rows of a data set on median hourly demand for bike-share rentals in Washington, DC's Capital Bikeshare program, stratified by whether it's a working day (1) or not (0):

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
bikeshare = read.csv('data/bikeshare.csv')
bike_summary = bikeshare %>%
  group_by(hr, workingday) %>%
  summarize(median_rentals = median(total))
bike_summary %>% round(0) %>% head(6) %>% knitr::kable()
```

So, for example, during the 2 AM hour on working days, the median demand was 7 bikes, whereas during that same hour on weekends and holidays, the median demand was 54 bikes.

Now let's visualize this data using a specific type of plot, called a __line graph__: 

```{r bike-lines, echo=FALSE,  out.width="85%", fig.align='center', message=FALSE, fig.cap = 'A line graph.', fig.asp = 0.5}
ggplot(bike_summary, aes(x=hr, y=median_rentals, color=factor(workingday))) + 
  geom_line() + geom_point() + 
  labs(x="Hour of day (0 = midnight)", y="Median bike rentals",
       title="Bike-share rentals in Washington, DC (2011-12)") + 
  scale_x_continuous(breaks=0:23) + 
  scale_color_discrete(name = "Working day?", labels = c("No", "Yes"))
```

Remember, a plot tells a story.  The story here is fairly intuitive: working days show sharp peaks in bike-rental demand during morning and evening rush hours, while non-working days have a broader plateau across the middle of the day, along with notably higher demand [during the wee hours](https://www.washingtonpost.com/news/tripping/wp/2017/04/12/dont-drink-and-walk-or-bike/).

Let's unpack the grammatical elements of this plot.

- The __variables__ are the hour of the day, the median bike rentals for that hour, and whether the data point in question falls on a working day.  
- The __geometric objects__ are the dots, as well as the lines that connect those dots. 
- The __aesthetic mappings__ are what connect, or "map," the variables to visual properties of the geometric objects, as follows:   
   - the hour of the day (`hr`) is mapped to the horizontal (x) location of each dot.   
   - the median demand for that hour (`median_rentals`) is mapped to the vertical (y) location of each dot.
   - the categorical variable `workingday` is mapped to the color of each dot, as well as the color of the lines that connect those dots.  


Let's try one more plot, where we use the same variables, but change the geometric objects and the aesthetic mappings.  The result is an entirely different plot of the same basic data, called a __bar plot__.  This bar plot happens to have two different _facets_ or panels:  

```{r bike-bars, echo=FALSE, out.width="85%", fig.align='center', message=FALSE, fig.asp = 0.5, fig.cap = 'A faceted bar plot.'}
ggplot(bike_summary, aes(x=hr, y=median_rentals)) + 
  geom_col() + facet_wrap(~workingday, labeller = label_both) + 
  labs(x="Hour of day (0 = midnight)", y="Median bike rentals",
       title="Bike-share rentals in Washington, DC (2011-12)") + 
  scale_x_continuous(breaks=0:23) + theme_bw(base_size=9)
```

Let's unpack the grammatical elements of this new plot.

- The __variables__ are unchanged: hour of the day, median rentals, and whether it's a working day.  
- The __geometric objects__ are the bars within each facet, as well as the facets themselves (left vs. right).  
- The __aesthetic mappings__ are as follows:
   -  `workingday` is mapped to the two facets of the plot, left for 0 (no) and right for 1 (yes).    
   - the hour of the day (`hr`) is mapped to the horizontal (x) location of bar.    
   - the median demand for that hour (`median_rentals`) is mapped to the height of each bar.  
   

So there you have it: same grammar, but different combinations of the basic grammatical elements, and therefore different plots.

The rest of this lesson is about how to use R to make plots like these, and many more.  As you might guess from the "gg" in its from its name, `ggplot2` (the R library we'll use) relies heavily on the grammar of graphics.  


## The five basic plots

Start by loading the `tidyverse`, which will load the `ggplot2` library automatically behind the scenes:  

```{r, message=FALSE}
library(tidyverse)  
```

You'll also want to download several data sets:  

- [tvshows.csv](data/tvshows.csv), which contains data on 40 television shows, and which you already saw in our lesson on [Importing a data set].  
- [power_christmas2015.csv](data/power_christmas2015.csv), hourly data on electricity consumption in Texas on December 25, 2015.    
- [rapidcity.csv](data/rapidcity.csv), daily average temperatures in Rapid City, South Dakota from 1995 to 2011.  
- [kroger.csv](data/kroger.csv), data on sales of packaged, sliced [American cheese](https://www.seriouseats.com/2016/07/whats-really-in-american-cheese.html) at Kroger's grocery stores in 11 different cities.  
- [car_class_summaries.csv](data/car_class_summaries.csv), summary statistics on six different classes of vehicle (sedans, SUVs, etc).  


### Scatter plots {-}

```{r, echo=FALSE, message=FALSE}
tvshows = read.csv('data/tvshows.csv', header=TRUE)
```

__Scatter plots__ are built out of the simplest geometric object of all: points.  They're great for __visualizing the relationship between two numerical variables.__ 

Let's see an example, revisiting the [tvshows](data/tvshows.csv) data you saw in [this example analysis](#sec_example_analysis) awhile ago.  The two variables we'll plot are each TV show's `GRP`, which measures ratings/audience size, and its `PE`, which stands for "predicted engagement" and which measures how closely viewers are paying attention to the show.  Here's the basic `ggplot2` code for a scatter plot of these two variables.  

```{r tvshows-basic-plot,  out.width="85%", fig.align='center'}
ggplot(tvshows) + 
  geom_point(aes(x=GRP, y=PE))
```

It seems that shows with larger audiences also tend to have more _engaged_ audiences, on average.  

Let's go line by line to understand what's happening in this code block.  Here's the code again, without the plot, so you don't have to keep scrolling back up:

```{r, eval=FALSE}
ggplot(tvshows) + 
  geom_point(aes(x=GRP, y=PE))
```

The first line, `ggplot(tvshows)`, tells R that you're going to be plotting data from the `tvshows` data set.  All plots in these lessons will begin with a similar line: `ggplot(name_of_data_set)`.

The second line, `geom_point(aes(x=GRP, y=PE))`, is our first actual "sentence" in the grammar of graphics, and it's where the real action is.  `geom_point` means that the type of __geometric objects__ we're using in our plot are __points__ (where each point is a TV show).  Inside the parentheses of `geom_point`, `aes` specifies the __mapping__ between data __variables__ and aesthetic properties of the __points__, as follows:   

   - a TV show's `GRP` gets mapped to the $x$ coordinate of that show's corresponding point.  
   - a TV show's `PE` gets mapped to the $y$ coordinate of the point.  

Notice that the two lines are separated by a `+` symbol.  Don't think of this as mathematical addition, but rather as a punctuation mark in the grammar of graphics that means "add a layer."  All plots in ggplot are built up in layers using `+`, just like paragraphs are built up in sentences.  The base layer is always the data set itself, here constructed via `ggplot(tvshows)`.


#### Beyond two variables: color, shape, etc. {-}

To build your intuition about aesthetic mappings, let's try adding some further detail to this scatter plot.  Specifically, we'll add a third aesthetic mapping, from a show's `Genre` to the color of the point.   This is just one of several techniques we can employ to encode more than two variables within the constraints of a flat screen or page:

```{r tvshows-plot-color,  out.width="85%", fig.align='center'}
ggplot(tvshows) + 
  geom_point(aes(x=GRP, y=PE, color=Genre))
```

You've actually [seen this plot before](#sec_example_analysis), in "mimic first, understand later" mode---and we're now in a position to understand the code that produced it.  It's still a scatter plot (hence `geom_point`), but the aesthetic mapping now involves three variables:  

   - `GRP` gets mapped to the $x$ coordinate of the point.  
   - `PE` gets mapped to the $y$ coordinate of the point.  
   - `Genre` gets mapped to the color of the point.  

It turns out that `color` isn't our only option for encoding the information in the `Genre` variable.  For example, here we map `Genre` to the shape of each point:  

```{r tvshows-plot-shape, out.width="85%", fig.align='center'}
ggplot(tvshows) + 
  geom_point(aes(x=GRP, y=PE, shape=Genre))
```

Do you find this easier or harder to interpret than the color-based plot?  

There are still more options beyond `color` and `shape.`  Try, for example, mapping the `Genre` variable to one of these:

- `size`
- `alpha` (transparency)

These are less successful for encoding the information in a discrete variable like `Genre`.  But they might be really useful for encoding the information in a continuous numerical variable, like a TV show's production budget or its critical rating on [IMDb](https://www.imdb.com/chart/toptv/).  


#### Beyond two variables: faceting {-}

Let's briefly discuss the concept of __faceting__, which is another effective strategy for showing more than two variables on a flat screen or page.  Faceting means constructing multiple plots of the same fundamental type, with the same axes and scale, with each individual plot, called a _facet_, corresponding to some subset of the data.  (You already saw an example of a faceted bar plot in Figure \@ref(fig:bike-bars).)

Let's now see how the code works, continuing with our data set on TV shows:

```{r tvshows-plot-facet, out.width="100%", fig.asp = 0.4, fig.align='center'}
ggplot(tvshows) + 
  geom_point(aes(x=GRP, y=PE)) + 
  facet_wrap(~Genre)
```

As you can see, this plot contains three distinct scatter plots of `PE` versus `GRP`, one plot per genre.  The individual plots are referred to as _facets_; `Genre`, which defines the subsets of the data used for constructing the facets, is called the _faceting variable_. Faceting variables are always categorical (although you can also facet on a numerical variable if you discretize it first).  

Let's understand each element of the code block in terms of the grammar of graphics.  Here's the code block once more, so you don't have to scroll back up:

```{r, eval=FALSE}
ggplot(tvshows) + 
  geom_point(aes(x=GRP, y=PE)) + 
  facet_wrap(~Genre)
```

The first two lines will look familiar, but we'll cover them anyway for the sake of completeness.  

- The first line, `ggplot(tvshows)`, is the base or data layer.  It tells R where to look for the variables you'll be naming on subsequent lines.  
- The second line, `geom_point(aes(x=GRP, y=PE))`, tells R to make a scatter plot with each point as a TV show, with the following aesthetic mappings:  
   - `GRP` gets mapped to the $x$ coordinate.  
   - `PE` gets mapped to the $y$ coordinate.  
- The third line, `facet_wrap(~Genre)`, tells R to add a faceting layer to the plot, mapping the variable `Genre` to the individual facets. The tilde symbol (`~`) here means "by," as in "facet by genre."  (If you don't include the tilde in your command, you'll get an error.)

Faceting is a very general strategy, and it works for pretty much any kind of plot.  Below we'll see several examples of this strategy in action.  


### Line graphs  {-} 

If you connect points, you get lines, the second most basic geometric object.  The resulting plot is called a __line graph.__  Line graphs are useful for __showing change over time__, or more generally for showing change in a numerical variable ($y$) as a function of some sequentially ordered explanatory variable ($x$).  To make a line graph, you play "connect the dots"---that is, you connect the $(x,y)$ coordinates of your data points sequentially, in order of the $x$ variable.  

Begin by importing [power_christmas.csv](data/power_christmas.csv) into RStudio.  This data shows hourly electricity demand in Texas on December 25, 2015.  The two columns are:  

- `hour`, marking the beginning of a one-hour period, with 0 meaning midnight and 23 meaning 11:00 PM.  
- `ERCOT`, the peak instantaneous demand for electricity (in megawatts) on the Texas power grid during that one-hour period.  

```{r, echo=FALSE, message=FALSE}
power_christmas2015 = read.csv('data/power_christmas2015.csv', header=TRUE)
```

Here's how to make a line graph of electricity demand over the course of that particular Christmas day.  The code is almost the same as that for a scatter plot, except that we substitute `geom_line` for `geom_point`:  

```{r}
ggplot(power_christmas2015) + 
  geom_line(aes(x=hour, y=ERCOT))
```

Is that dip in power demand from 1 PM to 5 PM a result of millions of Texans turning off their TVs, dimming the lights, and gathering around the table for a mid-afternoon holiday meal?  Maybe, maybe not; below we'll examine how this data compares to data in other years and on other days, to see if this "dinner dip" is a consistent pattern unique to Christmas day, or whether it's something that happens on other days, too.  

Before we add complexity, however, let's interpret each bit of this basic code block through the grammar of graphics:

- `ggplot(power_christmas2015)` tells R that you're going to be plotting data from the `power_christmas2015` data set.  
- `+` means add a layer.  
- `geom_line` means that the type of __geometric objects__ we're using in our plot are __lines__.   
- `aes` specifies the __mapping__ between data __variables__ and aesthetic properties of the __lines__, as follows:   
   - `hour` gets mapped to the $x$ coordinate.     
   - `ERCOT` gets mapped to the $y$ coordinate.  
   
As you can see, both scatter plots (`geom_point`) and line graphs (`geom_line`) are built out of the $(x,y)$ coordinates of data points.  Scatter plots show those $(x,y)$ coordinates as points, whereas line graphs connect the points in the order of the $x$ variable.  If you ever find yourself in doubt about whether a scatter plot or a line graph is more appropriate, ask yourself a simple question:

> Does my $x$ variable have an inherent progression or sequential ordering, like the hours of the day?  

If the answer is no, a line graph almost certainly doesn't make sense; you're better off with a scatter plot.  But if the answer is yes, a line graph might be worth trying, to emphasize the sequential nature of the data.  


### Histograms {-}

```{r, echo=FALSE, message=FALSE}
rapidcity = read.csv('data/rapidcity.csv', header=TRUE)
```

A __histogram__ is used to __visualize the distribution__ of a single numerical variable ($x$).  Let's jump right in, using the [rapidcity.csv](data/rapidcity.csv) data, which you should import into RStudio.  The first six lines of the data set should look like this.  The `Temp` column, measured in degrees F, shows the average daily temperature, which is the midpoint between that day's high and low temps:

```{r}
head(rapidcity)
```

The plot below shows a histogram of the `Temp` variable:  

```{r hist-rapidcity1, message=FALSE, echo=FALSE, fig.cap="A histogram of daily average temperatures in Rapid City, SD."}
ggplot(rapidcity) + 
  geom_histogram(aes(x=Temp), binwidth=10, center=25, color='white') + 
  scale_x_continuous(breaks = seq(-30, 100, by=10))
```

A histogram like this is constructed by first chopping up the range of the $x$ variable (here, `Temp`) into a set of non-overlapping intervals, or __bins__, where each bin represents a specific range of values.  Then for each bin, we __count__ the number of data points within that bin's range, and we draw a bar whose height ($y$ position) marks the corresponding count.  

In the plot above, the bins are 10 degrees in width: 

- The height of the bar between 10 and 20 is about 300.  So there were about 300 days in the data set with temperatures between 10 and 20 degrees.  
- The height of the bar between 20 and 30 is about 600.  So there were about 600 days in the data set with temperatures between 20 and 30 degrees.  
- And so on.  

The resulting plot is useful for answering high-level questions about a data distribution, such as:  Where is the center of the distribution?   How spread out are the data points?   Does the data show "heavy tails", with a small number of data points very far from the center?  Is the distribution skewed?  In other words, does it fall off more quickly to one side of its peak than the other?  (The distribution above looks mildly skewed left, since it falls off less quickly on the left-hand side of the plot.)

The code to make a basic histogram in `ggplot2` looks like this, with just a single aesthetic mapping:

```{r, message=FALSE}
ggplot(rapidcity) + 
  geom_histogram(aes(x=Temp))
```

The $y$ aesthetic, the height of the bars, gets calculated automatically.  If we don't specify _how_ we want R to bin up the data, it will choose on its own, usually aiming for about 30 bins overall.  But we can also exercise finer control over the bin width, like this:

```{r}
ggplot(rapidcity) + 
  geom_histogram(aes(x=Temp), binwidth=1)
```

This tells R to pick bins of width 1 (so 49-50 degrees, 50-51 degrees, and so on).  Compare this with the [first histogram above](fig:hist-rapidcity1), which had bins of width 10.  Or even this one, with extra-wide 20-degree bins:

```{r}
ggplot(rapidcity) + 
  geom_histogram(aes(x=Temp), binwidth=20)
```

As you can see, the bin width has a dramatic effect on the overall look of a histogram.  Narrower bins show fine-scale detail, but also look spikier and noisier.  Wider bins obscure fine-scale detail, but generally look smoother.  There's no single best choice for all purposes.  I generally play around with the bin width until I find one that looks like a nice compromise between the extremes of "not showing enough relevant detail" and "overwhelming the viewer with superfluous detail."  This often entails drawing on prior knowledge of the problem.  On a data set like this, I might reason as follows.  

- Would I notice a temperature swing of 1 degree F?  I'm not a meteorologist or a [goldfish](https://www.jstor.org/stable/2458687), so I doubt it; 1-degree bins are probably needlessly narrow.  
- Would I notice a temperature swing of 10 degrees F?  Heck yes!  So 10-degree bins might be too coarse.  They'd obscure some relevant detail.  
- Would I notice a temperature swing of 2 or 3 degrees F?  Probably, but I'm actually not sure!  So a bin width of roughly 2 or 3 degrees seems pretty reasonable.  It's not obscuring big 10-degree differences in temperature within a single bin.  But it's also not making distinctions so needlessly fine that they don't actually matter to a person who just wants to know whether they need a t-shirt, a sweater, or a plane ticket to Texas.  

Let's try it:

```{r}
ggplot(rapidcity) + 
  geom_histogram(aes(x=Temp), binwidth=3)
```

This bin width seems like a nice middle ground.   


#### A faceted histogram {-}

We can also use the same strategy of faceting if we wish to plot multiple histograms, each corresponding to some subset of the data.  As above, this is easily accomplished by adding a `facet_wrap` layer to the base plot.  Here's a histogram of daily average temperatures in Rapid City, faceted by the `Month` variable:   

```{r, eval=FALSE}
ggplot(rapidcity) + 
  geom_histogram(aes(x=Temp), binwidth=3) + 
  facet_wrap(~Month)
```


```{r, echo=FALSE}
ggplot(rapidcity) + 
  geom_histogram(aes(x=Temp), binwidth=3) + 
  facet_wrap(~Month) + 
  theme(panel.spacing.x = unit(1.25, "lines"))
```


You can also tell R how how many rows you want in your faceted plot.  So, for example, if you wanted a really tall and skinny plot that lined up all the months vertically, you could create it with the following code block.  This is kind of a neat way to see seasonal changes in temperature:  

```{r, eval=FALSE}
ggplot(rapidcity) + 
  geom_histogram(aes(x=Temp), binwidth=3) + 
  facet_wrap(~Month, nrow=12) 
```


```{r, echo=FALSE, fig.asp = 2.5, out.width="35%", fig.align = "center"}
ggplot(rapidcity) + 
  geom_histogram(aes(x=Temp), binwidth=3) + 
  facet_wrap(~Month, nrow=12)  + 
  theme_grey(base_size = 16)
```



### Boxplots {-}

A __boxplot__, sometimes called a "box and whisker plot," is used for __comparing distributions.__  Specifically, it allows us to visualize the distribution of a numerical variable, stratified according to the values of a second categorical variable. In that sense, it plays a role very similar to that of a faceted histogram.  The difference between a boxplot and a histogram is that, while a histogram tries to show the full shape of a data distribution, a boxplot shows only a _summary_ of that distribution, in the form of a little cartoon with a box and whiskers.  

Let's see an example, using the [data on weekly cheese sales](data/kroger.csv) across 68 weeks at 11 different Kroger grocery stores.  Here are the first six lines of the file, via `head(kroger)`:  

```{r, echo=FALSE, message=FALSE}
kroger = read.csv('data/kroger.csv')
head(kroger)
```

Each row shows the sales volume (`vol`) at a Kroger store in a particular city over a particular week. Our goal is to examine the distribution of weekly sales volume, both within individual cities and across the different cities.  The boxplot below allows us to do just this:  

```{r, eval=FALSE}
ggplot(kroger) + 
  geom_boxplot(aes(x = city, y = vol))
```

```{r, echo=FALSE, message=FALSE, out.width="100%", fig.align="center", fig.asp = 0.5}
ggplot(kroger) + 
  geom_boxplot(aes(x = city, y = vol)) + 
  theme_grey(base_size = 10) + ylim(0, 15000)
```

Each box represents the distribution of weekly cheese sales at Kroger store within a single city.  Since there are 11 different boxes, we're looking at 11 different data distributions, one distribution per city.  Each box encodes a five-number summary for that particular Kroger's sales volume.  Let's focus on Columbus as an example.  The data distribution of weekly sales volume in Columbus has the following properties:  

- Minimum: 2149 packages of cheese
- First quartile (25th percentile): 2958 packages
- Median (50th percentile): 3670 packages
- Third quartile (75th percentile): 5105 packages
- Maximum: 13588 packages

Now let's see how this information is encoded in the box for Columbus.

```{r, echo=FALSE, out.width="75%", fig.align='center'}
knitr::include_graphics("images/boxplot_annotated.png")
```

As you can see, the box spans the middle 50% of the data distribution, while the whiskers show what happens in the top and bottom quartiles.  The whiskers, however, don't necessarily go all the way to the minimum and maximum values.  By default, they extend only 1.5 times the height of the box itself, with points falling outside this range shown individually as dots.  (There's a temptation to refer to these points as "outliers," but that's a loaded term, so we'll avoid it for now.)

With one box, we can ask questions like: where is the data distribution centered?  How spread out is it around that center?  With multiple boxes, we can compare the groups, asking questions like: how far apart are the group averages (medians)?  How do these between-group differences compare in magnitude to the within-group variation (that is, the spread of the individual boxes themselves)?  In fact, that's the most common use case for a box plot: to understand between-group differences placed in the context of the within-group variation.  

To illustrate this idea, here's the boxplot again:

```{r, echo=FALSE, message=FALSE, out.width="100%", fig.align="center", fig.asp = 0.5}
ggplot(kroger) + 
  geom_boxplot(aes(x = city, y = vol)) + 
  theme_grey(base_size = 10) + ylim(0, 15000)
```

We can use this plot to make a few example statements about within-group and between-group variation:

- Columbus has higher average weekly sales than Dallas.  But in context, that difference looks pretty small: the difference in their averages looks much smaller than the typical week-to-week variation in sales within either city individually.   Said another way, the difference between a bad sales week (25th percentile) and a good sales week (75th percentile) in Dallas is _much larger_ than the difference between an average sales week in Columbus and an average sales week in Dallas.  
- Houston has higher average weekly sales than Detroit.  Moreover, in context, this difference looks pretty large: in fact, the difference in their averages looks larger than the typical week-to-week variation in sales within either city individually.  Said another way, the difference between a bad sales week (25th percentile) and a good sales week (75th percentile) in Detroit is _much smaller_ than than the difference between an average sales week in Detroit and an average sales week in Houston.    
- Nashville has higher average weekly sales than Louisville.  This difference between their averages looks to be about the same size as the typical week-to-week variation in either Louisville or Nashville.  Said another way, the difference between a bad sales week (25th percentile) and a good sales week (75th percentile) in Louisville is _about the same size_ as the difference between an average sales week in Louisville and an average sales week in Nashville.  

The basic principle is: _within-group variation_ is a really valuable yardstick for reasoning about the magnitude of group differences.  Boxplots make both the within-group variation and the between-group differences readily apparent.  

A boxplot always has a numerical $y$ variable (here, sales volume) and a categorical $x$ variable (here, city).  I suppose you could make a boxplot with just one group (and therefore no $x$ variable), but in that case, you're probably better off with a histogram.

Let's look once again at the code to make a boxplot, so we can unpack it piece by piece:

```{r, eval=FALSE}
ggplot(kroger) + 
  geom_boxplot(aes(x = city, y = vol))
```

This code says that:  

- the geometric objects in our plot are boxes with whiskers.  
- the `city` variable gets mapped to the location of the box itself, along the $x$ axis  
- the `vol` variable gets used to calculate summary statistics (medians/quartiles/min/max), which are mapped to the $y$-axis heights of each box (and whiskers).   

You might ask: when is it reasonable to use a faceted histogram versus a boxplot?  To some extent this is a matter of preference, but here's the guideline I generally follow.  I tend to use faceted histograms when I want to compare distributions for a small number of groups, because histograms offer more high-resolution detail.  On the other hand, I tend to use boxplots when I want to compare distributions for a large number of groups, where too much detail about each individual distribution might be overwhelming.  

For example, here's a boxplot of cheese sales from a much larger data set encompassing 88 different stores, arranged top to bottom by median sales.  You might argue that this is too much information in a single plot.  Maybe so! The point is, using a faceted histogram here would _certainly_ be too much.  

```{r, echo=FALSE, message=FALSE, out.width="65%", fig.asp = 2, fig.align='center'}
cheese= read.csv('data/cheese.csv', header=TRUE)
cheese = mutate(cheese, store = stringr::str_to_title(store))

ggplot(cheese) + 
  geom_boxplot(aes(x=reorder(store, vol, FUN = 'median'), y=vol)) + 
  scale_y_log10() + 
  labs(y = "Sales volume (log scale)", x = "Store") + 
  coord_flip() 
```

### Bar plots {-}

__Bar plots__ are typically used to __visualize group-level summary statistics__, like counts, averages, or proportions.  Thus unlike any of our previous plots, the typical workflow to make a bar plot actually has two distinct stages:  

(1) __Summary stage__: split your data set into subgroups and calculate summary statistics for each subgroup.  
(2) __Plotting stage__: make a bar plot of those summary statistics, one bar per group.  

```{r, echo=FALSE, message=FALSE}
car_class_summaries = read.csv("data/car_class_summaries.csv", header=TRUE)
```

In this section, we're going to assume that stage 1 (calculating summary statistics), has already been accomplished, and that you're starting from stage 2 (plotting).  In a later series of lessons, we'll show how to deal with stage 1 itself.  

To illustrate, we'll use [car_class_summaries.csv](data/car_class_summaries.csv), which contain simple summary statistics on seven different classes of vehicle.  Go ahead and import this data set into RStudio, making sure that R is aware that the file has a header row.  Here it is:  

```{r}
car_class_summaries
```

This data set has only seven lines in it---remember, it's a set of summary statistics about entire _classes_ of vehicle, rather than data on individual vehicles.  The four variables are:  

- `class`: the class of vehicle  
- `n`: the number of vehicles in that class in the original data set from which these summary statistics were derived  
- `average_cty`: the average city gas mileage (miles per gallon) of cars within that class  
- `prop_4cyl`: the proportion of cars within that class having a four-cylinder engine.  (Four-cylinder engines are generally smaller engines, with larger vehicles often having six- or even eight-cylinder engines.)  

To make a bar plot in R, we use `geom_col`, as follows.  Here's a bar plot of average city gas mileage across vehicle classes:

```{r}
ggplot(car_class_summaries) + 
  geom_col(aes(x=class, y=average_cty))
```

We see seven bars/columns, one per vehicle class.  The `average_cty` variable for that class is mapped to the height ($y$) of the bar.  

Let's see a second example, where we plot the proportion of four-cylinder engines in each vehicle class.  This entails changing the aesthetic mapping to use `prop_4cyl` for the $y$ variable:

```{r}
ggplot(car_class_summaries) + 
  geom_col(aes(x=class, y=prop_4cyl))
```

In both of these examples, the group-level summaries were "pre-computed" in the table we imported, in `car_class_summaries.csv`.  Later on, in the lesson on [Data Wrangling], we'll see how to start from a "raw" data set and calculate these summary statistics ourselves.  


#### What about `geom_bar`? {-}

We've seen how to make bar plots using `geom_col`, assuming we have a table of pre-computed summary statistics, one row per group.  Confusingly, there's also a function in `ggplot2` called `geom_bar`.  It sounds like _that_ should be for a bar plot, right?  So what's the difference?  

It's actually pretty simple:

- If you want to make a bar plot of counts, __and__ you need R to do the counting for you, use `geom_bar`.  
- For pretty much any other bar plot, use `geom_col`.  

To illustrate, consider the `mpg` data set that comes pre-packaged with the `tidyverse`.  Assuming you've already loaded the tidyverse, you can load this data set using the `data` command, like this:

```{r}
data(mpg)
```

Let's look at a random sample of ten lines of this data set:

```{r, echo=FALSE, message=FALSE}
mosaic::sample(mpg, 10, orig.id=FALSE) %>%
  knitr::kable() %>%
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = 8)
```

As you can see, each row is a vehicle, and one of the variables is `class`.  In fact, this was the original _vehicle-level_ data set from which the _class-level_ summary statistics I gave you in `car_class_summaries.csv` were derived.  (Again, we'll see how that process works in the lesson on [Data Wrangling].)

We can now compare `geom_col` with `geom_bar`.  Here is bar plot of class counts, based on the "derived" or summary variable `n` in `car_class_summaries`:

```{r}
ggplot(car_class_summaries) + 
  geom_col(aes(x=class, y=n))
```

This tells you have many vehicles were in each class, by explicitly mapping the summary variable `n` to the height of the bars ($y$).

Now here's the _exact same bar plot_, but based on the original `mpg` data set:

```{r}
ggplot(mpg) + 
  geom_bar(aes(x=class))
```

Notice that we use the `mpg` data, and that there's no $y$ aesthetic.  This example illustrates the difference between `geom_col` and `geom_bar`:  

- with `geom_col`, you need to supply a $y$ aesthetic for the heights of the bars.  This is usually derived from a table of summary statistics. It could be a count, a mean, a proportion---anything.   
- with `geom_bar`, you don't supply a $y$ aesthetic for the heights of the bars.  R calculates this for you, by counting how many cases fall into each group.  

As you can see, `geom_bar` only makes sense if you want a quick bar plot of how many cases fall into each class.  Virtually all other use cases for bar plots require `geom_col`, which is much more generally useful.   Personally, I use `geom_col` approximately 100% of the time.  Depending on the data sets you work with, your mileage may vary.  

## Customizing plots  

`ggplot2` offers near infinite possibilities for customizing your plots.  If you can Google it, you can probably make it happen in ggplot, but I can't promise the code will be pretty!  (Unfortunately, with great power comes great complexity.)  In this section, we'll discuss a few of the most basic and important customization options. 

In the beginning, these options may have the feel of mystical incantations you have to utter to get your plots to behave the right way.  That's OK, even normal; eventually you'll start to build muscle memory for `ggplot2`.  My advice is not to try to memorize anything.  If you want to customize a plot, just start from the code blocks below as templates, and copy and paste the relevant bits of customization code into your plot, changing the names of the data and variables as appropriate. 


### Changing titles and labels {-}  

It's easy to customize the title and axis labels of a plot.  You do this by adding a `labs` layer to an existing plot.  To illustrate, let's go back to the data on weekly cheese sales in Kroger stores:

Here was our basic box plot, with default axis labels and no title:

```{r}
ggplot(kroger) + 
  geom_boxplot(aes(x = city, y = vol))
```

And here's the same plot, but with some nicer labels:  

```{r}
ggplot(kroger) + 
  geom_boxplot(aes(x = city, y = vol)) + 
  labs(x="Location of store",
       y="Weekly sales volume (packages sold)",
       title="Weekly cheese sales at 11 U.S. Kroger stores")
```


### Color scales {-}

Color is a fascinating, rich topic at the intersection of physics, neuroscience, and art. We can't hope to cover even a tiny fraction of what there is to say about color here.  Suffice it to say that in R, you can [geek out as much as you want](https://github.com/karthik/wesanderson) with custom color palettes for your plots.

The one color-related topic we'll discuss seriously is the creation of color-blind friendly plots.  Take, for example, this plot from above:

```{r,  out.width="85%", fig.asp = 0.6, fig.align='center'}
ggplot(tvshows) + 
  geom_point(aes(x=GRP, y=PE, color=Genre))
```

Not so great for color-blind viewers, for whom reds and greens can be challenging.  But this next version is much friendlier.

```{r,  out.width="85%", fig.asp = 0.6, fig.align='center'}
ggplot(tvshows) + 
  geom_point(aes(x=GRP, y=PE, color=Genre)) + 
  scale_color_brewer(type="qual")
```

This adds a "color-scale" layer to the basic scatter plot, thereby changing how R encodes each genre in a color.  Specifically, these are [Color Brewer](https://colorbrewer2.org/#type=qualitative&scheme=Accent&n=3) colors, a palette put together by an [expert in visibility and color theory named Cynthia Brewer.](https://en.wikipedia.org/wiki/Cynthia_Brewer)   Our command is telling R that the type of color scale we want (`qual`) is one appropriate for qualitative (that is, categorical) variables.  

Another option is to install `viridis`, [another library of color palettes](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html) that are specifically designed to be easier to read by those with colorblindness.  (They also print well in grayscale.)

```{r,   out.width="85%", fig.asp = 0.6, fig.align='center', message=FALSE}
library(viridis)  

ggplot(tvshows) + 
  geom_point(aes(x=GRP, y=PE, color=Genre)) + 
  scale_color_viridis(discrete=TRUE)
```

The layer `scale_color_viridis(discrete=TRUE)` tells R to use the `viridis` color scale for showing the individual genres (which is a discrete categorical variable, hence `discrete=TRUE`).  This is another good, research-backed  option when you want to use color to encode multivariate information in a plot and want to maximize accessibility.  



### Font size {-}

It's a little bit tedious to change font sizes in `ggplot2`.  But this is something we probably want to do in our boxplot above, because the names of the stores are running together along the $x$ axis.  

In `ggplot2`, font sizes are governed by adding a `theme` layer.  Here we are making the font smaller, to avoid overlapping text.

```{r}
ggplot(kroger) + 
  geom_boxplot(aes(x = city, y = vol)) + 
  labs(x="Location of Kroger store",
       y="Weekly sales volume (packages sold)",
       title="Weekly cheese sales at 11 U.S. Kroger stores") + 
  theme(axis.text = element_text(size = 8)) 
```

`theme(axis.text = element_text(size = 8))` is not the prettiest or most intuitive command in the world, but it works.  You can also change font sizes for other aspects of the plot, including axis labels and titles.  [This reference](https://statisticsglobe.com/change-font-size-of-ggplot2-plot-in-r-axis-text-main-title-legend) shows a lot of these other customization options, which follow a very similar template to the example above.    


### Flipping the $x$ and $y$ axes {-}

You can flip the coordinate axes in any `ggplot` using `coord_flip`:

```{r}
ggplot(kroger) + 
  geom_boxplot(aes(x = city, y = vol)) + 
  labs(x="Location of Kroger store",
       y="Weekly sales volume (packages sold)",
       title="Weekly cheese sales at 11 U.S. Kroger stores") + 
  coord_flip()
```

This is a perfectly fine solution to the problem of overlapping text in our previous boxplot, but for some data sets you might prefer horizontal boxes/bars/whatever for their own sake.  Here's a second example, based on our bar plot of vehicle mileage vs. class:  

```{r}
ggplot(car_class_summaries) + 
  geom_col(aes(x=class, y=average_cty)) + 
  coord_flip()
```



## Plotting cheat sheet {-}

In the table below, $x$ refers to an explanatory variable and $y$ refers to a response variable (i.e. the variable whose behavior is being "explained" by $x$).  

| If you have...      | Consider a... |
| :----------- | :----------- |
| Numerical $y$ and numerical $x$      | Scatter plot of $y$ vs $x$      |
| Numerical $y$ and sequential $x$      | Line graph of $y$ vs $x$|
| Numerical $y$ alone (no $x$)    | Histogram of $x$ |
| Numerical $y$ and categorical $x$   | Boxplot or faceted histogram of $y$ vs $x$       |
| Summary statistics by group   | Bar plot        |
| More than two explanatory variables   | Plot that uses faceting or additional aesthetic mappings (color/shape/etc.)        |
| A plot that encodes information via color  | Colorblind-friendly color scale        |



## Study questions {-}

1. Subjects, objects, and verbs are three key elements in the grammar of English.  What are the three key elements in the grammar of graphics?  

2. In the section on "[The grammar of graphics]", you saw two examples of a plots: a line graph and a bar plot.  Both were plots of the same data, and both had the same goal: to compare bike-rental demand across the day and for working vs. non-working days.  Do you think one plot is more effective than the other at making that comparison.  Or do you think they're about equally effective?  Explain your reasoning.

3. Recall this code block, that created a scatter plot of `PE` vs. `GRP` for television shows:

    ```
      ggplot(tvshows) + 
         geom_point(aes(x=GRP, y=PE))
    ```
Now download the [gapminder2007](data/gapminder2007.csv) data, originally from [www.gapminder.org](https://www.gapminder.org/), which contains human development data on 142 countries in 2007.   
   (A) Start from the code block above as a template.  Modify the code to produce a scatter plot of a country's life expectancy (`lifeExp`, $y$ variable) versus its per-capita GDP (`gdpPercap`, $x$ variable).  This should entail changing the data set name and the variable names, leaving the other elements of the code block as-is.  
   (B) Starting from the plot in (A), modify your scatter plot of `lifeExp` vs. `gdpPercap` so that the `continent` variable gets mapped to the color of each point.   
   (C) Starting from the plot in (B), modify your plot so that the population variable (`pop`) gets mapped to the size of each point.   Your points now encode four pieces of information about each country: GDP, life expectancy, continent, and population.  Can you guess which points correspond to China, India, and the United States?   

4. Something here,.

<!--chapter:end:04_plots.Rmd-->

# Summaries

In this lesson, you'll learn to calculate basic summary statistics, including:  

- means and medians to measure the typical value in a data set.  
- standard deviations and IQRs to measure variation from the typical value.   
- min, max, and, and quantiles to measure the extremes.   
- $z$-scores, to make standardized comparisons of fundamentally different types of quantities.  

To get started, create a fresh new script, and then load the `tidyverse` library by placing the following line at the top of your script and running it in the console:

```{r, message=FALSE}
library(tidyverse)
```  

Please also import the [rapidcity.csv](data/rapidcity.csv) data set, which you saw in the previous lesson, and which contains daily average temperatures in Fahrenheit (the `Temp` column) in Rapid City, SD over many years.  My code below assumes that you've imported the data set into an object called `rapidcity`, which will be the default object name if you used the `Import Dataset` button.  If you need a refresher on importing data, see [here](#importing_data).

```{r, echo=FALSE}
rapidcity = read.csv('data/rapidcity.csv', header=TRUE)
```

The `Temp` column, measured in degrees F, shows the average daily temperature, which is the midpoint between that day's high and low temps.  Let's begin by re-examining the histogram of daily temperatures that we saw in a prior lesson.   

```{r, out.width = "90%"}
ggplot(rapidcity) + 
  geom_histogram(aes(x=Temp), binwidth=2)
```

[Brrr!](https://www.fodors.com/wp-content/uploads/2019/09/RapidCityWinter__HERO_shutterstock_1164246820.jpg)

Statistical summaries entail distilling a complex data distribution like this one into a few simple numbers that answer questions like these.    

- _the typical value_:  what's the temperature on a typical day in Rapid City? (`mean` and `median`)     
- _the variation_: how much do the individual days vary from a "typical" day?  (`sd` and `IQR`)
- _the extremes_: what temperatures should we expect on days that are unusually hot or unusually cold (at least, for Rapid City)? (`min`, `max`, and `quantile`)  

We'll step through the Rapid City temperature data one summary at a time, building up a list of statistics bit by bit.  

## The typical value 

Two conventional ways to measure the typical value in a data set are the [mean and the median](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-median-mode/).

There are many ways we can calculate a mean in R.  The way I'm going to show isn't actually the most concise, at least for a simple summary like the mean.  But it _is_ very transparent, and it also has the virtue of generalizing much more readily to situations where we want to calculate more complex summaries.  

Here's my preferred way in R to calculate a mean.  This uses pipes (`%>%`); if you need a refresher on pipes, revisit [this lesson](#simple_probabilities).

```{r}
rapidcity %>%
  summarize(avg_temp = mean(Temp))
```

So about 47.3 degrees F.  Let's look at this statement piece by piece:  

- we start by piping (`%>%`) the `rapidcity` data set into the `summarize` function, which---as its name suggests---is the function in R for computing data summaries.  
- inside the parentheses of the `summarize` function, we have an assignment statement, `avg_temp = mean(Temp)`...  
   - The right-hand side of this statement, `mean(Temp)`, tells R what summary statistic to calculate.  Here we calculate the mean of the `Temp` variable.  
   - The left-hand side of the statement, `avg_temp`, is what _name_ we want to give this summary.  

We can calculate the median, or the middle value in a sample, in a very similar way.  Let's augment our summary pipeline to calculate both the mean and median, so we can compare them:  

```{r}
rapidcity %>%
  summarize(avg_temp = mean(Temp),
            median_temp = median(Temp))
```

As you can see from this example, the `summarize` function lets you calculate as many summaries as you want, separated by commas.  You don't have to put each summary on a separate line the way I have, nor do you need to indent them so that they all line up.  But I find that doing so makes for more readable code.^[Good coders use a combination of strategies to make their code more readable, including line breaks, indentation, informative object names, and commenting.]  Here the mean and median are pretty similar.  This will generally be the case when data distribution isn't too [skewed](https://www.statisticshowto.com/probability-and-statistics/skewed-distribution/), i.e. unbalanced to the left or right.

On the other hand, for distributions that are highly skewed one way or another, or that contain large outliers, the mean and median can be quite different.  In those situations, the median is generally considered a more representative measure of what's "typical," because it is less affected by heavy tails and outliers.

Consider, for example, the distribution of U.S. household income in 2010, which is heavily skewed to the right:

```{r, echo=FALSE, fig.align='center', out.width="100%"}
knitr::include_graphics('images/income_census_2014.png')
```

The median of this distribution is about \$50,000: half of U.S. households earned more, and half earned less.  But the mean is much higher---nearly \$100,000---owing both to the overall skewness as well as the long upper tail of the distribution, consisting of a small number of very high earners (the ["one percent,"](https://review.chicagobooth.edu/economics/2017/article/never-mind-1-percent-lets-talk-about-001-percent) or, at least in this figure, the four percent shown at the far right).  Because income distributions tend to be highly skewed and contain huge outliers (like Jeff Bezos or Bill Gates), most economists prefer the median to the mean as a way to measure the "typical" household income.  

## Variation 

Another important question is, "How spread out are the data points from the typical value?"  As we discussed in the lesson on [Plots], this variation _around_ the average is usually at least as interesting as the average itself.  In fact, I'd encourage you imagine that this full spectrum of variation _is_ the underlying reality, and that any notion of a "typical" or "average" data value---like the notion of an "average American," with 12.2 years of schooling, 0.9 cars, 0.99 testicles, and 1.01 ovaries---is just an abstraction of that reality.^[The "average American" has more ovaries than testicles because there are slighty more females than males.  That's because females live a bit longer than males, on average.]  

In our example, a "typical day" in Rapid City is about 47.3 degrees.  But this typical day is actually quite atypical. Some days are much hotter than average; some are much colder; some are _about_ average, but not quite.  The number of days that are _exactly_ average is basically zero.  __Temperature varies__.   If we want to understand reality, we need the measure that variation.  We'll look at two ways to do this: the __standard deviation__ and the __inter-quartile range__.  

One common way to measure variation is with the __standard deviation__.  The standard deviation answers the question: "by how much do the _actual_ data points tend to deviate from the notionally _typical_ data point?"  The higher the number, the more spread out the data points are around their average.  

Let's add the standard deviation to our list of summary statistics that we want to calculate.  To do so we use the `sd` function, like this:

```{r}
rapidcity %>%
  summarize(avg_temp = mean(Temp),
            median_temp = median(Temp),
            sd_temp = sd(Temp))
```

In words: the actual days in Rapid City vary from the typical day by about 20 degrees, on average.

To give you a sense of what the standard deviation represents, consider the following table.  This shows ten randomly sampled rows of our data set, along with a new column called `Temp_deviation`.  In this column, I've calculated the daily deviation: that is, the _difference_ between each day's actual temperature and the overall average temperature of 47.3 degrees.

```{r, echo=FALSE, cache=TRUE}
ind = c(547, 698, 1434, 1814, 2449, 3111, 4462, 4516, 5166, 5852)
getter = function(x, ind) return(x[ind,])
rapidcity %>% 
  mutate(Temp_deviation = round(Temp - mean(Temp), 1)) %>%
  getter(ind) %>%
#rapidcity[ind,] %>%
  arrange(Year, Month, Day) %>%
  knitr::kable()
```

For example:  

- The first row (30 June 1996) shows a temperature of 68.2 degrees F.  This is 20.9 degrees above average, and so the `Temp_deviation` column shows 20.9.     
- The second row (30 November 1997) shows a temperature of 28.4 degrees F.  This is 18.9 degrees below average, and so the `Temp_deviation` column shows -18.9.    
- And so on for the remaining rows.  

Some of these deviations are positive, some are negative; some are large, some are small.  What the standard deviation measures is, essentially, the average _magnitude_ of these deviations---not just for these 10 rows, but for all 6159 rows of the data set. Said another way, it measures the typical deviation from the average!^[This is not quite mathematically precise.  The trick in defining the standard deviation is how you ensure the negative and positive deviations don't cancel each other out in calculating the average. For the formal definition, see pretty much any [online tutorial](https://www.mathsisfun.com/data/standard-deviation-formulas.html) on the [concept.](https://www.investopedia.com/terms/s/standarddeviation.asp)]  

A second conventional measure of variation is called the __inter-quartile range__, or IQR.  This measures the range spanned by the middle half of a data distribution---that is, from the 25th to the 75th percentiles.  Let's add the IQR to our summary table:

```{r}
rapidcity %>%
  summarize(avg_temp = mean(Temp),
            median_temp = median(Temp),
            sd_temp = sd(Temp),
            iqr_temp = IQR(Temp))
```

So in Rapid City, the range from the 25th to the 75th percentile of temperature spans about 31 degrees F.  

__Some simple advice.__ Like the median, the IQR is more robust to outliers and heavily skewed data distributions.  That's because it depends only on the middle half of the data, and not what happens in the top or bottom quartile.  (If you take a room of 100 school teachers and add Jeff Bezos, the mean and standard deviation of income will change a lot, but the median and IQR will hardly budge.)  So if your data distribution is highly skewed or has big outliers in it, you're usually better off summarizing it with the median and IQR.  If not, you're usually better off with the mean and standard deviation.  

## Extremes and quantiles 

Another conventional way to summarize a data distribution is to look at the extremes.  This is common, for example, in the insurance industry, where they ask questions like, "what's the worst flood or worst earthquake we might see in a 100-year period?"

The simplest measures of the extremes are the minimum and maximum values we've seen before.  Let's add the calculation of these numbers to our summary pipeline, using `min` and `max`:  

```{r}
rapidcity %>%
  summarize(avg_temp = mean(Temp),
            median_temp = median(Temp),
            sd_temp = sd(Temp),
            iqr_temp = IQR(Temp),
            min_temp = min(Temp),
            max_temp = max(Temp))
```

 Minus 19F!  Yikes.

Of course, we might think that the `min` and `max` are _too_ extreme, to the point of being unrepresentative of the underlying data distribution.  I'll give you an example.  By 2021, I'd spent over 15 years of my life in Austin, Texas, and the coldest temperature I'd encountered there was 18 degrees F---until one day in February 2021, when the overnight temperature hit 6 F, shattering both my own personal record low _and_ the state's [power grid](https://www.texasmonthly.com/news-politics/responsible-texas-blackouts/).  So now when out-of-towners ask me, "What's the coldest it gets in Austin?" I guess I have to say, "6 degrees."^[And about 40 degrees inside without power for a few days.]  But that figure is actually quite unrepresentative of even the _very cold_ days in an Austin winter---which, as a native Texan, I'd peg at 30 or maybe 35 degrees F, to the derision of my northern friends.   

So instead of calculating the min and max temperatures, we might instead want to ask questions about the extremes, but not the _very_ extremes.  These kinds of questions are naturally phrased in terms of percentiles.  For example:    

- What's the 5th percentile of temperatures?  That is, what's a temperature so cold that only 5% of days in Rapid City are colder?  
- What's the 95th percentile of temperatures? That is, what's a temperature so hot that 95% of days in Rapid City are colder---or equivalently, only 5% of days are warmer?  

To do this, we use the `quantile` function in R.  By convention, __quantiles__ are percentiles expressed on a 0-to-1 scale, so the 5th and 95th percentiles correspond to 0.05 and 0.95 quantiles, respectively.  Let's add these quantiles to our summary pipeline.  We'll also add a step in our pipeline to round these numbers to one decimal place, since this table is getting kinda wide:  

```{r}
rapidcity %>%
  summarize(avg_temp = mean(Temp),
            median_temp = median(Temp),
            sd_temp = sd(Temp),
            iqr_temp = IQR(Temp),
            min_temp = min(Temp),
            max_temp = max(Temp),
            q05_temp = quantile(Temp, 0.05),
            q95_temp = quantile(Temp, 0.95)) %>%
  round(1)
```

So the 5th percentile of temperature in Rapid City is 12.9F, while the 95th percentile is 77.2 F.  

We can ask R for any quantiles we want.  For example, here are the 25th and 75th percentiles of temperatures in Rapid City, juxtaposed with the inter-quartile range:

```{r}
rapidcity %>%
  summarize(iqr_temp = IQR(Temp),
            q25_temp = quantile(Temp, 0.25),
            q75_temp = quantile(Temp, 0.75))
```

Remember that the IQR is precisely the difference between the 75th and 25th percentiles: IQR = 63.95 - 33.3 = 30.65.


## z-scores


Which temperature is more extreme: 50 degrees in San Diego, or 10 degrees in Rapid City?  In an absolute sense, of course 10 degrees is a more extreme temperature.^[I mean compared to what's comfortable for a person.  Clearly if you interpret "absolute" as meaning "compared to the average temperature of the universe," (2.73 K or -454.76 F), then 50F is the more extreme temperature.]  But what about in a relative sense?  In other words, is a 10-degree day more extreme _for Rapid City_ than a 50-degree day is _for San Diego_?  This question could certainly be answered using quantiles, which you've already learned how to handle.  But let's discuss a second way: by calculating a z-score for each temperature.  

The z-score of some numerical variable $x$ is the number of standard deviations by which $x$ is above its mean.   (If a z-score is negative, then the corresponding observation is below the mean.)

To calculate a z-score for a number $x$, we subtract the corresponding mean $\mu$ and divide by the standard deviation $\sigma$:

$$
z = \frac{x - \mu}{\sigma} \, .
$$

It turns out that the average year-round temperature in San Diego is 63.1 degrees, with a standard deviation of 5.7 degrees.  So for a 50-degree day in San Diego, the $z$-score is:

$$
z = \frac{50 - 63.1}{5.7} \approx -2.3 \, .
$$

Or about 2.3 standard deviations below the mean.  On the other hand, Rapid City has a mean temperature of 47.3 degrees and a standard deviation of 20.1 degrees.  So for a 10-degree day in Rapid City, the z-score is

$$
z = \frac{10 - 47.3}{20.1} \approx -1.9 \, .
$$

Or about 1.9 standard deviations below the mean.  Thus a 50-degree day in San Diego is actually more extreme than a 10-degree day in Rapid City!  The reason is that temperatures in Rapid City are both colder on average (lower mean) and more variable (higher standard deviation) than temperatures in San Diego.

As this example suggests, z-scores are useful for comparing numbers that come from different distributions, with different statistical properties. It tells you how extreme a number is, relative to other numbers from that same distribution.  

Let's calculate z-scores for each day in Rapid City, using `mutate`.  Really, the worst part about this code is making sure that all the parentheses are balanced! Beyond that, it's just applying the formula for a $z$ score:  

```{r}
rapidcity = rapidcity %>%
  mutate(z = (Temp - mean(Temp))/sd(Temp))
```

Now here are ten random rows of the data set after we've calculated the $z$ scores:

```{r, echo=FALSE, message=FALSE}
rapidcity %>% 
  getter(ind) %>%
  arrange(Year, Month, Day) %>%
  knitr::kable()
```

In general, we think of $|z| \approx 2$ as the very lower limit of where a data point might be considered "unusual" or "surprising" relative to other numbers from the same distribution. That figure is based on the normal distribution: for normally distributed data, roughly 95% of the data points will fall within 2 standard deviations of the mean.  Of course, since most data sets aren't exactly or even approximately normal, this guideline is _very_ rough indeed.  But it at least gives you an order-of-magnitude sense for how to interpret a $z$-score.   A $z$-score of $\pm 1$, for example, is very typical and not at all surprising.  But a $z$ score of $\pm 10$ would be a _very_ unusual data point, pretty much no matter how the data are distributed.   


## Study questions {-}  

<!--chapter:end:05_summaries.Rmd-->

# Data wrangling

```{r, echo=FALSE, set.seed(19820518)}
knitr::opts_chunk$set(cache = T)
```

__Data wrangling__ refers to the process of getting your data into a useful form for visualization, summary, and modeling.  Wrangling is an important part of data science, because data rarely comes in _precisely_ the form that suits some particular analysis.  For example, you might want to focus only on specific rows or columns of your data, or calculate summary statistics only for specific subgroups.   Maybe you want to create a new variable derived from existing ones.  Or else you just want to sort the rows according to some variable, to make a table a little easier to read.  All of these tasks involve wrangling your data.  

Luckily, R---or more specifically, the `tidyverse` library---comes equipped with several handy data verbs that streamline these wrangling tasks.   In this lesson, you'll learn to:  

- use six key data verbs (`summarize`, `group_by`, `mutate`, `filter`, `select`, and `arrange`) to transform your data into a more convenient form.   
- calculate complex summary statistics for arbitrary subsets of your data and combinations of variables.  

This covers just the very basics of data wrangling in R.  For a more advanced treatment, I recommend [R for Data Science](https://r4ds.had.co.nz/index.html), by Hadley Wickham and Garrett Grolemund.  ([Wickham](https://twitter.com/hadleywickham) is the author of many popular R libraries, including the `tidyverse`.)

For this lesson, create a fresh new script, and then load the `tidyverse` library by placing the following line at the top of your script and running it in the console:

```{r, message=FALSE}
library(tidyverse)
```  

Let's get wranglin'!  

## Key data verbs

```{r, echo=FALSE}
rapidcity = read.csv('data/rapidcity.csv', header=TRUE)
```

You've already learned one important data verb: `summarize`.  Now, we'll learn five new verbs:

- `group_by`, for splitting a data set into groups.  
- `filter`, for looking at specific rows (cases).  
- `select`, for looking at specific columns (variables).  
- `mutate`, for defining new variables from old ones.  
- `arrange`, for sorting a data frame according some specific variable.  

In the examples to follow, we'll again work with the [rapidcity.csv](data/rapidcity.csv) data set that we used in the previous lesson on [Summaries].  Go ahead and get this data set imported into RStudio.  

### `group_by` {-}

A nice feature of R is that we can calculate any common summary statistic _grouped_ according to the value of some other variable.  For example, take (a simplified version of) our summary pipeline from the lesson on [Summaries], where we calculated summary statistics for temperatures in Rapid City:  

```{r}
rapidcity %>%
  summarize(avg_temp = mean(Temp),
            sd_temp = sd(Temp),
            q05_temp = quantile(Temp, 0.05),
            q95_temp = quantile(Temp, 0.95)) %>%
  round(1)
```

These summary statistics tell us what's happening across the entire year.  But what if we wanted to calculate these statistics separately for each month?  After all, as a simple boxplot confirms, Rapid City has a lot of seasonal variation in temperature.  In the summer months, temperatures are higher, but also less variable:

```{r}
ggplot(rapidcity) + 
  geom_boxplot(aes(x=factor(Month), y=Temp))
```

To calculate our summaries by month, we insert a `group_by` statement before `summarize`, like in the code block below:

```{r}
rapidcity %>%
  group_by(Month) %>%
  summarize(avg_temp = mean(Temp),
            sd_temp = sd(Temp),
            q05_temp = quantile(Temp, 0.05),
            q95_temp = quantile(Temp, 0.95)) %>%
  round(1)
```

The result is a table of summary statistics with one row per month. In English, this code block reads as follows:  

- start with the `rapidcity` data set...  
- _then_ group the observations by the `Month` variable...  
- _then_ calculate summary statistics separately for each month...  
- _then_ round the numbers to the first decimal place.  

It's important that `group_by` precede `summarize` in our code block, because that reflects the logical order of operations.  If we called `summarize` first, we'd be summarizing the _full_ data set, not the subgroups defined by the different months.  

We can perform the same grouping trick with most common summary statistics.  You'll see several other examples below; for a full list, type `?summarize` into your console.

#### Bar plots again {-}

The combination of `group_by` and `summarize` is especially useful for making tables of group-level summary statistics to feed into bar plots. As we discussed in the lesson on [Plots], the typical workflow to make a bar plot actually has two distinct stages:  

(1) __Summary stage__: split your data set into subgroups and calculate summary statistics for each subgroup.  
(2) __Plotting stage__: make a bar plot of those summary statistics, one bar per group.  

In the lesson on [Bar plots], I took care of stage 1 for you and just handed you a table of summary statistics.  But you're now in a position to undertake both stages yourself.  Let's see an example on our Rapid City data.  Below, we calculate two summary statistics for each month: the average temperature and the proportion of sub-freezing days.  We then store the result in an object called `rapidcity_summary`:

```{r}
rapidcity_summary = rapidcity %>%
  group_by(Month) %>%
  summarize(avg_temp = mean(Temp),
            prop_freeze = sum(Temp <= 32)/n())
```

Now let's make bar plots for each of our two summary statistics.  First, average temperature:  

```{r}
ggplot(rapidcity_summary) + 
  geom_col(aes(x=factor(Month), y=avg_temp))
```

Next, proportion of days that are sub-freezing, on average:

```{r}
ggplot(rapidcity_summary) + 
  geom_col(aes(x=factor(Month), y=prop_freeze))
```

You might notice an oddity in this code: why `factor(Month)` rather than just `Month`?  The reason is that bar plots expect a _categorical_ $x$ variable to define the groups/bars.  But `Month` in our data set is actually a number: 1 for January, 2 for February and so on.  R will therefore treat it as a _numerical_ variable by default.  The command `factor` overrides this default behavior, telling R to treat the number as a label, not as a number with a meaningful magnitude.  


### `filter` {-}

The verb `filter` is how we look at specific rows our data frame.  Suppose, for example, that we didn't care about calculating our monthly temperature statistics in every year---just, for some odd reason, in 2009.  We can `filter` the data set to include only the 2009 data, like this.

```{r}
# create a 2009-only subset
rapidcity2009 = rapidcity %>%
  filter(Year == 2009)
```

You might be confused by the two different types of equal signs here.  The double-equals sign (`==`) inside `filter` is used to _test for equality_.  That is, we are filtering the data frame to include only those cases where the `Year` variable is equal to 2009.  The single equals sign, `=`, is used for _object assignment._  The assignment, although it comes first in our sequence of commands, actually doesn't happen until the very end.  So in English, this code block says:  

- start with the `rapidcity` data frame...
- _then_ filter down to only those rows where `Year == 2009`.  
- _and finally_ store the resulting filtered data frame in an object called `rapidcity2009`.  

We can verify that this worked as expected by peeking at the first six lines of our filtered data frame.  

```{r}
head(rapidcity2009)
```

We can use any of the following logical tests in our `filter` statements:  

- `==` for exact equality, e.g. `Month == 3` for March.  Note that if our variable is a text string, we need to surround it with quotation marks.  So if the months were encoded using English names rather than letters, we'd need `Month == "March"` instead.  You'll see examples of this soon.  
- `!=` for "is not equal to."  
- `<` and `<=` for "less than" and "less than or equal to," respectively.  For example, `Temp <= 50` will include only those days where the `Temp` variable is less than or equal to 50 degrees.  Similarly, `>` and `>=` mean "greater than" and "greater than or equal to," respectively.  
- `|` for "or" statements.  E.g. `Month == 1 | Month == 2` will get you all rows from January and February across all years.  
- `&` for "and" statements.  E.g. `Month == 1 & Year == 2009` will get you all rows specifically from January 2009.   

Finally, we can combine `filter` with other verbs---like in the code block below, where we calculate monthly summary statistics for a subset of the data that spans 2006--09:  

```{r}
rapidcity %>%
  filter(Year >= 2006 & Year <= 2009) %>%
  group_by(Month) %>%
  summarize(avg_temp = mean(Temp),
            sd_temp = sd(Temp)) %>%
  round(1)
```

In English, this code block says:  

- start with the `rapidcity` data...  
- _then_ filter down to cases from 2006 to 2009 (inclusive)...   
- _then_ group those cases by month...  
- _then_ calculate summary statistics...  
- _then_ round the numbers to the first decimal place.  

### `select` {-}

The verb `select`, meanwhile, is used to select specific columns (variables) in your data set.  I find this particularly useful for removing superfluous detail and de-cluttering output tables.  For example, in our `rapidcity2009` data frame, we might feel that the `Year` column is now unnecessary, since every data point is from 2009.  We can use `select` to pick only the variables we want to retain, separating those variable names by commas:   

```{r}
rapidcity2009 %>%
  select(Month, Day, Temp) %>%
  head
```

The `Year` column no longer appears in our data frame.  Note that we could have accomplished the same task by saying `select(-Year)`, like this:

```{r}
rapidcity2009 %>%
  select(-Year) %>%
  head
```

This code chunk selects all columns _except_ `Year` and prints the first six lines of the filtered data frame.    

### `mutate` {-}

Use `mutate` to define new variables from old ones.  For example, suppose we wanted to augment our `rapidcity` data frame with a new variable, `Summer`, telling us whether a specific row was in June, July, or August.  We'd accomplish this as follows:  

```{r}
rapidcity_augmented = rapidcity %>%
  mutate(Summer = ifelse(Month == 6 | Month == 7 | Month == 8,
                         yes="summer", no="not_summer"))
```

In English, this code block says:  

- Start with the `rapidcity` data frame.
- _Then_ add a variable called `Summer`, defined as follows:  
   - if the `Month` variable is either 6, 7, or 8, set `Summer` to have the value `summer`.  
   - if not, set `Summer` to have the value `not_summer`.  
- _Then_ store the result in a data frame called `rapidcity_augmented`.  

Let's verify that this worked as intended:  

```{r}
head(rapidcity_augmented)
```

Looks good!  

We can now use the `Summer` variable just like any of our original variables. For example, here we are showing a faceted histogram of temperatures by summer status:  

```{r rapidcity_augmented_histogram}
ggplot(rapidcity_augmented) +
  geom_histogram(aes(x=Temp), binwidth=2) + 
  facet_wrap(~Summer, nrow=2)
```


### `arrange` {-}

Use `arrange` for sorting on specific variables.  This is useful on a raw data frame if you wanted to see the "Top-N" cases according to some measure.  It's also useful on tables of summary statistics, as we'll see below.  

As an example, let's find the ten coldest days in Rapid City over the sample period:  

```{r}
rapidcity %>%
  arrange(Temp) %>%
  head(10)
```

If we wanted the ten hottest days, we'd have to arrange in _descending_ order of `Temp` using `desc`, like this:  

```{r}
rapidcity %>%
  arrange(desc(Temp)) %>%
  head(10)
```


## Complex summaries

In this section we'll see some examples of how we can combine data verbs to perform complex tasks very concisely.

Anytime you're faced with complex data-analysis tasks like these, I'd encourage you to remember our basic mantra of data science that we introduced all the way [back here](#sec_objects):

> Manage complexity by breaking complex tasks down into simple tasks, and then stitching the simple tasks together.  

Keep this mantra in mind at all times.  For each of these examples below, focus on three questions:  

1) What are main high-level tasks that must be accomplished?
2) How can those "main" tasks be broken down into a sequence of simpler subtasks?  (What are the tasks themselves, and how should they be sequenced)  
3) How can those sub-tasks be translated into data verbs, like `group_by`, `summarize`, `filter`, etc.?



### Example 1: the five coldest months {-}  

For our first example, our main high-level task is to find the five coldest individual months in our data set on Rapid City, which spans the seventeen years from 1995 to 2011.

This main task breaks down into simpler sub-tasks that look something like the following:  

- Import the data set (we've done this already).  
- Split the data set into individual months in individual years: January 1995, February 1995, March 1995, and so on, all the way through December 2011.  
- For each individual month, calculate the average of the `Temp` variable (along with any other summaries we might find interesting).    
- Sort the individual months according to their average temperatures.  
- Make a table of the five coldest months.  

Our pipeline for this analysis, shown below, reflects this logical structure.  It uses a combination of `group_by`, `summarize`, and `arrange`, before piping the results into `head` and `round` (which truncate the table and round the numbers):  

```{r, message=FALSE}
rapidcity %>%
  group_by(Year, Month) %>%
  summarize(avg_temp = mean(Temp),
            coldest_day = min(Temp),
            warmest_day = max(Temp)) %>%
  arrange(avg_temp) %>%
  head(5) %>%
  round(1)
```

It seems January of 1996 was very cold.  

Notice how we can group by multiple variables---in this case, both `Year` and `Month`.  When we call `group_by` with multiple variables, it groups the data according to all possible combinations of those variables.  So to be concrete, because there are 17 distinct years and 12 distinct months, this `group_by` statement splits the data set into $17 \times 12 = 204$ groups:

- Group 1: `Year = 1995`, `Month = 1`  
- Group 2: `Year = 1995`, `Month = 2`
- ...
- Group 203: `Year = 2011`, `Month = 11`
- Group 204: `Year = 2011`, `Month = 12`

With the data grouped in this fashion, my code chunk then computes the requested summary measure separately for each group (in this case, the mean temperature and the min/max daily temperatures),  and then sorts the groups by `avg_temp`.  

The other thing to notice is that variables and summaries defined earlier in the pipeline become available to be used by later steps in the pipeline.  In this example, the summary measure `avg_temp` is _created_ by `summarize` on the 3rd line, and then subsequently _used_ as a basis for sorting on the 4th line, via `arrange(avg_temp)`.  

But the ordering of the steps here is crucial.  Your pipeline can only use variables and summaries from the "pipeline past," not the "pipeline future."  This is one of those times where it's informative to see what happens if we make a mistake.  If we'd switched the order of `summarize` and `arrange`, we'd get a cryptic error message, rather than a table of really cold months:

```{r, error=TRUE}
rapidcity %>%
  group_by(Year, Month) %>%
  arrange(avg_temp) %>%
  summarize(avg_temp = mean(Temp))
```

The core of this error is `object 'avg_temp' not found`.  Basically, R is telling us that it can't arrange the data frame according to `avg_temp` because the `avg_temp` summary has not yet been defined.  In fact, it was supposed to be defined in the next step of our ill-fated pipeline---by which point R had already choked and thrown the error.

Refining data is like [refining oil](https://www.eia.gov/energyexplained/oil-and-petroleum-products/refining-crude-oil-the-refining-process.php): when it comes to pipelines, the order in which you perform the tasks really matters.  Breakdowns in code sometimes just reflect silly mistakes, like typos or missing commas.  But other times, breakdowns in code reflect much more fundamental breakdowns in _logic_.  The code block above is a good example of the latter.  


### Example 2: survival on the Titanic {-}  

```{r, echo=FALSE, message=FALSE}
titanic = read.csv('data/titanic.csv')
```

For this example, you'll need the data in [titanic.csv](data/titanic.csv).  Go ahead and import the data set into R, calling the imported object `titanic` (which, once again, will be the default if you use the `Import Dataset` button).  

If you take a quick peek at the first 6 lines...

```{r}
head(titanic)
```

...you'll see that each row is a person: specifically, a passenger on the Titanic when it sank on April 15, 1912.  Each column contains details about that passenger, including their sex, age, class of travel, and whether they survived the sinking of the ship.  

For this example, we'll answer the question: how did survival among adult passengers vary by sex and cabin class?  Again, remember the basic mantra of data science:

> Manage complexity by breaking down complex tasks into simpler tasks and then stitching those simple tasks together.  

Here our simple tasks look something like this:   

- create a new variable, which we'll call `Adult`, that determines whether a passenger is at least 18 years old.  
- filter the data set down to adults only.  
- group the filtered data set by sex and cabin class (2 sexes $\times$ 3 classes = 6 groups).  
- calculate the survival percentage for each group.  

To do all this, we'll use a combination of `mutate`, `filter`, `group_by`, and `summarize`.  We'll then pass the resulting summary table into `ggplot` to make a bar plot, adding a little visual pizzazz to what might otherwise be a dry table.  

We'll actually break our sequence of tasks down into two parts.  Here's part 1, where we create a table called `surv_adults`:  

```{r, message=FALSE}
surv_adults = titanic %>%
  mutate(age_bracket = ifelse(age >= 18,
                              yes="adult", no="child")) %>%
  filter(age_bracket == "adult") %>%
  group_by(sex, passengerClass) %>%
  summarize(total_count = n(),
            surv_count = sum(survived == 'yes'),
            surv_pct = surv_count/total_count)
```

Remember, `=` is for assigning values to objects, while `==` is for testing equality.  Notice that in the `filter` step, we had to put quotation marks around `adult`, since `age_bracket` is a categorical variable rather than a number.

The only other unfamiliar part of this pipeline might be in the details of the `summarize` step.  Here we created three summary variables:  

- `total_count` is created by the handy function `n()`, which counts the total number of cases in each group.  `n()` behaves a bit like `xtabs` does, but it's used inside pipelines.  
- `surv_count` calculates the number of people that survived in each group, by summing up the cases where the `survived` variable is `yes`.  This is done via `sum(survived == 'yes')`
- `surv_pct` is then calculated as the ratio of of these two numbers: `surv_count/total_count`.  

The result is a table of our requested summary statistics for each combination of sex and passenger class:

```{r}
surv_adults
```

This is fine as a table, but you might want to make it more visually appealing.  So in part 2, let's feed this table into `ggplot` to create a faceted bar plot, comparing survival percentages by sex across all three passenger classes:

```{r}
ggplot(surv_adults) + 
  geom_col(aes(x=sex, y=surv_pct)) + 
  facet_wrap(~passengerClass, nrow=1)
```

### Example 3: toy imports {-}

```{r, echo=FALSE}
toyimports = read.csv('data/toyimports.csv', header=TRUE)
```

For our final example, we'll look at the data in [toyimports.csv](data/toyimports.csv), which tracks imports of toys to the United States from 129 countries over the period 1996--2005.  Here's a random sample of 10 rows from this data set.  

```{r, echo=FALSE, message=FALSE, cache=TRUE}
mosaic::sample(toyimports, 10, orig.id=FALSE) %>%
  arrange(year, partner) %>%
  select(-partner, -pop2000, -region) %>%
  knitr::kable() %>%
  kableExtra::kable_styling(font_size = 12)
```

Every row shows the total dollar value of toys imported to the U.S. (`US_report_import`, in multiples of \$1,000) in a specific product category from a specific country in a specific year.  The product categories have unique numerical codes (`product`) as well as product names exciting enough to quicken the heart of any toy-loving child ("Parts and accessories :– Other", "Toys representing animal or non-human figures", and so on).  

Our goal for this data analysis is to make a line graph showing total toy imports over time, summed across all categories, for the U.S.'s top 3 trading partners by total dollar value of toys imported.

Let's break this down into simpler tasks.  First, we need to find the top 3 partners by total dollar value.  This task itself can be broken down into simpler sub-tasks:  

- Group all the observations by trading partner (the `partner_name` variable).  
- For each partner, calculate total dollar value by summing toy imports (`US_report_import`) across all categories and years.  
- Arrange the partners by total dollar value.  

Let's see how to accomplish this using a combination of `group_by`, `summarize`, and `arrange`:

```{r}
country_totals = toyimports %>%
  group_by(partner_name) %>%
  summarize(total_dollar_value = sum(US_report_import)) %>%
  arrange(desc(total_dollar_value))
```

Now we can look at the top trading partners:

```{r}
head(country_totals)
```

So the U.S.'s top 3 toy-trading partners were China, Denmark, and Canada.  Let's encode that information in a list, taking taking to use the precise values reflected in the `parter_name` column of the table above:

```{r}
top3_partner_names = c('China', 'Denmark', 'Canada')
```

Here `c` means "combine", and it's how we create a list^[Technically it's a vector, if you want to get specific about R's internal data structures.] of multiple elements.  This list of names will be useful when we turn to our second main task: plotting toy imports over time for each categories, separately for each of these three top trading partners.  

This second main task also breaks down into sub-tasks:

- Filter the data set so that it includes only data points from the top 3 trading partners.  
- Group the data points by partner and year.  
- Within each group, sum the toy imports across all categories.

Here's how to translate this into R code, as well as the top 6 lines of the data frame created as a result:

```{r, message=FALSE}
top3_byyear = toyimports %>%
  filter(partner_name %in% top3_partner_names) %>%
  group_by(year, partner_name) %>%
  summarize(yearly_dollar_value = sum(US_report_import))

head(top3_byyear)
```

Now we can make our line graph of `yearly_dollar_value` versus `year`.  We'll color in each line according to trading partner, like this:  

```{r}
ggplot(top3_byyear) +  
  geom_line(aes(x=year, y=yearly_dollar_value, color=partner_name))
```

But this isn't great for two reasons:

1) The color scale is unfriendly to those with colorblindness.  We'll fix this by using `scale_color_brewer`, as we learned in the lesson on [Customizing plots].  
2) The lines for Denmark and Canada are dwarfed by the line for China.  We'll fix this by plotting the data on a logarithmic axis, which is a more natural way to compare quantities on very different scales.  

We can do this as follows:  

```{r}
ggplot(top3_byyear) +  
  geom_line(aes(x=year, y=yearly_dollar_value, color=partner_name)) +
  scale_color_brewer(type='qual') + 
  scale_y_log10()
```

The last, minor issue here is that `ggplot` has made a poor choice about where to place axis labels.  We can fix that easily, while also adding more informative axis labels:

```{r}
ggplot(top3_byyear) +  
  geom_line(aes(x=year, y=yearly_dollar_value, color=partner_name)) +
  scale_color_brewer(type='qual') + 
  scale_y_log10() + 
  scale_x_continuous(breaks = 1996:2005) + 
  labs(x="Year", y = "Dollar value of imports (log scale)",
       title="Toy imports from the U.S.'s top-3 partners, 1996-2005")
```

And we're done!  Now we can compare across countries, as well as examine change over time within countries.


## Summary shortcuts

Having covered several examples of complex summaries, we'll finish off this section at the opposite end of the difficulty spectrum: with some shortcuts for calculating simple summaries "on the fly," in a single line of code.  We'll also revisit these shortcuts in our section on statistical inference---specifically, in the upcoming lesson on [The bootstrap].  

For these shortcuts to work, you'll need the `mosaic` library loaded:

```{r, message=FALSE}
library(mosaic)
```

Here are the basic shortcuts, illustrated on the `titanic` data.  To calculate the mean of a variable, you can use `mean` directly, like this:

```{r}
mean(~age, data=titanic)
```

This statement computes the mean value of the `age` variable for everyone in the `titanic` data set.  Don't forget the tilde (`~`) in front of `age`.

You can also calculate a mean stratified by some other grouping variable, like this:  
```{r}
mean(age ~ sex, data=titanic)
```

This tells us the mean of the `age` variable for males and females separately.  Of if all you care about is the _difference_ between means, you can use `diffmean`:  
```{r}
diffmean(age ~ sex, data=titanic)
```

This tells us that the average age among males is about 1.9 years older than among females.  (`diffmean` only works if the grouping variable on the right-hand side of the tilde has exactly two levels.)

The same type of shortcut works for proportions, too, using `prop`.  For example, here's the overall proportion of those who died on the Titanic (i.e. where `survived == "no"`):

```{r}
prop(~survived, data=titanic)
```

Here is that same proportion stratified by sex:  
```{r}
prop(survived ~ sex, data=titanic)
```

And here is the difference of those two proportions, using `diffprop`:

```{r}
diffprop(survived ~ sex, data=titanic)
```

This tells us that the survival rate among females was about 55% higher than among males.  

The following summaries all have shortcut forms that follow the same basic logic:

- `median`
- `range`, `sd`, and `IQR` for measuring dispersion
- `max` and `min`  
- `favstats` for a collection of multiple summary statistics  

I particularly like the `favstats` shortcut.  For example:

```{r}
favstats(age ~ sex, data=titanic)
```

Of course, these shortcuts aren't nearly as flexible as the full set of data verbs covered above, in that they don't let us `filter`, `mutate`, etc.  Nor do they let us compute any old summary statistic we might care about.   But they are useful for quick data exploration, when simple summaries often do the trick.

<!--chapter:end:06_data_wrangling.Rmd-->

# Fitting equations {#basic_regression}  

In data science, the term "regression" means, roughly, "fitting an equation to describe relationships in data."  This is a _huge_ topic, and we'll cover it across multiple lessons.  This lesson is about the most basic kind of equation you can fit to a data set: a straight line, also known as a "linear regression model."  You'll learn:  

- what a linear regression model is.  
- how to fit a regression model in R.  
- why regression models are useful.  
- how to go beyond "simple" straight lines.  


## What is a regression model?  

Let's first see a simple example of the kind of thing I mean.  You may have heard the following rule from a website or an exercise guru: to estimate your maximum heart rate, subtract your age from 220.  This rule can be expressed as an equation:

$$
\mathrm{MHR} = 220 - \mathrm{Age} \, .
$$

This equation provides a mathematical description of a relationship in a data set: maximum heart rate (the _target_, or thing we want to predict) tends to get slower with age (the _feature_, or thing we know).  It also provides you with a way to make predictions.  For example, if you're 35 years old, you predict your maximum heart rate by "plugging in" Age = 35 to the equation, which yields MHR = 220 – 35, or 185 beats per minute.  

A "linear regression model" is exactly like that: an equation that describes a linear relationship between input ($x$, the feature variable) and output ($y$, the target or "response" variable).  Once you've used a data set to find a good regression equation, then any time you encounter a new input, you can "plug it in" to predict the corresponding output---just like you can plug in your age to the equation $\mathrm{MHR} = 220 - \mathrm{Age}$ and read off a prediction for your maximum heart rate.  This particular equation has one __parameter__: the baseline value from which age is subtracted, which we chose, or __fit__, to be 220.

So how would you actually come up with the number $220$ as your fitted parameter in this equation?  Basically, like this:

1) Recruit a bunch of people of varying ages and give them heart rate monitors.
2) Tell them to run really hard, ideally just short of vomiting, and record their maximum heart rate.  
3) Fiddle with the parameter until the resulting equation predicts people's actual maximum heart rates as well as possible. 

That last part is where we have to get specific.  In data science, the criterion for evaluating regression models is pretty simple: how big are the errors the model makes, on average?  No regression model can be perfect, mapping every input $x$ to exactly the right output $y$.  _All regression models make errors._  But the smaller the average error, the "better" the model, at least in this narrow mathematical sense.  In our equation $\mathrm{MHR} = 220 - \mathrm{Age}$, we could have chosen a baseline of 210, or 230, or anything.  But it turns out that the choice of 220 fits the data best.  It's the value that leads to the smallest average error when you compare the resulting age-based _predictions_ for MHR versus people's _actual_ MHR that they clocked on the treadmill.

We can visualize the predictions of this equation in a scatter plot like the one below.  The points show the individual people in the study, while the grey line shows a graph of the equation $\mathrm{MHR} = 220 - \mathrm{Age}$:  

```{r heart-rate-example, echo=FALSE, out.width="85%", fig.align='center', fig.cap="Heart rate versus age."}
knitr::include_graphics("images/08_heart_rate.jpeg")
```

But as this also picture shows, there's actually a slightly more complex equation that fits the data even better: $\mathrm{MHR} = 208 - 0.7 \cdot \mathrm{Age}$.  In words, multiply your age by 0.7 and subtract the result from 208 to predict your maximum heart rate.  The original model had only one parameter, while this new model has two: the baseline of 208 and the "age multiplier" or "weight" of 0.7.  These values fit the data best, in the sense that, among all possible choices, they result in the smallest average prediction error.  

In the picture above, you can probably just eyeball the difference between the grey and black line and conclude that the black line is  better: it visibly passes more through the middle of the point cloud.  But you could also conclude logically that the black line __must__ be better, because it allows us to fiddle with both the baseline _and_ the age multiplier to fit the data.  After all, given a free choice of both the baseline and the age multiplier, we _could_ have picked a baseline of 220 and a weight on age of 1, thereby matching the predictions of the original "baseline only" model.  But we didn't---those weren't the best choices!   We can therefore infer that the equation $\mathrm{MHR} = 208 - 0.7 \cdot \mathrm{Age}$ _must_ fit the data better than the first equation of $\mathrm{MHR} = 220 - \mathrm{Age}$.

And that's a very general principle in regression: more complex models, with more parameters, can always be made to fit the observed data better.  (That _doesn't_ necessarily mean they'll generalize to future observations better; that's an entirely different and very important question, and one that we'll consider at length soon.)    

So how do we actually _know_ that these fitted parameters of 208 and 0.7 are the best choices?  Well, you _could_ just guess and check until you're convinced you can't do any better.  (You can't.)   But modern computers do this for us in a much more intelligent fashion, finding the best-fitting parameters by cleverly exploiting matrix algebra and calculus in ways that, [while fascinating](https://math.mit.edu/icg/resources/teaching/18.085-spring2015/LeastSquares.pdf), aren't worth going into here.  If you major in math or statistics or computer science, or if you program statistical software for a living, those details are super important.  (I was a math major and I still remember sweating them out line by line.)  Most people, on the other hand, are safe in simply trusting that their software has done the calculations correctly.  


####  An aside: multiple regression models {-}

We can also fit regression models with multiple explanatory variables.  Let's briefly see what one of these models looks like.  Suppose that you were a data scientist at Zillow, the online real-estate marketplace, and that you had to build a rule for predicting the price of a house.  You might start with two obvious features of a house, like the square footage and the number of bathrooms, together with weights for each feature.  For example:

$$
\mbox{Expected Price = 10,000 + 125 $\times$ (Square Footage) + 26,000 $\times$ (Num. Bathrooms)}
$$

In words, this says that to predict the price of a house, you should follow three steps:

1)	Multiply the square footage of the house by 125 (parameter 1).  
2)	Multiply the number of bathrooms by 26,000 (parameter 2).
3)	Add the results from A and B to 10,000 (the baseline parameter).  This yields the predicted price.

This is called a _multiple regression model_, because it's an equation that uses multiple features (square footage, bathrooms) to predict price.  In real life, the weights or multipliers would be tuned to describe the prices in a particular housing market.  You'll always have one more parameter (here, 3) than you have features (here, 2): one parameter per feature, plus one extra parameter for the baseline.  

Of course, we don't need to stop at two features!  Houses have lots of other features that affect their price—for example, the distance to the city center, the size of the yard, the age of the roof, and the number of fireplaces.  With modern software, we can easily fit an equation that incorporates all of these features, and hundreds more.  Every feature gets its own weight; more important features end up getting bigger weights, because the data show that they have a bigger effect on the price.  Now, if you try to write out such a model in words---"add this," "multiply this," like we did for the two-feature rule above---it starts to look like the IRS forms they hand out in Hell.   But R just churns through all the calculations with no problem, even for models with hundreds of parameters.

In this lesson, we're going to focus on basic regression models with just one feature.  In a later lesson, we'll learn about _multiple regression models_, which incorporate lots of features (like our imaginary Zillow example above). But whether a model has 1 feature or 101 features, the underlying principles are the same:

1) A regression model is an equation that describes relationships in a data set.  
2) Regression models have free parameters: the baseline value, and the weights on each feature.  
3) We choose these parameters so that the regression model's predictions are as accurate as possible.  



## Fitting regression models    

First, a bit of notation.  The general form of a simple regression model with just one feature looks like this:  

$$
y_i = \beta_0 + \beta_1 x_i + e_i
$$

Each piece of this equation (a.k.a. "regression model") has a name:

- $x$ and $y$: the feature (x) and target (y) variables.    
- $\beta_0$ and $\beta_1$: the intercept and slope of the line (the _parameters_ or _weights_ of the model)  
- $e$: the model error, also called the _residual_  
- $i$: an index that lets us refer to individual data points ($i=1$ is the first data point, $i=2$ the second, and so on).   

Given a data set with bunches of pairs $(x_i, y_i)$, we choose the intercept ($\beta_0$) and slope ($\beta_1$) to make the model errors ($e_i$) as small as we can, on average.  To be a bit more specific, for mathematical reasons not worth going into, we actually make the _sum of the squared model errors_ as small as we can.  That's why this approach is called _ordinary least squares,_ or OLS.  ("Ordinary" to distinguish it from other variations on the same idea that you might learn about in another course, like "weighted" or "generalized" least squares.)  

```{r, echo=FALSE, message=FALSE}
heartrate = read.csv("data/heartrate.csv", header=TRUE)
```

Let's see an example in R.  First, load the `tidyverse` and mosaic libraries, which we pretty much always need:

```{r, message=FALSE}
library(tidyverse)
library(mosaic)
```

Next, download the data in [heartrate.csv](data/heartrate.csv) shown in Figure \@ref(fig:heart-rate-example), and import it into RStudio as an object called `heartrate`.  Here are the first several lines of the file:

```{r}
head(heartrate)
```

This data is from an exercise study on maximum heart rates.  Each row is a person.  The two variables are the person's age in years and their maximum heart rate (`hrmax`) in beats per minute, as measured by a treadmill test.

Let's fit a model for maximum heart rate versus age, and then ask R what the weights (or "coefficients") of the fitted model are:

```{r}
model_hr = lm(hrmax ~ age, data=heartrate)
coef(model_hr)
```

And that's it---your first regression model!  Let's go line by line.  

- The first line, `model_hr = lm(hrmax ~ age, data=heartrate)`, fits a regression model using the `lm` function, which stands for "linear model."  
   - `hrmax` is the the target (or "response") variable in our regression. 
   - `age` is the feature (or "predictor") variable.  
   - `heartrate` is the data set where R can find these variables.
   - the `~` symbol means "by", as in "model `hrmax` by `age`."  
   - `model_hr` is the name of the object where information about the fitted model is stored.  (We get to choose this name.)  
- The second line, `coef(model_hr)`, prints the weights ("coefficients") of the fitted model to the console.  

The output is telling us that the baseline (or "intercept") in our fitted model is about 208, and the weight on age is about -0.7.  In other words, our fitted regression model is:

$$
\hat{y} = 208 - 0.7 x
$$

The notation $\hat{y}$, read aloud as "y-hat," is short for "predicted $y$".  It represents the _fitted value_ or _conditional expected value_: our best guess for the $y$ variable, given what we know about the $x$ variable. Here are the fitted values for the first five people in our data set:

```{r}
fitted(model_hr) %>%
   head(5)
```

## Using and interpreting regression models  

There are four common use cases for regression models:

1) Summarizing a relationship. 
2) Making predictions.  
3) Making fair comparisons.  
4) Decomposing variation into predictable and unpredictable components.    

Let's see each of these ideas in the context of our heart-rate data.  


### Summarizing a relationship {-}  

How does maximum heart rate change as we age?  Let's turn to our fitted regression model, which has these coefficients:

```{r}
coef(model_hr)
```

The weight on `age` of about -0.7 is particularly interesting.  Remember, it represents the slope of a line, and it therefore summarizes the overall or average relationship between max heart rate and age.  Specifically, it says that a one-year change in `age` is associated with a -0.7 beats-per-minute change in max heart rate, on average.  Of course, this isn't a guarantee that _your_ MHR will decline like this.  Virtually all of the points lie somewhere off the line!  The line just represents an average trend across lots of people.  

What about the baseline or "intercept" of about 208?  Mathematically, this intercept is where the line crosses or "intercepts" the $y$-axis, which is an imaginary vertical line at $x=0$.  (See the cartoon below.)  So if you want to interpret this number of 208 literally, it would represent our "expected" maximum heart rate for someone with `age = 0`.  But this literal interpretation is just silly.   Newborns can't run on treadmills.  Moreover, the youngest person in our data set is 18, so using the fitted model to predict someone's max heart rate at age 0 represents a pretty major extrapolation from the observed data.   Therefore let's not take this interpretation too literally.

```{r data-free-zone, echo=FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics("images/data_free_zone.png")
```



This example conveys a useful fact to keep in mind about the "intercept" or "baseline" in a regression model:  

- By mathematical necessity, every model has an intercept.  A line's gotta cross the $y$ axis somewhere!  
- Mathematically, this intercept represents the "expected outcome" ($\hat{y}$) when $x=0$.  
- But only _sometimes_ does this intercept have a meaningful _substantive_ interpretation.  If you've never observed any data points near $x=0$, or if $x=0$ would be an absurd value, then the intercept doesn't really provide much useful information by itself.  It's only instrumentally useful for forming predictions at more sensible values of $x$.  

### Making predictions {-}  

This brings us to our second use case for regression: making predictions.  For example, suppose your friend Alice is 28 years old. What is her predicted max heart rate?

Our equation gives the _conditional expected value_ of MHR, given someone's age.  So let's plug in `Age = 28` into our fitted equation:  

$$
\mbox{E(MHR | Age $=$ 28)} = 208 - 0.7 28 = 188.4
$$

This is our best guess for Alice's MHR, knowing her age, but without actually asking her to do the treadmill test.  Just remember: this is an informed guess, but it's still a guess!  If we were to run the treadmill test for real, Alice's _actual_ max heart rate will almost certainly be different than this.  

Let's see how to use R to do the plugging-in for us, to generate predictions for many data points at once.  Please download and import the data in [heartrate_test.csv](dara/heartrate_test.csv).  This data file represents ten imaginary people whose heart rates we want to predict, and it has a single column called `age`:

```{r, echo=FALSE, message=FALSE}
heartrate_test = read.csv("data/heartrate_test.csv", header=TRUE)
```

```{r}
heartrate_test
```

Let's now use the `predict` function in R to make predictions for these people's heart rates, based on the fitted model. 

```{r}
predict(model_hr, newdata=heartrate_test)
```

This output is telling us that:  

- the first person in `heartrate_test` has a predicted MHR of about 160, based on their age of 69.  
- the second person in has a predicted MHR of about 167, based on their age of 59.  
- and so on.  

The `predict` function expects two inputs: the fitted model (here `model_hr`), and the data for which we want predictions (here `heartrate_test`).  This `newdata` input must have a column in it that exactly matches the name of features used in your original fitted model.  Since our fitted model used a feature called `age`, our `newdata` input must also have a column called `age`.  

The output above is perfectly readable, but I personally find it easier if I put these predictions side by side with the original $x$ values.  So I'll use `mutate` to add a new column to `heartrate_test`, consisting of the model predictions:  

```{r}
heartrate_test = heartrate_test %>%
   mutate(hr_pred = predict(model_hr, newdata = .))
```

The dot (`.`) is just shorthand for "use the thing piped in as the new data."  Here the "thing piped in" is the `heartrate_test` data frame.  

With this additional column, now we can see the ages and predictions side by side:

```{r}
heartrate_test
```

### Making fair comparisons {-}

Regression models can also help us make fair comparisons that adjust for the systematic effect of some important variable.  To understand this idea, let's compare two hypothetical people whose max heart rates are measured using an actual treadmill test:  

- Alice is 28 with a maximum heart rate of 185.  
- Brianna is 55 with a maximum heart rate of 174.  

Clearly Alice has the higher MHR, but let’s make things fair!  We know that max heart rate declines with age.  So the real question isn't "Who has a higher maximum heart rate?"  Rather, it's "Who has a higher maximum heart rate _for her age_?"

The key insight is that a regression model allows us to make this idea of "for her age" explicit.   Alice's predicted heart rate, given her age of 28, is:  

$$
\hat{y} = 208 - 0.7 \cdot 28 = 188.4 
$$

Her actual heart rate is 185.  This is 3.4 BPM _below average_ for her age.  In regression modeling, this difference between actual and expected outcome is called the "residual" or "model error":

$$
\begin{aligned}
\mbox{Residual} &= \mbox{Actual} - \mbox{Predicted} 
&= 185 - 188.4 \\
& = -3.4
\end{aligned}
$$

Brianna, on the other hand, is age 55.  Her predicted max heart rate, given her age, is:  
$$
\hat{y} = 208 - 0.7 \cdot 55 = 169.5 
$$

Her actual heart rate is 174.  This is 4.5 BPM _above average_ for her age:

$$
\begin{aligned}
\mbox{Residual} &= \mbox{Actual} - \mbox{Predicted} \\
&= 174 - 169.5 \\
& = 4.5
\end{aligned}
$$

So while Alice has the higher max heart rate in absolute terms, Brianna has the higher heart rate _for her age._

This example illustrates how to use a regression model to make comparisons that adjust for the systematic effect of some variable (here, age).  Fair comparison just means _subtraction_: you take the difference between actual outcomes and expected outcomes, and then compare those differences, rather than the outcomes themselves.  In other words, __just compare the residuals__!

Let's see how to do this in R.  The relevant function here is called `resid`, which calculates the residuals, or model errors, for a fitted regression model.  In the code block below, I extract the residuals from our fitted model, and then I use `mutate` to add them as a new column to the original `heartrate` data frame:  

```{r}
heartrate = heartrate %>%
   mutate(hr_residual = resid(model_hr))
```

Now we can look at the newly augmented data frame:

```{r}
head(heartrate)
```

We see, for example, that of these 6 people, the 3rd person has the highest absolute `hrmax` (203), but the 6th person has the highest max heart rate for their age (11.6 BPM above their age-adjusted average).  

We can also use `arrange` to sort the data set by residual value. Remember that `arrange`, by default, sorts things in ascending order.  So here are the bottom 5 folks in our data set, as measured by age-adjusted max heart rate:  

```{r}
heartrate %>% 
   arrange(hr_residual) %>%
   head(5)
```

And here are the top 5, based on sorting the residuals in _descending_ order: 

```{r}
heartrate %>% 
   arrange(desc(hr_residual)) %>%
   head(5)
```

### Decomposing variation {-}

Consider the question posed by the following figure.  

```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics('images/green_beige_people.png')
```

Why, indeed?  Let's compare.  

- The two dots circled in beige represent people whose max heart rates diff by 25 BPM.  This difference is _entirely predictable_: each has a max heart rate that is almost perfectly average for their age, but they differ in age by 36 years.  The difference of 25 BPM, in other words, is exactly what we'd expect based on their ages.  
- The two dots circled in green _also_ represent people whose heart rates diff by 25 BPM.  But this difference is _entirely unpredictable_, because these two people are basically the same age.  Almost by definition, therefore, the difference in their maximum heart rates must have _nothing to do with_ their age. It must be down to something else.  

Now, to make things more interesting, let's consider a third pair of points:

```{r, echo=FALSE, out.width = "100%"}
knitr::include_graphics('images/green_beige_people2.png')
```

Here's yet another pair of people in the study whose max heart rates differ by 25 BPM.  But this difference is clearly intermediate between the two extreme examples we saw in the previous figure:  

- One person is 21 and the other 36.  This difference in age accounts for _some_ of the difference in their maximum heart rates.  To be specific, since the weight on age in our model is $-0.7$, age can account for a difference of $-0.7 \cdot (36-21) = -10.5$ BPM.  
- But even once you account for age, there's still an unexplained difference: the 21-year-old has an _above-average_ MHR for their age, while the 36-year old has a _below-average_ MHR for their age.  So age predicts some, but not all, of the observed difference in their MHRs.  

This example illustrates an important point.  Some variation is predictable.  Other variation is unpredictable.  Any _individual_ difference can be one or the other, but is usually a combination of both types. 

And that brings us to our fourth use-case for regression: to decompose variation into predictable and unpredictable components.  Specifically, this is based on examining and comparing the following quantities:

- $s_{e}$, the standard deviation of the model errors $e$.  This represents _unpredictable_ variation in $y$.  This is sometimes called the "root mean squared error," or RMSE.  
- $s_{\hat{y}}$, the standard deviation of the model's predicted or fitted values $\hat{y}$.  This represents _predictable_ variation in $y$.  

The simplest thing to do here is to just quote $s_e$, the standard deviation of the model residuals (or RMSE).  Here this number is for our heart-rate regression:

```{r}
sd(resid(model_hr))
```

This says that the typical model error is about 7.5 beats per minute.  This represents the unpredictable component of variation in max heart rate. 

How does this compare to the predictable component of variation?  Let's check by calculating the standard deviation of our model's fitted values:  

```{r}
sd(fitted(model_hr))
```

So it looks like the predictable and unpredictable components of variation are similar in magnitude.

As a more conventional way of measuring this, we often report the following quantity:

$$
R^2 = \frac{s_{\hat{y}}^2}{s_{\hat{y}}^2 + s_{e}^2} = \frac{PV}{TV}
$$

This number, pronounced "R-squared," measures how large the _predictable_ component of variation is, relative to the _total_ variation (i.e. the sum of both the predictable and unpredictable components).  Hence the mnemonic: $R^2 = PV/TV$.  

In English, $R^2$ answers the question: what proportion of overall variation in the $y$ variable can be predicted using a linear regression on the $x$ variable? Here are some basic facts about $R^2$.  

- It's always between 0 and 1.
- 1 means that $y$ and $x$ are perfectly related: all variation in $y$ is predictable.  
- 0 means no relationship: all variation in $y$ is unpredictable (at least, unpredictable using a linear function of $x$).
- $R^2$ is independent of the units in which $x$ and $y$ are measured.  In other words, if you ran the regression using "beats per second" rather than "beats per minute", or using "age in days" rather than "age in years", $R^2$ would remain unchanged.  
- $R^2$ does not have a causal interpretation.  It measures strength of linear association, not causation.  High values of $R^2$ do not imply that $x$ causes $y$.  
- If the true relationship between $x$ and $y$ is nonlinear, $R^2$ doesn't necessarily convey any useful information at all, and indeed can be [highly misleading](https://en.wikipedia.org/wiki/Anscombe%27s_quartet).    

You might stare at the definition of $R^2$ above and ask: why do we square both $s_{\hat{y}}$ and $s_e$ in the formula for $R^2$?  Great question!  There's actually a pretty deep mathematical answer---one that you'd learn in a mathematical statistics course, and that is, believe it or not, intimately connected to the Pythagorean theorem for right triangles.  But that takes us beyond the scope of these lessons.  If you'd like to read more, [the Wikipedia page for $R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination) goes into punishing levels of detail, as would an advanced course with a name like "[Linear Models](https://link.springer.com/book/10.1007/978-1-4419-9816-3)."

The easiest way to calculate $R^2$ in R is to use the `rsquared` function from the `mosaic` library:

```{r, message=FALSE}
rsquared(model_hr)
```

This number is telling us that about 52% of the variation in maximum heart rates is predictable by age.  The other 48% is unpredictable by age.  (Although it might be predictable by something else!)

As a quick summary:

- $s_e$, the standard deviation of the model residuals, represents an _absolute_ measure of our model's ability to fit the data  It tells us about unpredictable variation alone: that is, the typical size of a model error, in the same units as the original response variable.  
- $R^2$ represents a _relative_ measure of our model's ability to fit the data.  It a unit-less quantity that tells us how large the _predictable_ component of variation is, relative to the _total_ variation.  

## Beyond straight lines

Linear regression models describe additive change: that is, when x changes by 1 unit, you add to (or subtract from) y.  This is clear enough in our running heart-rate example: when you _add_ 1 year to age (x), you _subtract_ 0.7 BPM from your expected max heart rate (y).   We describe this kind of relationship with a _slope_: "rise over run," or the expected change in y for a one-unit change in x.  This kind of relationship is very common in the real world. For example:

- If you increase your electricity consumption by 1 kilowatt-hour, your energy bill in Texas will go up by about 12 cents, on average.  
- If you travel 1 mile further in a ride-share like an Uber or Lyft, your fare will go up by about \$1.50.  
- If you add 100 square feet to a house in Los Angeles, you'd expect its value to go up by about \$50,000.  

But not every quantitative relationship is like that.  We'll look at two other common kinds of relationships: _multiplicative_ change and _relative proportional_ change.  

### Exponential models {-}

Many real-world relationships are naturally described in terms of multiplicative change: that is, when x changes by 1 unit, you _multiply_ y by some amount.   These relationships are referred to as  _exponential growth_ (or _exponential decay_) models.  For example:  

- If your bank offers you interest of 5% on your savings account, that means your money grows not by a constant amount each year, but at a constant rate: when you _add_ 1 year to the calendar, you _multiply_ your account balance by 1.05.  If you start with more, you gain more each year in absolute terms.  
- In New York City's first pandemic wave in March/April 2020, each new day saw 22% more Covid cases than the previous day, on average.  This, too, represents multiplicative change: when you _add_ 1 day to the calendar, you _multiply_ expected cases by 1.22.  This reflects the way diseases spread through the population: the more infected people there are transmitting the disease, the more _new_ infections there will be the next day.      
- If you start with a given mass of Carbon-14 and wait 5,730 years, about half of what was there previously will remain, due to radioactive decay.  Once again, this is multiplicative change: when you _add_ 5,730 years to the calendar, you _multiply_ the mass of your remaining Carbon-14 by 0.5.  (Since the multiplier is less than 1, this is exponential decay.)  

We describe this kind of multiplicative change not in terms of a slope, but in terms of a _growth rate_---or, if the relationship is negative, a _decay rate._  This describes how y changes multiplicatively as x changes additively.  

Mathematically, the equation of an exponential model is pretty simple.  It looks like this:

\begin{equation} 
y = \alpha e^{\beta x} 
  (\#eq:exponential-model)
\end{equation} 

The two parameters of this model are:  

- $\alpha$, which describes the _initial model value_ of $y$, when $x=0$ (i.e. before $y$ starts growing or decaying exponentially).
- $\beta$, which is the growth rate (if positive) or decay rate (if negative).  You can interpret $\beta$ as like an interest rate.  For example, if $\beta = 0.05$, then $y$ grows at the same per-period rate as an investment account with 5% interest.  

An exponential model is nonlinear.  But the punch line here is that, via a simple math trick, we can actually use _linear_ regression to fit it.  That simple trick is to _fitting our model using a logarithmic scale_ for the y variable.  To see why, observe what happens if we take the (natural) logarithm of both sides of Equation \@ref(eq:exponential-model):

\begin{equation}
\begin{aligned}
\log y & = \log( \alpha e^{\beta x} ) \\
&= \log \alpha + \beta x
\end{aligned}
\end{equation} 

This is a linear equation for $\log$ y versus x, with intercept $\log \alpha$ and slope $\beta$.  The implication is that we can fit an exponential growth model using a linear regression for $\log$ y versus x.  Let's see two examples to walk through the details.  

#### Example 1: Ebola in West Africa {-}

Beginning in March 2014, West Africa experienced the largest outbreak of the Ebola virus in history.  Guinea, Liberia, Niger, Sierra Leone, and Senegal were all hit hard by the epidemic.  In this example, we'll look at the number of cases of Ebola in these five countries over time, beginning on March 25, 2014 (which will be out "zero date" for the epidemic).

Please download and import the data in [ebola.csv](data/ebola.csv) into RStudio.  The first six lines of the file look like this:  

```{r, echo=FALSE, message=FALSE}
ebola = read.csv("data/ebola.csv", header=TRUE)
```


```{r}
head(ebola)
```

This file breaks things down by country, but we'll look at the `totalSus` column, for total suspected cases across all five countries.  Let's visualize this in a line graph, starting from day 0 (March 25):  

```{r}
# total cases over time
ggplot(ebola) + 
  geom_line(aes(x=Day, y = totalSus))
```

This is clearly non-linear.  But if we plot this on a logarithmic scale for the y axis, the result looks remarkably close to linear growth:

```{r}
# total cases over time: logarithm scale for y variable
ggplot(ebola) + 
  geom_line(aes(x=Day, y = log(totalSus)))
```

So let's use our trick: fit the model using a log-transformed y variable (which here is `totalSus`):

```{r}
# linear model for log(cases) versus time
lm_ebola = lm(log(totalSus) ~ Day, data=ebola)
coef(lm_ebola)
```

So our fitted parameters are:  

- intercept = $\log(\alpha)$ = 4.54.  So our initial model value is $\alpha = e^{4.54} = 93.7$.  
- A slope of $\beta = 0.0216$.  This corresponds to a daily growth rate in cases of a little over 2.1%.    

We can actually add this "reference" line to our log-scale plot, to visualize the fit:  

```{r}
# total cases over time with reference line
ggplot(ebola) + 
   geom_line(aes(x=Day, y = log(totalSus))) + 
   geom_abline(intercept = 4.54, slope = 0.0216, color='red')
```

This emphasizes that the slope of the red line is the average daily growth rate over time.  


#### Example 2: Austin's population over time  {-}

Austin is routinely talked about as one of the [fastest-growing cities in America](https://kinder.rice.edu/urbanedge/2019/10/22/wallethub-austin-dallas-houston-fastest-growing-cities-2019)..  But just how fast is it growing, and how long has this growth trend been going on?

Please import the data in [austin_metro.csv](data/austin_metro.csv), which contains estimated population for the Austin metropolitan area all the way back to 1840.^[All population estimates from 1950 onwards are rounded to the nearest thousand people.]    Here are the first six lines of the file:


```{r, echo=FALSE, message=FALSE}
austin_metro = read.csv("data/austin_metro.csv", header=TRUE)
```

```{r}
head(austin_metro)
```

And here's a plot of Austin's population over time:

```{r}
ggplot(austin_metro) + 
   geom_line(aes(x=Year, y=Population))
```

Let's fit an exponential growth model to this.  We'll first define a new variable that treats 1840 as our "zero" date, called `YearsSince1840`.  As you can see, the data looks relatively linear on a logarithmic scale:  

```{r}
austin_metro = mutate(austin_metro, YearsSince1840 = Year - 1840)
ggplot(austin_metro) + 
   geom_line(aes(x=YearsSince1840, y=log(Population)))
```

So let's fit a linear model for log(Population) versus years since 1840:  

```{r}
lm_austin = lm(log(Population) ~ YearsSince1840, data=austin_metro)
coef(lm_austin)
```

Our average population growth rate is a little over 4.1% per year, every year back to 1840. Here's this reference line, superimposed on the log-scale plot of the data:  


```{r}
ggplot(austin_metro) + 
   geom_line(aes(x=YearsSince1840, y=log(Population)))  + 
   geom_abline(intercept = 7.14, slope = 0.0412, color='red')
```

As you can see, the fit is quite good.  There's some wobble around the trend line in the earlier years, but since at least 1915 (i.e. 75 years after 1840), Austin has grown at a nearly constant 4.1% annual rate.  


<!-- ```{r} -->
<!-- ggplot(austin_metro) +  -->
<!--    geom_line(aes(x=YearsSince1840, y=Population))  +  -->
<!--    scale_y_continuous(trans = "log", breaks = 10^{2:7}) + -->
<!--    geom_abline(intercept = 7.14, slope = 0.0412, color='red') -->
<!-- ``` -->


So to summarize:

- if your data look roughly linear when you plot log(y) versus x, then an exponential model is appropriate.  
- you can fit this exponential model by running a linear regression for log(y) versus x.  
- the estimated slope of this regression model is the growth rate (or, if negative, the decay rate) of your fitted exponential model.  


### Power laws {-}  

So far we've covered additive change (described by a linear model) and multiplicative change (described by an exponential model).  A third important type of relationship is _relative proportional change_: when x changes by 1%, you expect y to change by some corresponding percentage amount, independent of the initial size of $x$ and $y$.  These relationships are referred to as _power laws._  For example:  

- if you double the radius of a circle, you multiply it's area by 4, no matter how big the initial circle is.  This is just math, since $A = \pi r^2$.  
- if you increase your running speed by 1% (x), you will reduce the amount of time it takes to finish a race by 1% (y), regardless of the race distance or your initial speed.  
- if stores charge 1% more for Bud Light beer (x), on average consumers will buy about 2% fewer six-packs of it (y).  Bud Light, relatively speaking, is an elastic good: consumers respond strongly to price changes (e.g. by buying some other beer instead).  
- if oil companies charge 1% more for gasoline (x), on average consumers will buy about 0.05% to 0.1% fewer gallons of it (y).  Gasoline, relatively speaking, is an inelastic good: consumers respond weakly to price changes, because unless you're Doc in "Back to the Future," you can't run your car on beer (or anything else).    
- if someone's income goes up by 1% (x), on average they buy 2% fewer cups of cheap coffee from gas stations (y).  In economics, gas-station coffee is called an "inferior good."  This isn't coffee snobbery; to an economist, "inferior" just means that people buy less of something when they get richer.  
- if someone's income goes up by 1% (x), on average they buy 1.5% more cups of coffee from Starbucks (y).  In economics, Starbucks coffee is called a "luxury good," because people buy more of it when they get richer.

We describe this kind of change in terms of an _elasticity_, which measures how responsive changes in y are to changes in x.  Indeed, as some of the examples above illustrate, elasticities are a fundamental concept in economics.  

The basic equation of a power-law model is this:

\begin{equation} 
y = K x^{\beta} 
  (\#eq:power-law-model)
\end{equation} 

The two parameters of this model are:  

- $K$, which describes the _base value_ of $y$, when $x=1$.  
- $\beta$, which is the elasticity, which describes relative proportional change in y versus x.  In a power law, when x changes by 1%, y changes by $\beta$%, on average.    

A power law, like an exponential model, is also non-linear.  But via a similar trick to what we tried with exponential models, we can still _linear_ regression to fit it.  That simple trick is to _fitting our model using a logarithmic scale_ for both the x variable _and_ the y variable.  To see why, observe what happens if we take the (natural) logarithm of both sides of Equation \@ref(eq:power-law-model):

\begin{equation}
\begin{aligned}
\log y & = \log( K x^{\beta} ) \\
&= \log K + \log (x^\beta) \\
&= \log K + \beta \log x
\end{aligned}
\end{equation} 

This is a linear equation for $\log$ y versus $\log$ x, with intercept $\log K$ and slope $\beta$.  The implication is that we can fit a power-law model using a linear regression for $\log$ y versus $\log$ x.  Let's see two examples to walk through the details.  

#### Example 1: brain versus body weight {-}

Which animal has the largest brain?  Let's look at the data in [animals.csv](data/animals.csv) to find out.  This data set has the body weight (in kilos) and brain weight (in grams) for 34 different land animal species.

```{r, echo=FALSE}
animals = read.csv('data/animals.csv')
```


In one sense, it's obvious which animals in this data set have the largest brains... elephants!  We can see this by sorting the `brain` column in descending order:  

```{r}
# the largest brains
animals %>%
   arrange(desc(brain))  %>%
   head(5)
```

But a more interesting question is: which animal has the the largest brain, relative to its body weight?  Your first thought here might be to compute the ratio of brain weight to body weight:

```{r}
# compute brain to body weight ratio
animals = animals %>%
  mutate(brain_body_ratio = round(brain/body, 2))

# print the top 5 animals
animals %>%
   arrange(desc(brain_body_ratio)) %>%
   head(5)
```

We might expect that a primate like a rhesus monkey would have a relatively large brain.  But rats with the gold medal?  Moles with the bronze?  Somehow this doesn't seem right.   

What's going on here is that a straight-up ratio is misleading, because body weight and brain weight follow a nonlinear relationship.  But this isn't necessarily obvious from a standard scatter plot.  The problem is that these weights vary over many, many orders of magnitude:  we have lots of small critters and a couple of elephants:

```{r, out.width="100%", fig.align="center", fig.asp = 0.6}
ggplot(animals) + 
  geom_point(aes(x=body, y=brain))
```

As a result, all the points look "squished" in the lower left corner of the plot.  However, things look much nicer when plotted on a log-log scale.  The log transformation of both axes has stretched the lower left corner of the box out in both x and y directions, allowing us to see the large number of data points that previously were all trying to occupy the same space.

```{r, out.width="100%", fig.align="center", fig.asp = 0.6}
ggplot(animals) + 
  geom_point(aes(x=log(body), y=log(brain)))
```

This pattern---a linear relationship in log(y) versus log(x)---is characteristic of a power law.  So let's run our regression on this log-log scale:

```{r}
lm_critters = lm(log(brain) ~ log(body), data=animals)
coef(lm_critters)
```

The fitted slope tells us the _elasticity_: when an animal's body weight changes by 1%, we expect its brain weight to change by 0.75%, regardless of the initial size of the animal.  As you can see, the fit of this line looks excellent when superimposed on the log-log-scale plot:

```{r, out.width="100%", fig.align="center", fig.asp = 0.6}
ggplot(animals) + 
   geom_point(aes(x=log(body), y=log(brain))) + 
   geom_abline(intercept=2.2, slope=0.75, color='red')
```

Our fitted equation on the log scale is $\log(y) = 2.2 + 0.75 \cdot \log(x)$, meaning that our power law is:

$$
y = e^{2.2} \cdot x^{0.75} \approx 9 x^{0.75}
$$

We can also look at the residuals of this model to help us understand which animal has the largest brain, relative to what we'd expect based on body size:

```{r}
# add the residuals to the data frame
animals = animals %>%
   mutate(logbrain_residual = resid(lm_critters))

# print the top 5 animals by residual brain weight
animals %>%
   arrange(desc(logbrain_residual))  %>%
   head(5)
```

This looks a bit more like what we'd expect based on prior knowledge of "smart" animals.  

__Random aside__: when I first fit a power law to this data set, it told me that the [chinchilla](https://www.google.com/search?q=cute+chinchilla&hl=EN&tbm=isch) had the largest brain relative to its body weight, and that humans had the second largest brains.  I initially regarded the chinchilla with new-found respect, and I began trying to understand what made chinchillas so smart.  Did they have complex societies?  Did they have a secret chinchilla language?  I couldn't find anything.  As far as I could discern from my research, chinchillas were just cute, chunky ground squirrels that could [jump surprisingly high,](https://www.youtube.com/watch?v=5d60b22YRf8)  but were not surprisingly brainy.

Then I finally discovered what was wrong.  See this line in the data set, where it says the chinchilla brain weighs 6.4 grams, or about half a tablespoon of sugar?  

```{r}
animals %>% filter(animal == 'Chinchilla')
```

There was a typo in the original version.  Someone had typed in 64 grams of brain weight, rather than 6.4 grams, making chinchillas look 10 times more cerebral than they really were.  This is what the original plot of the data on the log-log scale looked like.  The chinchilla stands out as the largest residual:


```{r, echo=FALSE, out.width="100%", fig.align="center"}
knitr::include_graphics("images/chinchilla_before.png")
```

I discovered and corrected this error only after a lot of looking through obscure sources on mammalian brain weights.  The corrected plot looked like this, with the chinchilla much closer to the line and the human standing out as the largest residual:

```{r, echo=FALSE, out.width="100%", fig.align="center"}
knitr::include_graphics("images/chinchilla_after.png")
```

Lesson learned: if a result is surprising, consider double-checking your data for dumb clerical errors!    


#### The residuals in a power-law model {-}

As we've just seen, we can fit power laws using ordinary least squares after a log transformation of both the predictor and response.  In introducing this idea, we didn't pay much attention to the residuals, or model errors.  If we actually keep track of these errors, we see that the model we're fitting for the $i$th data point is this:

$$
\log y_i = \log \alpha + \beta_1 \log x_i + e_i \, ,
$$

where $e_i$ is the amount by which the fitted line misses $\log y_i$.   We previously suppressed these residuals to lighten the notation, but now we'll pay them a bit more attention.

Our equation with the residuals in it, $\log y_i = \log \alpha + \beta_1 \log x_i + e_i$, says that the residuals affect the model in an additive way _on the log scale_. But if we exponentiate both sides, we find that they affect the model in a multiplicative way on the original scale:

$$
\begin{aligned}
e^{\log y_i} &= e^{\log \alpha }\cdot e^{ \beta_1 \log x} \cdot e^{e_i} \\
y_i &= \alpha x^{\beta_1} e^{e_i}  \, .
\end{aligned}
$$

Therefore, in a power low, the exponentiated residuals describe the percentage error made by the model on the original scale.  Let's work through the calculations for three examples:

- If $e_i = 0.2$ on the log--log scale, then the actual response is $e^{0.2} \approx 1.22$ times the value predicted by the model.  That is, our model underestimates this particular $y_i$ by 22\%.
- If $e_i = -0.1$ on the log--log scale, then the actual response is $e^{-0.1} \approx 0.9$ times the value predicted by the model.  That is, our model overestimates this particular $y_i$ by $10\%$.
- If $e_i = 1.885$ on the log--log scale, then the actual response is $e^{1.88} \approx 6.55$ times the value predicted by the model.  That is, our model underestimates this particular $y_i$ by a multiplicative factor of 6.55.  This was the residual for humans in our brain-weight regression; humans' brains are 6.55 times larger than expected based on their body weight.  


The key thing to realize here is that the _absolute_ magnitude of the error will therefore depend on whether the $y$ variable itself is large or small.  This kind of multiplicative error structure makes perfect sense for our body--brain weight data: a 10\% error for a lesser short-trailed shrew will have us off by a gram or two, while a 10\% error for an elephant will have us off by 60 kilos or more.  Bigger critters mean bigger errors---but only in an absolute sense, and not if we measure error relative to body weight.
 

#### Example 2: demand for milk {-}

A very important application of power laws in economics arises when estimating supply and demand curves.

- A supply curve answers the question: as a function of price (P), what quantity (Q) of a good or service will suppliers provide?  When price goes up, suppliers are willing to provide more.  But how much more?
- A demand curve answers the question: as a function of price (P), what quantity (Q) of a good or service do consumers demand?  When price goes up, consumers are willing to buy less.  But how much less?

Both curves are characterized by elasticities.  One goal of microeconomics is to understand how these two curves define a market equilibrium price.  But our goal here is a bit simpler: given some data on price and quantity demanded, can we estimate a demand curve and the corresponding [price elasticity of demand](https://www.youtube.com/watch?v=J82_xd5XxXg&ab_channel=MarginalRevolutionUniversity)?

```{r, echo=FALSE}
milk = read.csv("data/milk.csv")
```

Please download and import the data in [milk.csv](data/milk.csv).  This data comes from something called a "stated preference" study, which is intended to measure someone's willingness to pay for a good or service.  The basic idea of this study is that you give participants a hypothetical grocery budget, say \$100, together with a "menu" of prices for various goods, and you ask them to allocate their fixed budget however they want.  By manipulating the menu prices---for example, by making milk notionally more expensive for some participants and less expensive for others---you can indirectly measure how sensitive people's purchasing decisions are to prices.  In [milk.csv](data/milk.csv), there are two columns: `price`, representing the price of milk on the menu; and `sales`, representing the number of participants willing to purchase milk at that price.  Each study participant saw three versions of the menu: one where milk was cheap, one where it was expensive, and one where it was somewhere in the middle.  But within those broad price bands, the exact numerical value of the prices each participant saw were jittered, so that in the aggregate, many different prices were represented.  

Here's a scatter plot of the data:

```{r milk-plot}
ggplot(milk) +
   geom_point(aes(x=price, y=sales))
```

As you can see, people are generally less willing to buy milk at higher prices.  Moreover, it becomes clear that a power law makes sense for this data when we plot it on a log-log scale: 

```{r milk-plot-log}
ggplot(milk) +
   geom_point(aes(x=log(price), y=log(sales)))
```

Remember, the characteristic feature of a power law is that it looks linear when plotting log(y) versus log(x), which is exactly what we see here.  So let's fit a linear model for log(sales) versus log(price):

```{r}
lm_milk = lm(log(sales) ~ log(price), data=milk)
coef(lm_milk)
```

Our fitted equation on the log scale is $\log(y) = 4.72 - 1.62 \cdot \log(x)$, meaning that our power law is:

$$
y = e^{4.72} \cdot x^{-1.62} \approx 112.2 \cdot x^{-1.62}
$$

Our estimated elasticity is about -1.62: that is, when the price of milk increases by 1%, consumers want to buy about 1.62% less of it, on average.  

### Summary {-}


Here's a quick table wrapping up our discussion of "Beyond straight lines."  

| Type of change  | Model | Example | Fitting in R | Coefficient on x| 
|:--------|:--------|:---------|:------------------|:---------|
| Additive              | Linear      | Heart rate vs. age | `lm(y ~ x, data=D)` | Slope |
| Multiplicative        | Exponential | Population vs. time  | `lm(log(y) ~ x, data=D)` | Growth rate |
| Relative proportional | Power law   | Demand vs. price  | `lm(log(y) ~ log(x), data=D)` | Elasticity | 

If I had to summarize this table in a single phrase, it would be: additive change on the log scale is the same thing as multiplicative change on the original scale.  As long as you understand that basic principle, the whole table should fall into place.  

## Study questions: fitting equations {-}

<!--chapter:end:07_basic_regression.Rmd-->

# (PART\*) Part II: Statistical inference {-}

# Statistical uncertainty

```{r, echo=FALSE}
set.seed(19820518)
knitr::opts_chunk$set(cache = T)
```



```{css, echo=FALSE}
.spoiler {
  visibility: hidden;
}

.spoiler::before {
  visibility: visible;
  content: "(Spoiler.)";
  color: #CC5500
}

.spoiler:hover {
  visibility: visible;
}

.spoiler:hover::before {
  display: none;
}
```



In data science, we reach conclusions using data and models.  The tools of __statistical inference__ are designed to help us answer the logical follow-up question: "how sure are you of your conclusions?"  Inference means providing a margin of error: some way of quantifying our uncertainty about what the data (or our model of the data) are capable of saying.  Instead of saying something like, "I think the answer is 10," you say something like, "I think the answer is 10, plus or minus 3."  That "3" is your margin of error.  The question is how big that margin of error should be, in light of the data.

In this lesson, you'll learn the core ideas behind statistical inference, including:  

- how uncertainty might arise in data science.  
- the core thought experiment of statistical inference.  
- how to use Monte Carlo simulation to simulate a data-generating process.  
- what a sampling distribution is.    
- what the standard error is, and what it's supposed to measure.  
- the limitations of statistical inference when reasoning about real-world uncertainty.  

Compared with the other lessons, this one is unusual, in that there's no real data here.  That's by design: the goal here is to take a step back, understand a few crucial ideas, and get our minds limbered up for the subsequent lessons, where we'll discuss practical tools for statistical inference on real-world problems.  

## Sources of uncertainty

There are many reasons why a data analysis might still leave us in a position of uncertainty, even if all the numbers have been calculated correctly.  Here's a nonexhaustive list of five such ways.       

__Our data consists of a sample__, and we want to generalize from facts about that sample to facts about the wider population from which it was drawn.  For instance, when a political pollster asks a sample of several hundred Americans about their voting intentions, the fundamental question isn't about the survey respondents _per se_, but about how the results from that survey can be generalized to the entire electorate.  This process of generalizing from a sample to a population is inherently uncertain, because we haven't sampled everyone.   

__Our data come from a randomized experiment.__  Take, for example, this excerpt from a published paper on a new therapy for stomach cancer, from the _New England Journal of Medicine_:

> We randomly assigned patients with resectable adenocarcinoma of the stomach, esophagogastric junction, or lower esophagus to either perioperative chemotherapy and surgery (250 patients) or surgery alone (253 patients)...  With a median follow-up of four years, 149 patients in the perioperative-chemotherapy group and 170 in the surgery group had died.  As compared with the surgery group, the perioperative-chemotherapy group had a higher likelihood of overall survival (five-year survival rate, 36 percent vs. 23 percent).^[Cunningham, et. al. Perioperative chemotherapy versus surgery alone for resectable gastroesophageal cancer. _New England Journal of Medicine_, 2006 July 6; 355(1):11-20.]

So the group randomized to receive the new therapy (the "perioperative-chemotherapy group") had a 13% higher survival rate.  But is that _because_ of the new therapy?  Or could it have just been dumb luck?  In other words, maybe the designers of the experiment randomized the healthier patients to get the new therapy, _just by chance_, and these patients were always going to have a higher survival rate no matter what therapy they got.  Historically, the tools of statistical inference have been designed to address concerns just like this.

__We want to use our data to make a prediction about the future, and we expect the future to be similar to the past.__  Imagine that your morning commute consists of walking to a metro station, taking a train, and then walking the rest of the way to work.  There are several sources of uncertainty here: how long you'll have to wait for the green "Walk" signs, how long it will take the train to show up, how crowded the stairs will be, etc.  The point is that you could never predict _exactly_ what your commute time is going to be.  The best you could do, if you were a meticulous record-keeper, would be to collect data on past commute times and come up with a decent probabilistic forecast for future commute time---something like, "There's a 90% chance that my commute will take between 34 and 41 minutes."  (If you've turned on location tracking on your phone, this is more or less what Google Maps tries to do if you ask it for a travel time tomorrow, so if you think that's creepy, you should turn off location tracking.)  Of course, if one day the city needs to make emergency repairs on your local metro line, or you trip and fall on the stairs and need to take a detour via the hospital, then all bets are off; the past data isn't useful for those situations.  That's why it's important to add the qualification that we expect the future to be similar to the past.    


__Our observations are subject to measurement or reporting error.__  Do you wear an old-school watch that isn't connected to the internet?  If so, compare your watch to [time.gov](https://time.gov/), which calculates the official time based on averaging a large number of absurdly accurate atomic clocks, and then corrects for the few milliseconds of network lag between those clocks and your computer to tell you __the__ time.   Compared with this time.gov benchmark, your watch is probably wrong, maybe even by a few minutes.  _That's measurement error._  Now, if you're using your non-smart watch to decide when you should leave for work, a small measurement error might not matter very much in practical terms---especially when compared with the inherent uncertainty of your commute time (see above).  But if you're using that same watch to time your viewing of [a total solar eclipse](https://nationaleclipse.com/maps.html), whose moment of arrival is known _very_ precisely and which doesn't last long, then an error of a minute or two actually does matter. 

Similar measurement errors arise in nearly every area of human inquiry, from particle physics to economics to disease monitoring.  How precisely, for example, do you think that the following quantities are typically measured?  

  (1) the speed of a proton in [CERN](https://twitter.com/CERN)'s particle accelerator.    
  (2) the number of people with the flu in Travis County, TX.  
  (3) a professional cyclist's [V02 max](https://en.wikipedia.org/wiki/VO2_max).  
  (4) the U.S. unemployment rate.   
  (5) the average price for a gallon of milk paid by American consumers.    
  
Most measurements are subject to at least some kind of error, and sometimes those errors are large enough to matter.


__Our data arise from an intrinsically random or variable process.__  Try this: count your pulse for 15 seconds.  Wait 5 minutes, then do the same thing again.  You'll likely get a slightly different number.  That's because a heart rate is not some fixed property of a person, like the number of chromosomes they have.  Rather, heart rate varies from one measurement to the next, and probably the best we can say is that each person has a "typical" resting heart rate that we might estimate, but never know with 100% accuracy, by averaging many measurements collected over time.  Moreover, the intrinsic variability in someone's heart rate makes it difficult to be certain about more complex quantities---such as, for example, the relationship between age and heart rate.


<br>

Like it or not, uncertainty is a fact of life.  Indeed, sometimes the only way to be certain is to be irrelevant.  Statistical inference means saying something intelligent about _how uncertain_ our conclusions actually are, in light of the data.  

## Sampling distributions

### Real-world vs. statistical uncertainty {-}

If you look up "uncertain" in the dictionary, you'll see something like:

> __uncertain__: not known beyond doubt; not having complete knowledge

We'll call this __real-world uncertainty__; it's what people mean when they use the term outside of a statistics classroom.   

But _inside_ a statistics classroom, or inside the pages of an academic journal where statistics are the coin of the realm, that's not what the term "uncertainty" means at all.  In fact, historically, the field of statistics has adopted a different, _very_ narrow---some have even called it narrow-minded---notion of what is actually meant by the term "uncertainty."  It's important that you understand what this narrow notion of __statistical uncertainty__ is, and what it is not.  At the end of this lesson, we'll talk about why this term has been defined so narrowly, and what the consequences of this narrow definition are for reasoning about data in the real world.  But the short version is this: anytime someone tells you how much _statistical_ uncertainty they have about something, they are understating the amount of _real-world_ uncertainty that actually exists about that thing. 

Let's get down to details.  In statistics, "certainty" is roughly equivalent to the term "repeatability."  Suppose, for example, that we take a random sample of 100 Americans, and we ask them whether, if offered the choice for dessert, they would prefer [huckleberry pie](https://www.food.com/about/huckleberry-170) or [tiramisu.](https://www.seriouseats.com/how-to-make-the-best-tiramisu)  The numbers shake out like this:

- Huckleberry pie: 52%
- Tiramisu: 48%  

So the data seem to say that more people like huckleberry pie.  But this is just a sample.  If we want to generalize these results to the wider population, then clearly we need a margin of error, just like they have in political polls.  We didn't ask all 300+ million Americans about their dessert preference, so we're at least a little bit uncertain how these survey results might generalize.

The following definition will help us talk about these ideas precisely.  

```{definition}
In data science, an __estimand__ is any fact about the world, or any fact about some idealized model of the world, that we're trying to learn about using data.  An __estimator__ is any statistical summary (sample mean, sample proportion, etc.) designed to estimate the estimand!  
```

<br>

In our dessert example, the _estimand_ is the true nationwide proportion of folks that prefer huckleberry pie to tiramisu, while the _estimator_ is the corresponding proportion based on a survey of size n=100.  Clearly our sample provides _some_ information about the population, but not perfect information.  Even after seeing the data, we're uncertain.  

But just how uncertain are we?  The statistician's answer to this question is to invoke the concept of repeatability.  _You're certain if your results are repeatable._  So to measure repeatability, and therefore certainty, you take a _different_ sample of 100 people, you ask the same question again, and you see how much your estimator changes.  And then you do it again, and again, and again, thousands of times! If you get something close to 52% every time, then you're pretty certain the true value of the estimand is about 52%.

OK, so that's not the actual procedure that an actual statistician would recommend that you follow.  It's ludicrously infeasible.  But it is the fundamental _thought experiment_ of statistical inference, which seeks to answer the question: 

> If our data set had been different merely due to chance, how different might our answer have been?

To oversimplify, but not by much: in statistical inference, we're "certain" of our answer if, when we ask the same question over and over again, we get the same answer each time.  If our answer fluctuates each time, we're uncertain---and the _amount_ by which those answers fluctuate from one another provides us a quantitative measure of our statistical uncertainty.


The best way I know to build intuition for the concept of statistical uncertainty is to actually run the thought experiment.  In other words, let's pretend we actually _can_ collect thousands of different data sets, by using a computer to repeatedly simulate the random process that generated our data.  (This approach of using a computer to simulate a random process is called _Monte Carlo simulation_, named after the casino capital of Europe.)  

Let's see this process play out on some examples.  To get started, please load the `tidyverse` and `mosaic` libraries:

```{r, message=FALSE}
library(tidyverse)
library(mosaic)
```


### Example 1: dessert {-}

Suppose it were actually true that, if offered the choice, 52% of all Americans would prefer huckleberry pie and 48% would prefer tiramisu.  Now imagine setting out to measure these numbers using a survey.  What range of plausible survey results might we expect to see if we asked 100 people about their preference?  If our survey is designed well, we'd hope that _on average_, it would yield the right result: that 52% of people prefer huckleberry pie.  This is the _estimand_, or fact about the wider population, that we seek to uncover.   But any individual survey could easily deviate from the right answer, just by chance.  The question is: by how much?  What kind of statistical fluctuations should we expect in our survey result?   

Let's find out.  We'll use R to simulate a single survey of size 100, using the `rflip` function.  This function is designed to simulate coin flips; in our simulation, we imagine that each survey respondent is like a metaphorical coin, and that the result of each coin flip represents some particular respondent's dessert preference.  In the code below, we've specified that we want to flip 100 "weighted" coins, and that the probability that any one coin comes up "H" is 0.52:

```{r, eval=FALSE}
rflip(100, prob=0.52)
```

```
## 
## Flipping 100 coins [ Prob(Heads) = 0.52 ] ...
## 
## H H H T T H H H H H H H H T H H H H H T H H T H T T H T T H H H H H H H
## T H T H H H H T T H T H T T T H H H H H H T H T T T T T T T H H H H H H
## T T H T T H T H T T T H T T H T T T T T T H T H H H H T
## 
## Number of Heads: 57 [Proportion Heads: 0.57]
```

In interpreting these results, we'll just pretend that, instead of "heads" and "tails", the letters stand for "huckleberry pie" and "tiramisu."

Here, it looks like 57% of our survey respondents said they preferred huckleberry pie.  Remember, the true population-wide proportion was assumed to be 52%, so the _right_ answer (estimand) and the _survey_ answer (estimator) differ by about 5%.^[Of course, if you run this code yourself, you'll get a slightly different simulated survey result.  This is expected---the result is random!]  This represents an error arising from natural statistical fluctuations in our sample.   

Is 5% a typical error, or did we just get very unlucky?  Remember the basic mantra of statistical inference: you're certain if your results are repeatable.  So let's repeat our hypothetical survey 10 times, using the following statement.  

```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r, eval=FALSE}
do(10)*nflip(100, prob=0.52)
```

```
##    nflip
## 1     58
## 2     47
## 3     55
## 4     59
## 5     50
## 6     52
## 7     57
## 8     53
## 9     46
## 10    51
```

The statement `nflip(100, prob=0.52)` says to flip a weighted coin 100 times and count the H's, while `do(10)*` says to repeat that whole process 10 times.  (Notice we use `nflip` rather than `rflip` so that we don't print out all the individual H's and T's.)  The result is 10 simulated dessert surveys from the same underlying population.  Each number represents how many H's, or "huckleberry pie" answers, we got in one survey of size n=100.  Or if you prefer: you can think of each number as the survey result in some imaginary parallel universe in which you surveyed a different random set of 100 Americans.  

The key thing to appreciate about these results is their variation.  When compared with the right answer of 0.52... 

- Sometimes the huckleberry pie folks are over-represented by chance (surveys 1, 3, 4, 7, 8).  
- Sometimes the tiramisu folks are over-represented by chance (surveys 2, 5, 9, 10).  
- Occasionally the survey answer is exactly right (survey 6).  

Moreover, if you inspect the numbers themselves, you get the sense that a random error of 5% is actually quite typical for a survey like this:  

- Some errors are bigger than 5%, i.e. further away than this away from 52/100 (surveys 1, 4, 9).  
- Some errors are smaller than 5% (surveys 3, 5, 6, 8, 10).
- Some errors are exactly 5% (surveys 2, 7).

So clearly 10 simulated surveys gives us some idea for the likely magnitude of random error we should expect from our survey.  But we can get an even better idea if we simulate 10,000 surveys, rather than just 10.  Of course, that's an awful lot of survey results to inspect manually.  So rather than printing the results to the console, we'll save them in an object called `dessert_surveys`:  

```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r}
dessert_surveys = do(10000)*nflip(100, prob=0.52)
```

If you look at the first several lines of this object we just created, you'll see that it has a single column, called `nflip`, representing the number of H's in each simulated survey:

```{r}
head(dessert_surveys)
```

Let's divide these numbers by 100 (the notional sample size of each survey) so we can interpret the results as a proportion, and then make a histogram of the results:  

```{r dessert-sampling-distribution, fig.cap="The sampling distribution for the proportion of H's (huckleberry pie) in our hypothetical dessert survey."}
dessert_surveys = dessert_surveys %>%
  mutate(huckleberry_prop = nflip/100)

ggplot(dessert_surveys) + 
  geom_histogram(aes(x=huckleberry_prop), binwidth = 0.01)
```

This histogram depicts _the_ central object of concern in statistical inference.  It is an example of what's called a __sampling distribution.__  Here's the full definition of this very important concept, which formalizes what we mean when we use terms like "repeatability" or "statistical fluctuations."    


```{definition}
The __sampling distribution__ of a statistical summary is the distribution of values we expect to see for that summary under repeated sampling from the random process that generated our data.    Said another way: the sampling distribution is the distribution of values from many imaginary parallel universes, where in each parallel universe we experience a single realization of the same random data-generating process.
```
<br>

If your data is the result of a random process, then any statistical summary you compute from that data will have a sampling distribution. In Figure \@ref(fig:dessert-sampling-distribution), the statistical summary in question is a proportion---specifically, the proportion of H's in a sample of size 100, assuming that the true population proportion of H's is 0.52.  

### What the sampling distribution tells us {-}

There are at least two important questions to ask about a sampling distribution, one about its center and one about its spread.  

First, __where is the sampling distribution centered?__  In Figure \@ref(fig:dessert-sampling-distribution), we see that our sampling distribution of survey results is centered on the right answer of 0.52.  That is: on average, our survey can be expected to get the right answer about how many Americans prefer huckleberry pie.

We formalize this notion in the following definition.^[Minor technical point: this isn't the same definition of expected value you would find in a probability textbook, which invokes the idea of a sum or integral under an assumed probability distribution.  So if you want to get technical: what I've given is actually is an informal "working definition" of expected value. Specifically, it's the one implied by the Law of Large Numbers, rather than the more fundamental definition in a probability textbook.  It is _possible_ for this working definition to break down, of course; if you want to hear about an example, find a physicist, grab a liter or two of coffee, and ask them about ergodicity and the difference between ensemble and time averages.  But those kind of technical details take us _way_ beyond the scope of these lessons.]  

```{definition}
The __expected value__ of a statistical summary is the average of that summary's sampling distribution---that is, the average value of that summary under repeated sampling from the same random process that generated our data.
```  
<br>

So we've seen in Figure \@ref(fig:dessert-sampling-distribution) that our sample proportion of H's has an _expected value_ of 0.52 under repeated sampling.  This is re-assuring.  But remember, any _individual_ survey will likely make an error, reflected in the spread of the distribution around its expected value.  So that brings us to the second question: __how spread out is the sampling distribution?__  The answer to this question provides a quantitative measure measure of repeatability, and therefore statistical uncertainty.  

You could certainly just eyeball the histogram in Figure \@ref(fig:dessert-sampling-distribution) and almost surely end up saying something intelligent: for example, that about 2/3 of all samples look within 5% of the right answer, or the vast majority (at least 9 out of 10) look within 10% of the right answer.   But the most common way of providing a quantitative answer to this question is to quote the standard deviation of the sampling distribution.  In fact, this number is so special that statisticians have given it a special name: the _standard error_.  

```{definition, name="standard error", label="standarderror"}
The standard deviation of a sampling distribution is called the __standard error__.  This reflects the typical statistical fluctuation of our summary statistic---that is, the typical magnitude of error of our summary, compared with the expected or average value of that summary under repeated sampling.
```  

<br>

We can easily calculate that the sampling distribution in Figure \@ref(fig:dessert-sampling-distribution) has a standard deviation of about 0.05, or 5%:

```{r}
dessert_surveys %>%
  summarize(std_err = sd(huckleberry_prop))
```

Said in English: the "typical" error our survey makes is about 5% either way from the right answer.  So if we wanted to quote a margin of error for our survey, a decent place to start might be "survey result, plus or minus 5%", or maybe even "survey result, plus or minus 10%." if we wanted to be more conservative and go out _two_ standard errors to either side of our estimate.  By adding a "plus or minus" based on the standard error, we're telling our audience not to expect any more precision than what the data are capable of offering.  

So to summarize a few important facts about sampling distributions:  

1) If your data is the result of a random process, then any statistical summary you compute from that data will have a sampling distribution.  
2) The sampling distribution represents the results of a thought experiment: specifically, how much your answer might change from one sample to the next under the same random process that generated your data.  It therefore measures the repeatability of your results.  
3) The typical way to summarize a sampling distribution is to quote its standard deviation, which we call the "standard error."  This number measures the typical size of the error your statistical summary makes, compared to its expected value.  


### Example 2: fishing {-}


In the prior example, our statistical summary was the proportion of those surveyed, out of 100, that preferred huckleberry pie. Let's now see an example where the statistical summary is a bit more complicated: the estimated slope in a regression model.   

Imagine that you go on a four-day fishing trip to a lovely small lake out the woods.  The lake is home to a population of 800 fish of varying size and weight, depicted below in Figure \@ref(fig:fishing-trips1).  On each day, you take a random sample from this population---that is, you catch (and subsequently release) 15 fish, recording the weight of each one, along with its length, height, and width (which multiply together to give a rough estimate of volume).  You then use the day's catch to compute a different estimate of the volume--weight relationship for the entire population of fish in the lake.  These four different days---and the four different estimated regression lines---show up in different colors in Figure \ref{fig:fishingtrips}.

```{r fishing-trips1, echo=FALSE, out.width="100%", fig.cap="An imaginary lake with 800 fish, together with the results of four different days of fishing."}
knitr::include_graphics("images/fishingtrips.png")
```

In this case, the two things we're trying to estimate are the slope ($\beta_1$) and intercept ($\beta_0$) of the dotted line in the figure.  I can tell you exactly what these numbers are, because I used R to simulate the lake and all the fish in it:

$$
E(y \mid x) = 49 + 4.24 \cdot x
$$

 This line represents the population regression line: the conditional expected value of weight ($y$), given measured volume ($x$), for all 800 fish in the lake.  Each individual sample of size 15 provides a noisy, error-prone estimate of this population regression line.  To distinguish these estimates from the true values of their corresponding estimands, let's call these estimates $\hat{\beta}_0$ and $\hat{\beta}_1$, where the little hats mean "estimates."  (Once, on a midterm exam, a student informed me that "$\hat{\beta}$ wears a hat because he is an impostor of the true value."  So if that helps you remember what the hat means, don't let me stop you.)  

Four days of fishing give us some idea of how the estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ vary from sample to sample.  But 2500 days of fishing give us a better idea.   Figure \@ref(fig:fishing-trips2) shows just this: 2500 different samples of size 15 from the population, together with 2500 different regression estimates of the weight--volume relationship.  This is another example of a Monte Carlo simulation, where we run a computer program to repeatedly simulate the random process that gave rise to our data (in this case, sampling from a population).

```{r fishing-trips2, echo=FALSE, out.width="100%", fig.cap="The same imaginary lake with 800 fish, together with the results of 2500 different days of fishing.  Each translucent grey line represents a single regression estimate from a single sample."}
knitr::include_graphics("images/fishingtrips2.png")
```

These pictures show the sampling distribution of the least-squares line---that is, how the estimates for $\beta_0$ and $\beta_1$ vary from sample to sample, shown in histograms in the right margin.  In theory, to know the sampling distributions exactly, we'd need to take an infinite number of samples, but 2500 gives us a pretty good idea.  The individual sampling distributions of the intercept and slope are shown in histograms, below.  

```{r fishing-trips3, echo=FALSE, out.width="100%", fig.cap="The sampling distributions of the intercept (left) and slope (right) from our 2500 simulated fishing trips."}
knitr::include_graphics("images/fishingtrips3.png")
```

Let's ask our two big questions about these sampling distributions.

(1) Where are they centered?  They seem to be roughly centered on the true values of $\beta_0 = 49$ and $\beta_1 = 4.24$.  This is reassuring: our sample estimates are right, on average.  
(2) How spread out are they?  We'll answer this by quoting the standard deviation of each sampling distribution, also known as the standard error.  The standard error of $\hat{\beta}_0$ is about 50, while the standard error of $\hat{\beta}_1$ is about $0.5$.  These represent the "typical errors" or "typical statistical fluctuations" when using a single sample to estimate the population regression line, and they provide a numerical description of the spread of the grey lines in Figure \@ref(fig:fishing-trips2).  


### Summary {-}

The figure below shows a stylized depiction of a sampling distribution that summarizes everything we've covered so far.

The sampling distribution depicts the results of a thought experiment.   Our goal is to estimate some fact about the population, which we'll denote generically by $\theta$; that's our _estimand_.  We repeatedly take many samples (say, 1000) from same random process that generated our data.  For each sample, we calculate our estimator $\hat{\theta}$: that is, some summary statistic designed to provide our best guess for $\theta$, in light of that sample.  

```{r samping-distribution-schematic, echo=FALSE, out.width="100%", fig.cap="A stylized depiction of a sampling distribution of an estimator."}
knitr::include_graphics("images/samplingdistribution_schematic.png")
```

At the end, we combine all the estimates $\hat{\theta}^{(1)}, \ldots, \hat{\theta}^{(1000)}$ into a histogram.  This histogram represents the sampling distribution of our estimator.^[Technically, the sampling distribution is the distribution of estimates we'd get with an infinite number of Monte Carlo samples, and the histogram based on 1000 Monte Carlo samples is an approximation of this distribution.   The error in this approximation is called "Monte Carlo error."  This type of error is usually negligible.]

We then ask two questions about that histogram.  First, where is the sampling distribution centered?  We hope that it's centered at or near the true value of the estimand, i.e. that our estimator gets roughly the right answer _on average_ across multiple samples.    

Second, how spread out is the sampling distribution?  We usually measure this by quoting the standard error, i.e. the standard deviation of the sampling distribution.  The standard error answers the question: "When I take repeated samples from the same random process that generated my data, my estimate is typically off from the truth by about how much?"  The bigger the standard error, the less repeatable the estimator across different samples, and the less certain you are about the results for any _particular_ sample.  


## The truth about statistical uncertainty

I hope you now appreciate these two big take-home lessons:  

- the core object of concern in statistical inference is the sampling distribution. 
- the sampling distribution measures our "statistical uncertainty," which is a very narrow definition of uncertainty, roughly equivalent to the notion of repeatability.  _You're certain if your results are repeatable._  You're uncertain if your results are subject to a lot of statistical fluctuations.  

Of course, we've focused only on intuition-building exercises involving simulated data.  We haven't _at all_ addressed the very practical question of how to actually find the sampling distribution in a real data-analysis problem.  Don't worry---that's exactly what the next few lessons are for.    

But to close things off, I'd to revisit two questions we deferred from the beginning of this lesson.

First, __why is statistical uncertainty defined so narrowly?__  Basically, because uncertainty is really hard to measure!  By definition, measuring uncertainty means trying to measure something that we don't know about.  In the face of this seemingly impossible task, the statistician's approach is one of humility.  We openly acknowledge the presence of many contributions to real-world uncertainty.  But we set out to make quantitative claims _only_ about the specific contributions to uncertainty that we know how to measure: the role of randomness in our data-generating process.  If we don't know how large the _other_ contributions to uncertainty might be, we don't try to make dubiously precise claims about them.  

Second, __what are the implications of this narrowness for reasoning about data in the real world?__  Basically, it means that while statistical uncertainty is a very useful concept, it isn't _always_ useful---and even when it is useful, it nearly always provides an under-estimate of real-world uncertainty.  It's important that you understand both the usefulness and the limitations of this central idea.  

### Example 1: commuting {-}

Let's return to our hypothetical commuting example from above, where we imagine that your morning commute consists of walking to a metro station, taking a train, and then walking the rest of the way to work.  The sources of _statistical_ uncertainty include the typical random fluctuations subject to repeated measurement over time, like:  

- how long you have to wait for each green "Walk" sign.  
- how long you have to wait on the metro platform.  
- how long the elevator takes to arrive.  
- whether rain affects your commute.  

You may have heard it said that "history doesn't repeat itself, but it does rhyme."  One working definition of "statistical uncertainty" might be: "uncertainty about the stuff that rhymes."

On the other hand, the sources of _non-statistical_ uncertainty on your morning commute include major systematic distortions or departures from the past, like:  

- whether they suddenly close the subway line for emergency repairs.    
- whether state-sponsored hackers break into the metro's computers and shut down the whole system.    
- whether a global pandemic forces you to work from home, rendering your commute time zero.   
- whether a piano-moving company accidentally drops a piano on your head while you're walking down the street, rendering your commute time infinite (and irrelevant).  

The point about this latter type of uncertainty is that the chance of any _particular_ crazy thing, like the global pandemic or the piano falling on your head, is so rare, and so difficult to quantify using data, that you probably never even considered it.  But the probability of _at least one_ crazy thing happening isn't necessarily negligible.  (See: Covid.)  This is a very real source of uncertainty.  It's just not what we measure when we measure _statistical_ uncertainty.

Moreover, while this is an important limitation, it isn't necessarily a _bad_ limitation.  And it's definitely not, [as some folks have tried to argue](https://en.wikipedia.org/wiki/The_Black_Swan:_The_Impact_of_the_Highly_Improbable), an indictment of the whole concept of statistical uncertainty.  After all, do you really _want_ Google Maps to give you a commute-time forecast that tries to incorporate all the crazy things that might happen?  If your goal is to decide when you should leave for work to be pretty sure that you'll make your 9 AM meeting, a forecast that says "your commute time today will be between zero and infinity, because pandemic or piano" is frankly a useless forecast.  

### Example 2: dessert again {-}

So now let's apply the same logic to our hypothetical dessert survey, which presents a more traditional kind of statistics problem.   The source of statistical uncertainty here is obvious: we survey 100 people randomly.  Therefore two random surveys would reach two different sets of 100 people, giving us two different answers.  We have to account for this by reporting error bars, based on the sampling distribution.  

But there are also some very real sources of _non-statistical_ uncertainty here as well, involving the possibility of major systematic distortions in our measurement process.  Roughly speaking, these are the survey equivalents of pandemics and dropped pianos, except that they're much more common.  For example:  

(1) Is our survey biased?  Maybe we systematically over-sampled people from the Pacific Northwest, where huckleberries actually grow, and where huckleberry pie is therefore popular.  In the survey world, this is called "sampling bias" or "design bias."  

(2) Are some people more likely to respond to our survey than others?  This can happen even if our survey was unbiased in whom it reached; in the survey world, it's called "nonresponse bias."  Maybe, for example, you got more responses from Boston, with a large population of tiramisu fans who believe that huckleberry pie is a threat to American dessert and are notably eager to share their reasoning.  Or maybe the people who like huckleberry pie are secretly embarrassed about their preference, and therefore less likely to answer honestly when asked about it by a stranger over the phone.  

(3) Did we ask a leading question?  Maybe rather than asking about "huckleberry pie," we instead used a British English term for the [same fruit](https://en.wikipedia.org/wiki/Huckleberry) and asked about "whortleberry pie" or "bilberry pie"---which, to an American ear at least, sound uncomfortably close to non-dessert words like "hurl" and "bilge," or perhaps at best like [desserts that Severus Snape might have eaten](https://www.thespruceeats.com/strange-sounding-british-cakes-and-puddings-435002).    No doubt this would bias the results in favor of tiramisu.  (And if we received funding to conduct our survey from the National Tiramisu Council, we might feel some pressure, whether consciously or unconsciously, to do this.)

(4) Might people's preferences change?  Maybe somebody tells you over the phone _today_ that they prefer huckleberry pie, but then when they show up next week at the dinner table and cast their vote in the big dessert election, they change their mind and say, "Tiramisu, please."    Hey, [it happens](https://fivethirtyeight.com/features/voters-really-did-switch-to-trump-at-the-last-minute/)!

The possible presence of a bias, or a change in the underlying reality, should make us uncertain in interpreting the results from our survey.  But these uncertainties are not directly measured by the sampling distribution---and by extension, they are not encompassed by the idea of statistical uncertainty.  The sampling distribution measures _random_ or _repeatable_ contributions to uncertainty.  It doesn't measure _systematic_ or _non-random_ distortions of our measurement process, even though these also contribute to our real-world uncertainty.  

The corollary is simple.  Whenever our measurement process has a large systematic bias that we don't know about, then our _non-statistical_ uncertainty can easily dwarf our _statistical_ uncertainty.  In fact, if the bias is large enough, then discussions of statistical uncertainty are, at best, a pointless academic exercise, and at worst, a dangerous distraction from the _real_ problem with our data.  (Of course, if we know about the bias, then we can correct for it, and then we're back to a problem where statistical uncertainty matters again.  These kinds of bias-correction techniques are a more advanced topic we'll consider soon.)  

For examples, you need look no further than the [2016](https://fivethirtyeight.com/features/the-polls-missed-trump-we-asked-pollsters-why/) and [2020](https://fivethirtyeight.com/features/why-was-the-national-polling-environment-so-off-in-2020/) U.S. presidential elections.  In the late stages of both races, the Democratic candidate looked to be far enough ahead of Donald Trump in the polls to be outside the "margin of error."  But the margin of error in a political poll only measures statistical uncertainty.  The real problem was _non-statistical_ uncertainty, i.e. bias that was hard to measure: the polls just didn't reach Trump voters as effectively as they did non-Trump voters.  To polling insiders, [this kind of systematic polling error was not a big surprise](https://fivethirtyeight.com/features/the-polls-werent-great-but-thats-pretty-normal/).  And the best election forecasters are actually [pretty good](https://fivethirtyeight.com/features/the-death-of-polling-is-greatly-exaggerated/) at taking account of these errors in stating their overall level of uncertainty about a future election.  But it seems clear that both journalists and the general public put too much faith in the "margin of error" from traditional polls, incorrectly assuming that this margin represents _all_ or even most forms of uncertainty about the actual election result.  


### When is statistical inference useful? {-}

So does that make the concept of statistical uncertainty useless?  Not at all!  It just requires us to put the tools of statistical inference in the proper context.  Statistical inference is not a magic flashlight that we can shine into every dark corner of the universe to illuminate all possible sources of our ignorance.  Rather, it comprises a suite of tools that _are_ effective at quantifying uncertainty in data-science situations that happen to be very common.

Which ones?  Well, let's return to the list from the beginning of this lesson.  Statistical inference is useful when:  

(1) Our data consists of a well-designed sample.  
(2) Our data come from a randomized experiment.  
(3) We want to use data to make a prediction about the future, and we expect the future to be similar to the past.
(4) Our observations are subject to measurement error.  
(5) Our data arise from an intrinsically random or variable process.  

This list is far from exhaustive, but it still covers an awful lot of situations involving data.

On the other hand, statistical inference is likely to be less useful (although maybe not completely useless) when:  

(1) We have data on the entire relevant population.  
(2) Our data come from a biased study or a convenience sample.  
(3) We have no interest in generalizing from conclusions about our data to conclusions about anything or anyone else.  
(4) We expect that the future may look very different from the past.   
(5) Systematic biases in our measurement process dwarf random, repeatable sources of uncertainty.  

This list _also_ covers a lot of situations involving data. Therefore, an important skill that every good data scientist needs is the ability to examine a claim and ask: "How useful are the tools of statistical inference in evaluating this claim?"  This often boils down to asking the follow-up question: "Where does the uncertainty here come from?"

#### Ten claims about the world {-}

Let's practice this skill.  I'll give you ten claims about the world involving data.  For each claim, I'll provide my reasoning as to why statistical inference might be more useful, or less useful, in evaluating that claim.  I'd encourage you to think about each claim first and come to your own judgment before hovering over (or tapping) the spoiler alert to reveal my take. You might even disagree on some of these, and if you've got a good reason, that's OK.  I'm just giving you my personal thoughts, not necessarily the "right answer," whatever that might mean here.

__Claim 1: Based on a survey of 172 students, we estimate that 15.1% of UT-Austin undergraduates have experienced food insecurity in the last 12 months.__  [The tools of statistical inference are  _very useful_ here.  The data comes from a sample of students, and the explicit goal is to generalize the survey result to a fact about the wider population.  There may be other sources of _non-statistical_ uncertainty here, like whether the survey was free of sampling bias or nonresponse bias, or how food insecurity was operationally defined on the survey.  But you absolutely can't ignore the statistical uncertainty arising from the sampling process.]{.spoiler}


__Claim 2: Of 8,459 UT-Austin freshmen who entered last fall, 25.5% were in their family’s first generation to attend college__.  [This is a claim about a specific population observed in its entirety: UT-Austin freshmen in fall 2020.  It makes no pretense to generalize to any other group.  Therefore, the tools of statistical inference are likely to be _less useful_ here.  (But if you wanted to compare rates across years, or make a prediction of what next year's data might show, the tools of statistical inference would suddenly become useful, because of the intrinsic variability in family circumstances across different cohorts of students.)]{.spoiler}


__Claim 3: I asked ten friends from my data science class, and they all prefer tiramisu to huckleberry pie.  There's simply no way that huckleberry pie can win the upcoming dessert election.__  [This is a convenience sample: you didn't sample a random set of 10 students in your class; you just asked your friends.  And even if it _were_ a random sample, it would still be biased: students in data science classes are unlikely to be representative of the wider American population.  The tools of statistical inference are _not useful_ here.  All this data point tells you is what you should serve your friends for dessert.]{.spoiler}


__Claim 4: Global average temperatures at the peak of the last Ice Age, about 22,000 years ago, were 4 degrees C colder than in 1990.__  [The fundamental source of uncertainty here is measurement error: how do we know what the temperature was thousands of years ago, when there wasn't a National Weather Service?  It turns out we know this [pretty well](https://www.smithsonianmag.com/blogs/national-museum-of-natural-history/2018/03/23/heres-how-scientists-reconstruct-earths-past-climates/), but not with perfect certainty.  The tools of statistical inference are _very useful_ here. ]{.spoiler} 

__Claim 5: Global average temperatures in 2100 are forecast to be 4 degrees C warmer than in 1990.__  [One fundamental source of uncertainty involves both measurement error and natural variability: how the global climate responds to changes in C02 emissions, and how well we can measure those changes.  Another fundamental source of uncertainty here is people's behavior: how much C02 we will emit over the coming decades.  The tools of statistical inference are _very useful_ for understanding the former, but _nearly useless_ for understanding the latter.  Overall, statistical inference is _somewhat useful_ for evaluating this claim, but likely doesn't speak to the dominant sources of uncertainty about the climate in 2100.]{.spoiler}  

__Claim 6: Teams in the National Football League (NFL) averaged 3.46 punts per game in 2020__.  [This claim purports to be nothing more than a fact about the past.  There is no attempt to generalize it to any other season or any other situation.  The tools of statistical inference are _not useful_ here.]{.spoiler}  

__Claim 7: Reported crimes on and around the UT-Austin campus are 10% lower this year than last year, and therefore on a downward trend.__  [Crime statistics reflect an intrinsically variable process; as criminologists will tell you, many crimes are "[crimes of opportunity](https://popcenter.asu.edu/sites/default/files/opportunity_makes_the_thief.pdf)" and reflect very little or no advanced planning.  Therefore we'd expect crime statistics to fluctuate from year to year, without those fluctuations _necessarily_ reflecting any long-term trend.  (In fact, we'd likely be quite skeptical of any official crime statistics that didn't exhibit these "steady-state" fluctuations; like with [Bernie Madoff](https://en.wikipedia.org/wiki/Bernie_Madoff) or the Soviet Union, suspicious consistency may be a sign that someone's cooking the books.)  To establish whether a 10% drop in crime represents a significant departure from the historical norm, versus a typical yearly fluctuation, we need more information, so that we can place a 10% drop in historical context.  For this kind of thing, statistical inference is _very useful._]{.spoiler}  

__Claim 8: Of 15,000 unique visitors to our website, we randomly assigned 1,000 to see a new version of the home page.  Among those 1,000 visitors to the new page,  sales were 3% higher by average dollar value per visitor.__  [These data come from a randomized experiment.  It's possible that a few more of the "gonna spend big anyway" folks were randomized to the new home page, just by chance.  If so, the experimental results don't reflect a difference attributable to the home page itself.  They simply reflect a pre-existing difference in which visitors saw which version of the page.  The tools of statistical inference are _very useful_ in evaluating which explanation---real difference versus chance difference---looks more plausible.]{.spoiler}  

__Claim 9: In a study of 1000 contraceptive users, 600 chose a hormonal method (e.g. the pill or a hormonal IUD), while 400 chose a non-hormonal method (e.g. condoms or a copper IUD).  Those who chose non-hormonal methods gained an average of 1.4 pounds of body weight during the study period, versus an average of 3.8 pounds for those who chose hormonal methods.__  [This claim is qualitatively similar to the basic style of argument in many published research papers: there's a scientifically plausible hypothesis (that hormonal contraception leads to mild weight gain), and some data that seems consistent with the hypothesis (that hormonal vs. non-hormonal contraceptive users experienced different weight-gain trajectories over the study period).  Statistical inference _might be_ useful for this problem; basically, it would help you assess whether the observed difference in weight gain could have been explained by random fluctuations in people's weight.  But that's probably not the most important source of uncertainty surrounding this claim.  The most important source of uncertainty is: __how comparable are the hormonal and non-hormonal contraceptive users?__  There's actually a very good reason to think they're not comparable at all: many contraceptive users have heard the claim that hormonal methods might cause weight gain and aren't sure whether it's true, and so _specifically avoid those methods_ if they're more concerned about their weight than the average person.  If this were the case---I don't know if it is, but it seems plausible---then the comparison of hormonal vs. non-hormonal users is really more like a comparison of "more weight-conscious" versus "less weight-conscious" people, and it would therefore be no surprise if the "more weight-conscious" group gained less weight on average over some fixed time interval.  Overall, statistical inference might be _somewhat useful_ or even _very useful_ for evaluating this claim, but it's quite hard to say how useful without more information about the study cohort.  If the groups are very non-comparable in their prior tendency to gain weight, then statistical inference is a pointless academic exercise here, unless you've got some way of correcting for the differences between the two groups.]{.spoiler}    


__Claim 10: As of May 3, 2021, the U.S. had reported 573,042 deaths due to Covid-19.  But this represents an under-count of the true, unknown Covid-19 death total through May 3.  We compared overall death statistics during Covid to what would have been expected based on past trends and seasonalities, and we estimate the true number of Covid deaths to have been 905,289.__  [You can find this exact claim [here.](http://www.healthdata.org/special-analysis/estimation-excess-mortality-due-covid-19-and-scalars-reported-covid-19-deaths)   These scientists tried to estimate under-counted Covid-19 deaths by measuring the "excess death rate" during the pandemic, week by week, compared to what would have been expected based on past trends and seasonality, and then adjusting for pandemic-driven changes in how many people died of things like drug overdoses, car accidents, suicide, or flu.  But their reported estimates didn't have any error bars, and the figure of 905,289 seemed suspiciously precise.  A journalist e-mailed to ask me about this, so I'll just tell you exactly what I told her:  "I appreciate very much what the authors here are trying to do, and I think the basic approach makes sense.  But especially in an effort like this, where one is trying to reconstruct a fundamentally unknown quantity based on other fundamentally unknown quantities, you need error bars.  For example, some of those excess deaths were drug overdoses or suicides indirectly attributable to the devastation wrought by Covid on people's lives and livelihoods, but not 'direct' Covid deaths that the authors are trying to measure.   These 'indirect' deaths have to be subtracted out.  But how many drug overdoses do you subtract out?  How many suicides?  No one can tell you for sure: yes, there's data on it, but those numbers are probably subject to at least as much reporting and measurement error as the official Covid deaths themselves.  Moreover, you need to _propagate_ those errors bars for more 'basic' or primitive quantities, like how many people died of a drug overdose, through a complex calculation, which should lead to even wider error bars for the derived quantity you care about: direct Covid deaths.  Error bars are what turn a back-of-the-envelope calculation into something that one can actually judge and engage with as a scientific endeavor."   I think statistical inference is _very useful_ for a question like this.]{.spoiler}  


## Study questions {-}  

<!--chapter:end:08_inference_ideas.Rmd-->

# The bootstrap  

```{r, echo=FALSE}
knitr::opts_chunk$set(cache = T)
```



```{css, echo=FALSE}
.spoiler {
  visibility: hidden;
}

.spoiler::before {
  visibility: visible;
  content: "(Spoiler.)";
  color: #CC5500
}

.spoiler:hover {
  visibility: visible;
}

.spoiler:hover::before {
  display: none;
}
```



In this lesson, you'll learn an important practical tool for statistical inference on real data-analysis problems, called the __bootstrap.__  Specifically, you'll learn about:  

- the bootstrap sampling distribution.    
- bootstrap standard errors and confidence intervals.  
- how the bootstrap usually, but not always, works well.  


#### A quick review {-}

Let's remind ourselves of where things stand.  In the previous lesson, we learned about the core ideas of statistical inference.  Specifically, we learned how the sampling distribution formalizes the concept of "statistical uncertainty" in terms of a thought experiment: what would happen if we could run the same analysis in many imaginary parallel universes, where in each parallel universe we experience a single realization of the same random data-generating process?  Here's the picture we saw in the last lesson, to illustrate this idea:  

```{r sampling-distribution-schematic-recap, echo=FALSE, out.width="100%", fig.cap = "A review of the thought experiment behind a sampling distribution."}
knitr::include_graphics("images/samplingdistribution_schematic.png")
```

Of course, if you really _could_ peer into all those parallel universes, each with its own sample from the same data-generating process, life would be easy.  By examining all the estimates from all those different samples and seeing how much they differed from one another, you could calculate a standard error.  You could then report a measure of uncertainty: "My estimate should be expected to err by about ____," where you fill in the blank with the standard error you calculated.^[Let's ignore the obvious fact that, if you had access to all those parallel universes, you'd also have more data.  The presence of sample-to-sample variability is the important thing to focus on here.]  

In reality, however, we're stuck with one sample.  We therefore _cannot ever know the actual sampling distribution_ of an estimator, for the same reason that we cannot peer into all those other lives we might have lived, but didn't:

> Two roads diverged in a yellow wood,  
> And sorry I could not travel both  
> And be one traveler, long I stood  
> And looked down one as far as I could  
> To where it bent in the undergrowth.  
> -- Robert Frost, _The Road Not Taken_, 1916

Quantifying our uncertainty would seem to require knowing all the roads not taken---an impossible task.  

So in light of that you might ask, rather fairly: what the hell have we been doing this whole time?  Here's what.  Surprisingly, we actually can come quite close to performing the impossible.  There are two ways of feasibly constructing something like the histogram in Figure \@ref(fig:sampling-distribution-schematic-recap), thereby approximating an estimator's sampling distribution---all without ever taking repeated samples from the true data-generating process.  

(1) Resampling: that is, by pretending that the sample itself represents the population, which allows one to approximate the effect of sampling variability by resampling from the sample.  

(2) Mathematical approximations: that is, by recognizing that the forces of randomness obey certain mathematical regularities, and by drawing approximate conclusions about these regularities using probability theory.

In this lesson, we'll discuss the resampling approach at length, covering the mathematical approach in a later lesson and at a fairly coarse level of detail (i.e. with almost no math involved).  


## The bootstrap sampling distribution


At the core of the resampling approach to statistical inference lies a simple idea. Most of the time, we can't feasibly take repeated samples from the same random process that generated our data, to see how our estimate changes from one sample to the next.  But we can repeatedly take _resamples from the sample itself_, and apply our estimator afresh to each notional sample.  The variability of the estimates across all these resamples can be then used to approximate our estimator's true sampling distribution.

This process---pretending that our sample represents some notional population, and taking repeated samples of size $N$ with replacement from our original sample of size $N$---is called __bootstrap resampling__, or just __bootstrapping__.^[The term "bootstrapping" is a metaphor, from an old-fashioned phrase that means performing a complex task starting from very limited resources.  Imagine trying to climb over a tall fence.  If you don't have a ladder, just "pull yourself up by your own bootstraps."]  

Why would this work?  Remember that uncertainty arises from the randomness inherent to our data-generating process (be it sampling variability, measurement error, whatever).  So if we can _approximately_ simulate this randomness, then we can _approximately_ quantify our uncertainty.  That's the goal of bootstrapping: to approximate the randomness inherent to data-generating process, so that we can simulate the core thought experiment of statistical inference.

Here's what bootstrapping looks like in a picture.  

```{r bootstrapping-schematic, echo=FALSE, out.width="100%", fig.cap = "A stylized depiction of a bootstrap sampling distribution of an estimator."}
knitr::include_graphics("images/bootstrapping_schematic.png")
```

Each block of $N$ resampled data points is called a "bootstrap sample."  To bootstrap, we write a computer program that repeatedly resamples our original sample and recomputes our estimate for each bootstrap sample.  As we'll see shortly, R makes a non-issue of the calculational tedium involved in actually doing this. 

There are two key properties of bootstrapping that make this seemingly crazy idea actually work.  First, each bootstrap sample must be of the same size (N) as the original sample. Remember, we have to approximate the randomness in our data-generating process, and the sample size is an absolutely fundamental part of that process.  If we take bootstrap samples of size N/2, or N-1, or _anything else other than N_, we are simulating the "wrong" data-generating process.  

Second, each bootstrap sample must be taken __with replacement__ from the original sample.  The intuition here is that each bootstrap sample will have its own random pattern of duplicates and omissions compared with the original sample, creating _synthetic_ sampling variability that approximates _true_ sampling variability.    

To understand _sampling with replacement,_ imagine a lottery drawing, where there's a big bucket with numbered balls in it.  We choose 6 numbers from the bucket.  After we choose a ball, we could do one of two things: 1) put the ball to the side, or 2) record the number on the ball and then throw it back into the bucket.  If you set a ball aside after it's been chosen, it can be chosen only once; this is sampling _without_ replacement, and it's what happens in a real lottery.  But if instead you put the ball back into the bucket, it has a some small chance of being chosen again, and therefore being represented more than once in the final set of 6 lottery numbers.  This is sampling _with_ replacement, and it's what we do when we bootstrap.

Let's see how this "sampling with replacement" process works in the context of bootstrapping, and why it's important.  In the cartoon below, the left panel shows a hypothetical sample of size N=5, while the right panel shows a bootstrap sample from this original sample.  

```{r, echo=FALSE, out.width="50%", fig.align='center'}
knitr::include_graphics("images/bootstrap_cartoon1.png")
```

Notice the two key properties of our bootstrap sample: (1) the bootstrap sample is the same size (N=5) as the original sample; and (2) the bootstrap sample was taken _with replacement_, and therefore has a random pattern of duplicates and omissions when compared with the original sample.  Specifically, the red 4 has been omitted entirely, while the green 7 has been chosen twice.

Why does this matter?  Well, let's see what happens when we calculate the sample mean of the bootstrap sample, versus the original sample:  

- sample mean of original sample: $(4 + 5 + 4 + 7 + 2)/5 = 4.4$
- sample mean of bootstrap sample: $(4 + 2 + 7 + 5 + 7)/5 = 5$

And this is the core fact to notice: when we compute a summary statistic for the bootstrap sample, we won't necessarily get the same answer as we did for the original sample.  

Let's see another bootstrap sample, to emphasize this variability:

```{r, echo=FALSE, out.width="50%", fig.align='center'}
knitr::include_graphics("images/bootstrap_cartoon2.png")
```

Now the orange 4 and the green 7 have been omitted, while both the purple 5 and blue 2 have been duplicated.  As a result, we get a different sample mean:  

- sample mean of original sample: $(4 + 5 + 4 + 7 + 2)/5 = 4.4$  
- sample mean of 1st bootstrap sample: $(4 + 2 + 7 + 5 + 7)/5 = 5$
- sample mean of 2nd bootstrap sample: $(4 + 5 + 5 + 2 + 2)/5 = 3.6$

Let's see one final bootstrap sample, just to fix our intuition for what's going on:

```{r, echo=FALSE, out.width="50%", fig.align='center'}
knitr::include_graphics("images/bootstrap_cartoon3.png")
```

A third different pattern of duplicates and omissions, and a third different bootstrap sample mean.  By sheer luck, this one happens to be the same as the mean of the original sample:   

- sample mean of original sample: $(4 + 5 + 4 + 7 + 2)/5 = 4.4$  
- sample mean of 1st bootstrap sample: $(4 + 2 + 7 + 5 + 7)/5 = 5$
- sample mean of 2nd bootstrap sample: $(2 + 4 + 5 + 4 + 7)/5 = 3.6$
- sample mean of 3rd bootstrap sample: $(2 + 4 + 5 + 4 + 7)/5 = 4.4$

This is the core mechanism by which bootstrapping works: resampling creates synthetic variability in our statistical summaries, in a way that's design to approximate true sampling variability.   If we repeat this process thousands of times, some summaries will be too high, some will be too low, and a few will be just right when compared with the answer from the _original_ sample.  The point is, the bootstrap summaries differ from one another---and the _amount_ by which they differ from one another provides us a quantitative measure of our statistical uncertainty.

And that's the basic idea:  

- You're certain if your results are repeatable under different samples from the same random data-generating process.  
- Bootstrapping lets you measure the repeatability of your results, by approximating the process of sampling randomly from the wider population.  

#### The core assumption of the bootstrap {-}

The core assumption of the bootstrap is that the randomness in your data, and therefore the statistical uncertainty in your answer, arises from the process of sampling.  That is, the bootstrap implicitly assumes that your samples can be construed as random samples from some wider reference population.  While the bootstrap isn't explicitly designed for anything else, it's actually provides a pretty good approximation for _other_ common forms of randomness as well.^[This isn't a theorem or anything; it's just something I've noticed because I use the bootstrap a lot, and I therefore end up comparing it to more "bespoke" options in a bunch of different situations.  For the kinds of problems that we tend to look at in an intro data science course, it's a rare case that I notice much of a practical difference between the answer I get from the bootstrap and the answer I get from some other, more clever, more situation-specific technique for inference.]  These include:  

- experimental randomization.  
- measurement error.  
- intrinsic variability of some natural process (e.g. your heart rate).  

So while we'll motivate the bootstrap as an approximation to random sampling, we'll actually use it in a broader set of situations, where random sampling isn't necessarily the reason we face statistical uncertainty about our answers.^[There are varieties of the bootstrap designed for other forms of randomness, including the _parametric bootstrap_ and the _residual resampling_ bootstrap.  There's also a technique called the _randomization test_ or _permutation test_ that has a very bootstrap-like feel, and is explicitly designed to measure statistical uncertainty association with experimental randomization.]


## Bootstrapping summaries

```{r, echo=FALSE, message=FALSE}
NHANES_sleep = read.csv('data/NHANES_sleep.csv', header=T)
```

Let's bootstrap! Please load the `tidyverse` and `mosaic` libraries:

```{r, message=FALSE}
library(tidyverse)
library(mosaic)
```

Please also download and import the data in [NHANES_sleep.csv](data/NHANES_sleep.csv).  This file contains a sliver of data from the National Health and Nutrition Examination Survey, known as NHANES.  [NHANES](https://www.cdc.gov/nchs/nhanes/about_nhanes.htm) is a major national survey run by the US Centers for Disease Control and Prevention (CDC).  Per the CDC, it is "designed to assess the health and nutritional status of adults and children in the United States."  It is intended to be a nationally representative survey whose results can generalized to the broader American population---and indeed, it is basically as good as surveys ever get in that regard, at least outside the pages of a statistics textbook. Moreover, NHANES is unusually comprehensive as far as health surveys go, in that it combines both interviews and physical exams.

Here, we'll be looking at a subset of 1,991 survey participants from the 2011-12 version of the survey, focused on a few questions surrounding sleep and depression.  (The full data set has a _lot_ more information than what you'll see in this file.)  The `NHANES_sleep` file contains information on people's gender, age, self-reported race/ethnicity, and home ownership status.  It also has a few pieces of health information: 1) the self-reported number of hours each study participant usually gets at night on weekdays or workdays; 2) whether the respondent has smoked 100 or more cigarettes in their life (yes or no); and 3) the self-reported frequency of days per month where the participant felt down, depressed or hopeless, where the options are "None," "Several", "Majority" (more than half the days), or "Almost All".

The first five lines of the data file look like this.

```{r}
head(NHANES_sleep, 5) 
```

### Example 1: sample mean {.unnumbered #bootstrap-sample-mean}  

The first question we'll address is: how well are Americans sleeping, on average?  Our first pass here might be to plot the data distribution for the `SleepHrsNight` variable in a histogram:

```{r, eval=FALSE}
ggplot(NHANES_sleep) + 
  geom_histogram(aes(x = SleepHrsNight), binwidth=1)
```

```{r, echo=FALSE}
ggplot(NHANES_sleep) + 
  geom_histogram(aes(x = SleepHrsNight), binwidth=1) + 
  scale_x_continuous(breaks=1:13)
```

So it looks like the sample mean is somewhere around 7 hours per night, but with a lot of variation around that mean: some people sleep a lot more than 7 hours, and some people sleep a lot less. To calculate the mean a bit more precisely, we'll use one of the "shortcut" functions that we learned in the section on [Summary shortcuts], a few lessons back, to calculate the average number of hours per night that our survey respondents said they slept:

```{r}
mean(~SleepHrsNight, data=NHANES_sleep)
```

According to the survey, it's 6.88 hours per night, on average.  But remember, this is just a survey---a very well-designed survey, but a survey nonetheless.  We clearly have some uncertainty in generalizing this number to the wider American population.

How much?  To get a rough idea, let's take a single bootstrap sample to simulate the randomness of our data-generating process, like this:  

```{r, eval = FALSE}
NHANES_sleep_bootstrap = resample(NHANES_sleep)
mean(~SleepHrsNight, data=NHANES_sleep_bootstrap)
```

```
## [1] 6.833385
```

Taking each line in turn:  

- the first line says to `resample` the `NHANES_sleep` data set, and to store the result in an object called `NHANES_sleep_bootstrap`.  (By default, `resample` takes a sample with replacement of the same size as your original sample, which are the two key requirements of bootstrapping.)  
- the second line says to calculate the mean of the `SleepHrsNight` variable for the bootstrap sample (i.e. _not_ for the original sample).  

The result is about 6.83 hours, on average.  This differs from 6.88, the mean of the original sample, by about 0.05 hours, or 3 minutes.  Remember, this difference represents a sampling error.  (Or more precisely, it represents a _bootstrap_ sampling error, which is an approximation to an _actual_ sampling error.)  So we've already learned something useful: our survey result of 6.88 hours per night could easily differ from the true population average by 3 minutes, just because of the uncertainty inherent to sampling.  We know this is possible because we saw a 3-minute error happen right before our eyes!

I don't know about you, but my reaction to that number is that 3 minutes seems like a pretty small error, at least compared with an average sleep of nearly 7 hours.  It seems to suggest that our statistical uncertainty is pretty small---in other words, that we know the average sleep time of the American population in 2011-12 pretty precisely based on the NHANES survey.  

But of course, that error of 3 minutes, or 0.05 hours, was based on just a single bootstrap sample.  Was 3 minutes a _typical_ sampling error?  Or might it instead have been an abnormally small (or abnormally large) sampling error?

Let's run 10,000 bootstrap samples to find out.  It turns out that we can do this in a single line of R code that combines `do` with `resample`.  (Remember `do` from our prior lesson on [Sampling distributions].)  The following line might take ten seconds or so to execute on your machine, so be patient:



```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r}
boot_sleep = do(10000)*mean(~SleepHrsNight, data=resample(NHANES_sleep))
```

This statement says to: 1) create 10,000 bootstrap samples from the original `NHANES_sleep` data frame, and 2) for each bootstrap sample, recompute the mean of the `SleepHrsNight` variable.  We store the resulting set of 10,000 sample means in an object called `boot_sleep`.  

What does this `boot_sleep` object look like?  Let's examine the first several lines:

```{r}
head(boot_sleep)
```

We see a single column called `mean`, with each entry representing the mean for a single bootstrap sample.  Let's make a histogram of these 10,000 different means:

```{r, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_sleep) + 
  geom_histogram(aes(x=mean))
```

This histogram represents our _bootstrap sampling distribution_, which is designed to approximate the _true_ sampling distribution we talked about in the previous lesson.

### Bootstrap standard errors and confidence intervals {-}

So now what do we do with this sampling distribution?  The answer is: we ask the same two questions we're supposed to ask about any sampling distribution!

First, where is it centered?  It looks to be centered around 6.88 hours of sleep, which you'll recall is the sample mean from our original sample.  This makes sense, since by the logic of bootstrapping, the original sample is our surrogate for the population.  This is reassuring: it's telling us that the sampling process seems to be getting the "right" answer on average.  Of course, we don't know the right answer for the population, but we do know the right answer for our original sample.  And this "original sample answer" is exactly what we'd hope to see from the average of the bootstrap samples.  The logic here is, roughly: if sampling from the _original sample_ gets the _original sample's_ answer right, on average, then we feel reassured that sampling from the _population_ will get the _population's_ answer right, on average (even if we don't know what the population answer is).  

Second, how spread out is the sampling distribution?  Let's calculate a standard error:

```{r}
boot_sleep %>%
  summarize(std_err_sleep = sd(mean))
```

So it looks like a "typical" sampling error is about 0.03 hours, or roughly 2 minutes.  We can conclude that we know the average sleep time of the American population pretty precisely based on the NHANES survey: it's 6.88 hours, up to an error of about 0.03 hours, or two minutes. In other words, if we actually _could_ ask everyone in America, we feel pretty confident that the average of all their answers would fall somewhere in the range 6.88 $\pm$ 0.03 hours.  

We said we're "pretty confident" that the answer is 6.88 $\pm$ 0.03, but "pretty" confident is a bit of a weasel word.  Just _how_ confident are we?  To answer, let's examine the histogram again, and ask what proportion of sampling errors seem to be 0.03 or less.

```{r, echo=FALSE, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_sleep) + 
  geom_histogram(aes(x=mean)) + 
  geom_vline(xintercept=c(6.878955 - 0.03, 6.878955 + 0.03), color='red', size=2)
```

The area between the red lines corresponds to bootstrap sampling errors of 0.03 or less (that is, within one standard error of the middle of the sampling distribution).  Just eyeballing it, I'd say that roughly two-thirds of the bootstrap means fall in this window.  Therefore, again just eyeballing it, I'd say that I'm about 65-70% confident that the average American's sleep time falls in the interval 6.88 $\pm$ 0.03 hours.

Of course, we can use `summarize` to check, if we want to be sure:

```{r, eval = FALSE}
boot_sleep %>%
  summarize(prob_within_1se = sum(mean > 6.88 - 0.03 & mean < 6.88 + 0.03))
```

```
##   prob_within_1se
## 1            6825
```

That's 6,825 out of 10,000, or 68%.   Therefore, we'd refer to the interval 6.88 $\pm$ 0.03 as a 68% confidence interval.  In other words: our best guess is 6.88, and we're 68% confident that our guess is within 0.03 hours of the truth either way.  (Of course, you might get a slightly different number, like 67% or 69%, due to Monte Carlo variability.)

Now, you might object to one thing here, purely on the grounds of sociability: normal people don't walk around saying things like "I'm 68% confident that so and so is true."  I mean, 68%?  It just isn't done.  Of course there's nothing _wrong_ with 68% as a confidence level; it's just that most people will regard it as a weirdly specific number, and possibly regard _you_ as a weirdly specific individual, if you say it out loud.  Moreover, even if you're immune to such conformist pressure, you might also legitimately object that 68% is simply _too low_, that serious people will not tolerate such wishy-washiness, and that you therefore want to express your conclusions with a higher level of confidence.

So how does 95% sound?  It's a big, round number, which people seem to like---and moreover, it's also a number that falls just enough short of 100% so that you won't look like a _complete_ idiot if you end up being wrong.  Perhaps for these reasons, 95% is a conventional choice of confidence level that won't raise any eyebrows.  So let's roll with it, using the `confint` function:

```{r}
confint(boot_sleep, level=0.95)
```

This is a 95% confidence interval for the average weekday sleep time of the American population.  It says that our best guess is about 6.88 hours per night, and that we're 95% confident that the right answer is somewhere between 6.82 and 6.94, or equivalently, 6.88 $\pm$ 0.06.  

It might help to see this 95% confidence interval superimposed on the bootstrap sampling distribution.  Here it is:

```{r, echo=FALSE, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_sleep) + 
  geom_histogram(aes(x=mean)) + 
  geom_vline(xintercept=c(6.878955 - 0.06, 6.878955 + 0.06), color='red', size=2)
```

It's basically like taking a little axe and chopping off 2.5% of the bootstrap sampling distribution in either tail, leaving the middle 95% as your confidence interval.  

You might notice that this 95% interval ends up covering _two_ standard errors, or 0.06 hours, to either side of your sample estimate of 6.88.  This specific numerical correspondence (68% for 1 standard error, 95% for 2) happens frequently enough to warrant a rule of thumb:

> __onfidence interval rule of thumb:__ a 95% confidence interval tends to be about two standard errors to either side of your best guess.  A 68% confidence interval tends to be about one standard error to either side of your best guess.

But when in doubt, you can always use `confint` at whatever level you choose.  


### The biggest bootstrapping gotcha {-}

Let's talk about the biggest "gotcha" in bootstrapping. It concerns the following fact: that the _data distribution_ of some variable (like `SleepHrsNight`) and the _sampling distribution_ for the mean of that variable are two fundamentally different things.  The biggest bootstrapping "gotcha" turns on conflating the two.  

In fact, I've learned over the years that if the students in my data science class are crushing it, and if I fear that the dean will start to yell at me about grade inflation more loudly than the students will yell at me about their B+, then I can always buy myself some breathing room by throwing in a question along these lines on the midterm.  Mean, you say?  Not at all; it's not like this is a trick question.  You cannot possibly fall for this particular "gotcha" unless you exhibit an absolutely fundamental misunderstanding about the bootstrap, and about the sampling distribution more generally.

Let's see an example "gotcha."  The setup here is going to feel familiar, because it's precisely the NHANES survey we've already been talking about.  Suppose we have a random sample of 1,991 Americans, and we ask each of them how many hours of sleep they get per night.  We calculate the average as 6.88 hours, and then we bootstrap the sample in order to form a bootstrap sampling distribution:

```{r, eval=FALSE}
boot_sleep = do(10000)*mean(~SleepHrsNight, data=resample(NHANES_sleep))
```

We examine the sampling distribution and compute a 95% confidence interval, like this:

```{r, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_sleep) + 
  geom_histogram(aes(x=mean))

confint(boot_sleep, level = 0.95)
```

So here's the question.  I strongly suggest that, as a self-diagnostic, you answer for yourself before you click on the spoiler alert.  

__True or false:__ this histogram, and the associated confidence interval, tell us that about 95% of all Americans sleep somewhere between 6.82 and 6.94 hours on an average night (rounded to two decimal places).

[FALSE, FALSE, 10,000 times FALSE. Hey, look, I'm not disappointed in you or anything if you thought the answer was "true."  It's a _very common_ misunderstanding and there's no shame in it.  But if you're one of those folks who thought this statement was true, I will level with you: there's a very good chance that you need to go back to the beginning of the lesson on [Statistical uncertainty] and start over from there, proceeding very carefully until you reach this point again.  In the lessons to come, _almost nothing_ is going to hang together for you until you understand why "true" is the wrong answer---or, even better, how you could amend the statement so that it actually _is_ true.  <br> <br> So why is this statement false?  The reason is that the sampling distribution isn't telling us about variation in individual Americans' sleep habits.  That's what the _data distribution_ tells us.  Instead, the sampling distribution of the mean is telling about the _level of precision_ with which we can estimate the mean of the population, based on the mean of the data distribution.  The population mean collapses a huge amount of individual variation into a single number.   So a correct statement would be: "This histogram, and the associated confidence interval, tell us that we can be 95% confident based on this sample that the population average sleep time is between 6.82 and 6.94 hours per night (rounded to two decimal places)."]{.spoiler}

To emphasize the difference between the data distribution and the sampling distribution, let's actually look at the data distribution once again.  This is the same histogram we saw at the very beginning of the NHANES example.  It is the distribution of `SleepHrsNight` among all 1,991 survey respondents:  

```{r, eval=FALSE}
ggplot(NHANES_sleep) + 
  geom_histogram(aes(x = SleepHrsNight), binwidth=1)
```

```{r, echo=FALSE}
ggplot(NHANES_sleep) + 
  geom_histogram(aes(x = SleepHrsNight), binwidth=1) + 
  scale_x_continuous(breaks=1:13)
```


Look how much wider the data distribution is than the sampling distribution.  Clearly it is not true that 95% of all Americans sleep between 6.82 and 6.94 hours per night.  In fact, such a statement is wildly false.  Just eyeballing the histogram, a more reasonable statement might be that about 95% of all Americans sleep between 4 and 9 hours per night.  There's a huge amount of variation around the population mean.  

What _is_ true, however, is that this sample has allowed us to _estimate_ this population mean quite precisely: it's about 6.88 $\pm$ 0.06 hours per night, with 95% confidence.    That's our statistical uncertainty, and _that's_ what the sampling distribution is telling us.   

The moral of the story is: don't fall for the gotcha!  

- The _data distribution_ of `SleepHrsNight` is real.  You can plot it.  You can see it.  Variation around the mean of that distribution represents real things that happened to real people.  
- The _sampling distribution of the mean_ of `SleepHrsNight` is something fundamentally different.  It is, in the most literal sense of the word, imaginary.  It's a distribution of a bunch of imaginary means of imaginary answers from imaginary surveys about people's sleep.  For all that, however, this distribution is still really useful; it represents the results of a thought experiment about what we happen if we could take many _different_ samples, whose variation can tell us about the precision of the mean from _our_ sample.  
- The whole point of this lesson is that we can't feasibly run that real thought experiment, so we run an approximate version of that thought experiment via the bootstrap.    


### Example 2: sample proportion {-}

Let's see a second example of the bootstrap in action.  The next question we'll address is: how frequent are [feelings of depression](https://www.samhsa.gov/find-help/national-helpline) among the American population?  We'll look at this using the variable `Depressed`, which is the self-reported frequency of days per month where a participant felt down, depressed or hopeless.  The options are "None," "Several", "Majority" (more than half the days), or "Almost All."

To keep the analysis simple, we'll define a new variable called `DepressedAny` that encodes whether a participant's response to this question was anything other than `None`:

```{r}
NHANES_sleep = NHANES_sleep %>%
  mutate(DepressedAny = ifelse(Depressed != "None", yes=TRUE, no=FALSE))
```

Inside the `ifelse` statement, the `!=` means "is not equal to."  So this statement encodes the `DepressedAny` variable as `TRUE` if that participant's answer was anything other than `None`, and as `FALSE` otherwise.  

Now let's use the "shortcut" function that we learned in the section on [Summary shortcuts], a few lessons back, to calculate the proportion of those in our survey who reported any depression:

```{r}
prop(~DepressedAny, data=NHANES_sleep)
```

So about 20% of the sample.  Mental health professionals are often telling us that depression is much more common than many people realize; here's the evidence for you right here.  

How precisely does this survey result characterize the frequency of depression among _all_ Americans?  Let's bootstrap the sample to get a sense of our statistical uncertainty.   This code might take ten seconds or so to run:    

```{r}
boot_depression = do(10000)*prop(~DepressedAny, data=resample(NHANES_sleep))
```

This statement says to: 1) create 10,000 bootstrap samples from the original `NHANES_sleep` data frame, and 2) for each bootstrap sample, recompute the proportion of those for which the `DepressedAny` variable is equal to `TRUE`.  We store the resulting set of 10,000 bootstrapped proportions in an object called `boot_depression`.  

The first six lines of this object look as follows:

```{r}
head(boot_depression)
```

There's a single column called `prop_TRUE`, with each entry representing the proportion calculated from a single bootstrap sample.  We can now examine this sampling distribution and compute a 95% confidence interval, like this:

```{r, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_depression) + 
  geom_histogram(aes(x=prop_TRUE))
```

```{r, eval=FALSE}
confint(boot_depression, level = 0.95)
```

```
##        name     lower     upper level     method  estimate
## 1 prop_TRUE 0.1854993 0.2214967  0.95 percentile 0.2034154
```

So our best guess is that 20.3% of Americans feel depressed at least some of the time.  Moreover, based on the sample, we're 95% confident that the true population proportion is somewhere between 18.5% and 22.1%.  That interval represents our statistical uncertainty.

#### Another gotcha {-}

Let's once again take care to distinguish this confidence interval from the data distribution, via a second "gotcha."  

__True or false:__ this histogram, and the associated confidence interval, tell us that about 95% of all Americans experience depression on somewhere between 18.5% and 22.1% of their days.  

[FALSE again. The variability in the data distribution is much, much wider than this.  Some Americans experience depression every day.  Some Americans experience depression never, or nearly never.  This interval simply isn't telling us about the variability in individual Americans' experiences of depression.  Rather, it is characterizing _how precisely_ our sample has allowed us to estimate the population average frequency of Americans that experience any depressive days at all (our estimand).]{.spoiler}


## Bootstrapping differences

What if we wanted to go one lever deeper with our data, by looking at specific subgroups?  For example, we might ask how sleep or depression varies by gender.  Or maybe how sleep varies among those with and without depression.  We've already learned how to calculate summary statistics like means and proportions by group; now, with the bootstrap, we can also measure our statistical uncertainty about group differences.  

### Example 1: sleep hours by gender {-}

Let's take the case of average sleeping hours for males and females.  Using one of our by-now-familiar "summary shortcuts," we can calculate this in a single line of code, using `mean`:

```{r}
mean(SleepHrsNight ~ Gender, data=NHANES_sleep)
```

It looks like females sleep a bit longer than males, on average.  If we just cared about that difference, we could use `diffmean`:

```{r}
diffmean(SleepHrsNight ~ Gender, data=NHANES_sleep)
```

So among survey respondents, women sleep around 0.23 hours (14 minutes) longer per night than males, on average.  

But what about statistical uncertainty?  That is, how precisely can this difference for our sample characterize the corresponding difference for the wider American population?  Let's bootstrap our sample to find out:

```{r}
boot_sleep_gender = do(10000)*diffmean(SleepHrsNight ~ Gender, data=resample(NHANES_sleep))
```

This statement says to: 1) create 10,000 bootstrap samples from the original `NHANES_sleep` data frame, and 2) for each bootstrap sample, recompute the difference in the means of the `SleepHrsNight` variable between males and females.  We store the resulting set of 10,000 bootstrapped differences in an object called `boot_sleep_gender`.  

The resulting object has a single column called `diffmean`, with each entry representing the difference of means for a single bootstrap sample:

```{r}
head(boot_sleep_gender)
```

Let's visualize this sampling distribution in a histogram, and then use it to create a confidence interval:  

```{r, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_sleep_gender) + 
  geom_histogram(aes(x=diffmean))
```

```{r, eval=FALSE}
confint(boot_sleep_gender, level = 0.95)
```

```
##       name      lower      upper level     method   estimate
## 1 diffmean -0.3486014 -0.1171517  0.95 percentile -0.2335348
```

Based on this survey, we can say with 95% confidence that females get a bit more sleep than males, on average, with a difference in means of somewhere between 0.12 and 0.35 hours (7-21 minutes).  

You'll notice that this confidence interval rules out a difference of zero.  We therefore say that the difference is __statistically significant__, i.e. nonzero up to a specified level of statistical uncertainty.  

```{definition}
An estimate is said to be __statistically significant__ at some specified level $\alpha$ if a confidence interval at level $1-\alpha$ for that estimate _does not contain zero_.  
```

<br>

Somewhat confusingly, the convention is to report statistical significance as the opposite of confidence.  So here, we'd say that the difference in average sleep time between males and females is statistically significant at the 5% (0.05) level, because a 95% (0.95) confidence interval for that difference fails to contain zero.  

#### One last gotcha {-}

Remember, this confidence interval of (-0.35, -0.12) is a claim about the difference between two population averages, one for males and one for females.  It's not a claim about _individual_ males and females.  So, for example, here's one last "gotcha":

__True or false:__ this histogram, and the associated confidence interval, tell us that if we randomly sampled one male and one female from the population, we're 95% confident that the female would sleep between 0.12 and 0.35 hours longer than the male.  

[FALSE. Again, the variability in the data distribution is much, much wider than this.  Some males sleep 9 or 10 hours per night; some females sleep 4 or 5 hours per night.  The confidence interval just isn't telling us about the individual-level differences.  Rather, it is characterizing _how precisely_ our sample has allowed us to know the true value of our estimand: the difference between the average sleep time of all males living in America and the average sleep time of all females living in America.]{.spoiler}

To help you visualize this, let's actually try to answer the other question: if we randomly sample one male and one female from the population, what's the difference in how long they sleep?  Setting up a simulation like this requires a bit more work, but it's not too bad.  To do so, we can first group the data set by Gender, like this:

```{r}
by_Gender = NHANES_sleep %>% group_by(Gender)
```

Then we can a function called `sample_n` function on this grouped data frame to randomly sample 1 individual per gender:

```{r}
sample_n(by_Gender, 1)
```

We can now repeat this process 10,000 times and combine it with `diffmean`, like so:  

```{r}
gender_sleep_differences = do(10000)*diffmean(SleepHrsNight ~ Gender, data=sample_n(by_Gender, 1))
```

Each "difference of means" is just the observed difference between two randomly sampled individuals, one male and one female.  So the resulting histogram is a bootstrap approximation to the answer we're seeking: if we randomly sampled one male and one female from the population, what's the difference in how long they sleep?  

```{r, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(gender_sleep_differences) + 
  geom_histogram(aes(x=diffmean), binwidth=1)
```

Just eyeballing the histogram, it looks like about 95% of the differences are between -4 hours (female sleeps longer) and +3 hours (male sleeps longer).  Compare this distribution with the sampling distribution for the difference of means, above. As is pretty much always the case, the data distribution is _much_ wider than the confidence interval for the mean of that data distribution.  



### Example 2: smoking and depression {-}

Let's look at a second example and ask the question: does the frequency of smoking vary according to whether someone reports any days where they feel depressed?  Let's calculate smoking rates stratified by the `DepressedAny` variable:

```{r}
prop(Smoke100 ~ DepressedAny, data=NHANES_sleep)
```

This takes a bit of effort to parse.  It's saying that:  

- among those with at least one depressed day per month (`DepressedAny = TRUE`), the proportion of nonsmokers (`Smoke100 = "No"`) is about 46%.  
- among those with at least no depressed days (`DepressedAny = FALSE`), the proportion of nonsmokers (`Smoke100 = "No"`) is about 58%.  

Altogether, that means that that those reporting at least some symptoms of depression are 12% more likely to have smoked at least 100 cigarettes in their lives:

```{r}
diffprop(Smoke100 ~ DepressedAny, data=NHANES_sleep)
```


Let's characterize the statistical uncertainty of this number.  That is, how precisely can this difference for our sample characterize the corresponding difference in smoking rates for the wider American population?  We can use the bootstrap to find out:

```{r}
boot_smoke_depression = do(10000)*diffprop(Smoke100 ~ DepressedAny, data=resample(NHANES_sleep))
```

This statement says to: 1) create 10,000 bootstrap samples from the original `NHANES_sleep` data frame, and 2) for each bootstrap sample, recompute the difference in the proportions of the `Smoke100` variable respondents with and without at least one depressed day per month.  We store the resulting set of 10,000 bootstrapped differences in an object called `boot_smoke_depression`. 

As always, let's visualize this sampling distribution in a histogram, and then use it to create a confidence interval:  

```{r, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_smoke_depression) + 
  geom_histogram(aes(x=diffprop))
```

```{r, eval=FALSE}
confint(boot_smoke_depression, level = 0.95)
```

```
##       name      lower     upper level     method  estimate
## 1 diffprop 0.06634904 0.1731429  0.95 percentile 0.1201859
```

Based on this survey, it's clear that those with at least one depressed day per month are more likely to smoke, on average.  Our best guess is that there's a 12% difference in smoking rates, with a 95% confidence interval from 6.6--17.3%.  Because this confidence interval doesn't contain zero, we can say that the difference is statistically significant at the 5% level.  

Notice that this result doesn't say anything about causality. We can't tell if people are more likely to smoke because they're depressed, or more likely to become depressed if they've smoked, or if both smoking and depression are symptoms of a common cause.  In fact probably all three happen in real life, but we can't even say which is more common _on average._  All we can say is that there's a difference in the frequency of depression by smoking status, and how big we think that difference might be for the whole population.  The data just give us a fact; it's up to us how we _explain_ that fact.


## Bootstrapping regression models

### Example 1: sleep versus age {-}

Have you ever heard a 30- or 40- or 50-something complain that, with the pressures of work and kids and having to pee in the middle of the night, they sleep less than they used to?  No doubt this is true for plenty of individual 30- or 40- or 50-somethings.  (As I write this, I am a 30-something with a toddler at home, and it is __definitely__ true for me.)    But is it true at the population level?  That is, do older people sleep fewer hours than younger people, on average?

Let's make a quick scatter plot of the data to find out:  

```{r, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(NHANES_sleep) + 
  geom_jitter(aes(x=Age, y=SleepHrsNight), alpha=0.1)
```

Two quick notes here.  First, `geom_jitter` is just a version of `geom_point` that injects a tiny bit of artificial jitter, solely for plotting purposes, so that the individual points can be distinguished from one another.  (If that doesn't make sense, try using `geom_point` instead and you'll see what I mean.) Second, `alpha = 0.1` makes the points only 10% opaque (and 90% translucent).  

OK, back to the plot itself.  I don't know about you, but I don't see any obvious correlation between sleep hours and age from this plot!  Already I'm beginning to get suspicious of the idea that age has much to do with sleep.  

However, this is where a regression model might help us.  Regression models are useful for quantifying relationships in data, even (or perhaps especially) when those relationships might be too subtle to stand out as "obvious" in a plot.  So let's fit a linear model for `SleepHrsNight` versus age:

```{r}
lm_sleep_age = lm(SleepHrsNight ~ Age, data=NHANES_sleep)
coef(lm_sleep_age)
```

The coefficient on `Age` is actually positive: 0.006 extra hours of nightly sleep with each additional year of age, or equivalently, about 3.6 minutes extra nightly sleep with each passing decade.  This is certainly not consistent with the idea that older people sleep fewer hours than younger people, on average.  (As an aside, it also provides another example of a non-interpretable intercept: construed literally, this intercept would imply that newborns sleep 6.6 hours per night, which I assure you is not true.  It's just an artifact of extrapolating our model's predictions way beyond where we have data.)

Moreover, now that we know how to bootstrap, we can also measure the statistical uncertainty of our slope estimate, like this:  

```{r}
boot_sleep_age = do(10000)*lm(SleepHrsNight ~ Age, data=resample(NHANES_sleep))
```

This statement says to: 1) create 10,000 bootstrap samples from the original `NHANES_sleep` data frame, and 2) for each bootstrap sample, refit a linear model (`lm`) for `SleepHrsNight` versus `Age`.  We store the resulting set of 10,000 fitted regression models in an object called `boot_sleep_age`.  

Now we can ask for a confidence interval based on this bootstrapped sampling distribution:  

```{r}
confint(boot_sleep_age, level = 0.95)
```

There's a lot of information in this table that we haven't yet learned to interpret.  But for now, focus only on the `Age` row.  This row is telling us the lower and upper bounds for our 95% confidence interval on the `Age` coefficient in our linear model.  To be specific, we're 95% confident that the true population-wide slope of sleeping hours versus age is somewhere between 0.003 and 0.010 extra hours per night, each passing year---or equivalently, between 1.8 and 6 extra minutes of sleep per night, each passing decade.

To provide some context for this confidence interval, it helps to see the fitted line superimposed on top of the data, like this:

```{r, message=FALSE}
ggplot(NHANES_sleep) + 
  geom_jitter(aes(x=Age, y=SleepHrsNight), alpha=0.1) + 
  geom_smooth(aes(x=Age, y=SleepHrsNight), method='lm')
```

The line is essentially, but not _exactly_, flat.  

### Statistical vs. practical significance {-}

This example serves to illustrate a really important distinction in data science: the distinction between __statistical significance__ and __practical significance.__

As we've already discussed, __statistical significance__ just means whether the confidence interval for some estimate contains zero---nothing more, nothing less.  By this narrow definition, the relationship between age and sleeping hours per night is statistically significant at the 5% level.  After all, the 95% confidence interval for the `Age` slope doesn't contain zero:

```{r}
confint(boot_sleep_age, level = 0.95) %>% filter(name == 'Age')
```

__Practical significance__, on the other hand, means whether the numerical magnitude of something is large enough to matter to actual human persons, as opposed to statisticians performing their day jobs.  This is necessarily a subjective judgment: "large enough to matter" is not a well-defined mathematical term.  But it's often pretty obvious in context.   After all, ask yourself: in subjective terms, does 3.6 minutes of extra nightly sleep with each passing decade sound like a lot to you?  It certainly doesn't to me.  And in more objective terms, 3.6 minutes also doesn't seem large in the context of the intrinsic variability in `SleepHrsNight` across the population, with a standard deviation of...

```{r}
sd(~SleepHrsNight, data=NHANES_sleep)
```

...1.3 hours, or 78 minutes per night.  

So yes, there is a "statistically significant" relationship between age and sleep hours.  This relationship is, contrary to the short-term experience of new parents, actually positive.  On average, people sleep a bit more as they age.^[This is a tad misleading.  A more detailed look at the NHANES data shows a small average dip in average sleep time from from ages 20-50, and then a small rise from ages 50-80.  But the rise at the back end is a bit steeper than the dip at the front end, for a small net-positive average effect from ages 20-80.]    

But only on average, and even then only a little bit!  Don't fall into the trap of assuming that, just because some effect is labeled "statistically significant," it must therefore matter in the real world.  It just isn't so.  The age--sleep relationship in the NHANES data is a perfect example of an effect that is statistically significant, but that we're _nearly sure_, based on the confidence interval, must be so small that it hardly matters at all in practical terms.  (Again, that's my opinion; if 3.6 minutes of extra sleep is make-or-break for you, you're certainly free to disagree, but sadly you'll need to wait a decade for those coming nights of somnolent bliss.)

Now, don't get me wrong.  I'm not saying that statistical significance is an irrelevant concept.  Being able to speak the language of statistical significance is really important for communicating about data.  Rightly or wrongly, there are times and places where people just expect you to use the term, especially in certain allegedly rarefied academic contexts.  More to the point, there are _also_ cases, like in clinical trials, where it actually is super important to know whether an effect is statistically significant. (Does this new drug actually work better than the old one?  Does this new vaccine work at all?)

But confidence intervals are _almost always_ more useful than claims about statistical significance.  My advice to you is to entertain the full range of values encompassed by the confidence interval, rather than jumping straight to the needlessly binary question of whether that confidence interval contains zero.  

In fact, I'll tell you a little secret.  Sometimes people use the term "statistical significance" to sound fancy and scientific and credible.  Sometimes it even works.  But to those who know better, focusing too much on statistical significance, at the expense of focusing on practical significance, instead comes across as a mark of insecurity or pseudo-seriousness about statistics.


```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/aha_effect_sizes.png")
```

That's because there are at least three more important questions than whether an effect is statistically significant.

(1) How big is the effect, in practical terms?  
(2) How much statistical uncertainty is there surrounding that effect?     
(3) Is _statistical_ uncertainty even the dominant contribution to _real-world_ uncertainty about the effect?  (See "[The truth about statistical uncertainty].")  Or might there be a large hidden bias in the measurement process, potentially turning any discussion of statistical uncertainty into a pointless academic exercise?  

Once you get in the habit of asking these questions, the significance question---"Could the effect be _exactly_ zero, up to statistical uncertainty?"---can start to feel a bit silly.  

Moreover, there's also an unfortunate glory-hound phenomenon related to this whole discussion about statistical significance.  Consider two newspaper headlines you could write based on our analysis of the NHANES data:

> Age a significant predictor of sleep patterns, new study finds

> Age nearly irrelevant in explaining sleep patterns, new study finds

Paradoxically, both headlines are true.  But the first one is true only in a narrow pedantic sense.  It pulls a fundamentally dishonest trick; it expects that you'll read "significant" as meaning "having a large effect in the real world," rather than as "statistically significant at the 5% level."  Note that this probably _isn't_ a trick played by the journalist on the reader.  It's more likely to be a trick played by the study authors on both themselves and the journalist, because none of them know any better.

I hope you agree that the first headline is basically fake news.  But while the second headline may be non-fake, it is also, sadly, non-news---both to sleep researchers and to the general public.  Very few headlines of the form "Study finds no big deal" have ever been written, either in newspapers or academic journals.  You can therefore imagine which headline a journalist would rather write, and which study an academic would rather try to publish---maybe out of ignorance, maybe out of "[publish or perish](https://en.wikipedia.org/wiki/Publish_or_perish)" career insecurity, but _definitely_ to the detriment of public knowledge and trust in science.  

This emphatically _doesn't_ mean that all academic studies are just dumb stats tricks published by glory hounds.  (See: [mRNA vaccines](https://www.theatlantic.com/ideas/archive/2021/03/how-mrna-technology-could-change-world/618431/).)  But it does mean that it's hard to tell from a newspaper headline whether any _specific_ academic study is just a dumb stats trick published by glory hounds.  There's only one antidote.  Read the study.  Look for the confidence interval.  Judge for yourself!


### Example 2: West Campus rents {-}

Let's see a second example of bootstrap confidence intervals for a regression model.  Here, we'll try to answer a question about the Austin real-estate market that's very much on the minds of many UT-Austin students: how do apartment rents in the West Campus neighborhood fall off as you get further away from campus itself? 

Please download and import the data in [apartments.csv](data/apartments.csv), which contains data on a sample of 67 apartments in West Campus.  For each apartment, we know the monthly rent and square footage of a two-bedroom apartment, as well as the distance from the UT Tower, which marks the notional (if not the actual) center of campus. (The data set has several other feature of the unit that might affect price, but we're not concerned about those here.)  This analysis focuses on two-bedroom apartments, since virtually all buildings have many units of this size, and since we want a common basis for comparing across buildings.  

```{r, echo=FALSE, message=FALSE}
apartments = read.csv('data/apartments.csv', header=TRUE)
```

Here are the first several lines of the file.  The relevant variables for our purposes are `two_bed_rent`, measured in dollars per month, and `distance_from_tower`, measured in miles:

```{r}
head(apartments)
```



Let's build a linear regression model for rent versus distance from the UT Tower:

```{r}
lm_distance = lm(twobed_rent ~ distance_from_tower, data=apartments)
coef(lm_distance)
```

This suggests a drop-off in rent of about \$631 per mile from the Tower, on average.  That's a pretty steep price gradient, but it reflects the limited supply of (and intense demand for) properties close to campus.  

What about a confidence interval for that slope?  Let's bootstrap:

```{r}
boot_distance = do(10000)*lm(twobed_rent ~ distance_from_tower, data=resample(apartments))
confint(boot_distance, level = 0.95)
```

So our 95% confidence interval ranges from about -\$840 per mile to about -\$420 per mile.  (The numbers are negative because increased distance implies lower price, on average.  Also, I feel OK rounding to the tens place because the confidence interval says the number isn't certain even to the hundreds place; further precision beyond the tens place is pointless).

This is an example of a relationship that is both _statistically_ significant (because the confidence interval doesn't contain zero) and _practically_ significant (because all the values in the confidence interval look large, in real-world terms).  To reason a bit more specifically about practical significance, lets assume 1 mile equals a 20-minute walk, and therefore 0.1 miles equals a 2-minute walk.  So to save 2 minutes each way in walking distance, you'd expect to pay an extra 0.1 $\times$ 630 = \$63 per month for a two-bedroom West Campus apartment.  Or _maybe_, if this is an especially unlucky sample, you'd expect to pay 0.1 $\times$ \$420 = \$42 on average, across all apartments in the general population---but that's only on the very low end of the confidence interval.  No matter what, it seems like a big effect.  

## Bootstrapping usually, but not always, works  

Let's turn now to a question that may have occurred to you already: how do I know this bootstrap thing actually works?  Take the specific example of this bootstrap confidence interval we calculated earlier, for the average number of hours per night that Americans sleep:

```{r, eval=FALSE}
boot_sleep = do(10000)*mean(~SleepHrsNight, data=resample(NHANES_sleep))
confint(boot_sleep, level = 0.95)
```

```
##   name    lower    upper level     method estimate
## 1 mean 6.820191 6.937217  0.95 percentile 6.878955
```

When I calculate this interval and state, "I'm 95% confident that the average sleep time in America is between 6.82 and 6.94 hours per night," what reason do you have to believe me?  I'm sure you are a streetwise, highly experienced individual.  Indeed, I've heard tell of your legend.  Your sniffer is sound.  Your compass is true.  On many occasions, you've heard someone make statements with a high degree of self-professed confidence, while you've known the whole time that they were totally full of shit.  What, if anything, makes _my_ statement, and _my_ self-professed confidence, any different?  

### What "confidence" means {-}  

I'd argue that one very good reason to have confidence in someone is because they have a track record of making truthful statements.  It's why people generally trust nurses and school teachers, but not politicians or men on dating apps.

This very same notion of confidence is fundamental to data science.  It helps to start with an analogy.  Imagine, for example, the following conversation between two quality-control engineers on an assembly line for smart phones.  

> __Alice:__ Hi Bob!
> 
> __Bob:__ Hi, Alice!  Listen, you know that last phone that just came off the assembly line?  Is it actually working correctly?  We should probably check.   
> 
> __Alice:__ I'm 99.9% confident that it's working correctly.  
> 
> __Bob:__ Wait, what?  99.9%?  Why don't you just turn it on and check?  Then you'll know for sure.
> 
> __Alice:__  Do you even work here?   This phone doesn't have a battery in it yet.   Who do I look like, the Evil Emperor from Star Wars?  I can't just shoot electricity out of my fingers and turn it on.  
> 
> __Bob:__ So how are you 99.9% confident that it's working?
>  
> __Alice:__ Well, we make 10,000 phones a day here, and on average, only about 10 of them fail our quality-control checks later in the process, after their batteries are installed.  That's a 99.9% track record.   I'm therefore 99.9% confident that this particular phone is working just fine.  
>   
> __Bob:__ OK, that makes sense!



When it comes to data science, we can formalize this idea of a track record in terms of something called the "coverage principle."

```{definition coverage-principle}
The __coverage principle__: if you were to analyze one data set after another for the rest of your life, and you were to construct X% confidence intervals for every estimate you made, those intervals should cover (i.e. contain) their corresponding true values at least X% of the time.  Here X can be any number between 0 and 100.
```
<br>

With that definition in place, let's now imagine a second conversation, this one between two sleep researchers.  

> __Bob:__ Hey, Alice, you know that sleep study you were working on yesterday?  What did you learn from the results?  
> 
> __Alice:__ I'm 95% confident that Americans sleep between 6.82 and 6.94 hours per night, on average.  
> 
> __Bob:__ Wait, what?  95%?  Why don't you just ask people and check?  Then you'll know for sure.
> 
> __Alice:__  Do you even work here?   That would entail surveying more than 300 million people, or maybe tricking them all into downloading spyware that listens for snoring.  Who do I look like, the Evil Emperor from Facebook?  I don't have that kind of time, budget, or filth in my soul.  
> 
> __Bob:__ So how are you 95% confident in your answer?
>  
> __Alice:__ Well, I've made 10,000 confidence intervals in my life using this exact same data-analysis procedure, and only about 500 of them ended up failing to contain the true value I was aiming at, after other researchers looked into it and the right answer became more or less known.  That's a 95% track record.  I'm therefore 95% confident that this particular interval contains the right answer.      
>   
> __Bob:__ OK, that makes sense!


There is an appealing simplicity here: if you're going to claim 95\% confidence, you should be right at least 95\% of the time over the long run.  You'll notice that, in both cases, Alice's stated level of confidence was ultimately a claim about the track record of some specific procedure---a procedure for building smart phones in one case, and a procedure for building confidence intervals in the other.   

Well, we've learned only one procedure for building confidence intervals, and that's the bootstrap.  The key question, then, is actually pretty simple.  __Does the bootstrap have a good track record?__  More specifically, does bootstrapping, as a procedure for building confidence intervals, actually satisfy the coverage principle, as laid out in Definition \@ref(def:coverage-principle)?  

The answer is: basically, yes!  A slightly more nuanced answer is: usually and approximately.  We'll first see an example where the bootstrap does indeed work _very_ well, which is actually pretty representative of most common data-science situations.  After that we'll see a trickier case where the bootstrap doesn't work well at all.  

### Example 1: sample mean {-}

Please once again import the [rapidcity.csv](data/rapidcity.csv) data set from a previous lesson, containing daily average temperatures (the `Temp` column) in Rapid City, SD over many years.

```{r, echo=FALSE}
rapidcity = read.csv('data/rapidcity.csv', header=TRUE)
options(`mosaic:parallelMessage` = FALSE)
set.rseed(12345678)
```

You might remember that the average temperature in Rapid City over this entire time period (1995-2011) was about 47.3 degrees F:  

```{r}
mean(~Temp, data=rapidcity)
```

There's no statistical uncertainty associated with this number.  We can think of it as a "population" mean, because our data set is effectively a census: it contains every day from the beginning of 1995 to the end of 2011.  (There _would_ be uncertainty if we wanted to use this number make a guess about the average temperature over other time periods, but that's not our goal here.)  

But instead of working from this census, now imagine that we took a sample of, say, 50 days, and tried to estimate the population average of 47.28 degrees.  Although this seems kind of artificial here, where we actually _do_ have the census, it's a good working model for how sampling works in the real world, and therefore a good way to assess whether the bootstrap performs well as a procedure for building confidence intervals based on samples.    

Here's our sample of size 50, along with the corresponding sample mean.

```{r, echo=FALSE}
set.rseed(12345678)
options(`mosaic:parallelMessage` = FALSE)
```

```{r, message=FALSE}
rapidcity_sample = sample_n(rapidcity, 50)
mean(~Temp, data=rapidcity_sample)
```

Of course we don't get the right answer, which is 47.28.  But what if we now bootstrap that sample to construct a confidence interval?  Notice that, crucially, our code below resamples from the "temperature sample" in the `rapidcity_sample` data set we just created above, but _not_ the full "temperature census" in `rapidcity`:  

```{r, echo=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r}
rapidcity_sample_boot = do(1000)*mean(~Temp, data=resample(rapidcity_sample))
confint(rapidcity_sample_boot, level = 0.95)
```

It looks like our bootstrap confidence interval contains the right answer of 47.28.  That's a good result.  

Did we just get lucky?  Let's try it again.  First we'll take an entirely new sample of size 50, and use that  new sample to once again estimate the population average:

```{r, echo=FALSE}
set.rseed(12345661)
options(`mosaic:parallelMessage` = FALSE)
```

```{r, message=FALSE}
rapidcity_sample2 = sample_n(rapidcity, 50)
mean(~Temp, data=rapidcity_sample2)
```

Once again, we don't get the right answer, which is 47.28.  But let's bootstrap this second sample to get a confidence interval:  

```{r, echo=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r}
rapidcity_sample_boot2 = do(1000)*mean(~Temp, data=resample(rapidcity_sample2))
confint(rapidcity_sample_boot2, level = 0.95)
```

And again, our bootstrap confidence interval contains the right answer of 47.28.  We're 2 for 2.  Do you trust the bootstrap yet?

If not, perhaps 100 confidence intervals from 100 different samples will persuade you?  Let's try it. My code block below uses some more advanced R tricks to run this simulation, so don't worry too much about the details.  I don't expect you to understand or replicate each line of this code; I include it merely so you can see how I get the result you'll see below.  (The code is made a bit harder to read because it takes advantage of the fact that my machine has 8 compute cores on it, which allows me to parallelize the computational heavy lifting here.)

```{r, echo=FALSE, message=FALSE}
set.rseed(20205681)
options(`mosaic:parallelMessage` = FALSE)
```

```{r, message=FALSE}
library(foreach)
library(doParallel)
registerDoParallel(8)

one_hundred_intervals = foreach(iter = 1:100, .combine='rbind') %dopar% {
  rapidcity_sample = sample_n(rapidcity, 50)
  rapidcity_sample_boot = do(1000)*mean(~Temp, data=resample(rapidcity_sample))
  confint(rapidcity_sample_boot, level = 0.95) %>%
    mutate(Sample = iter)
}

one_hundred_intervals = one_hundred_intervals %>%
  mutate(contains_truth = ifelse(lower <= 47.28 & upper >= 47.28,
                                 yes="yes", no = "no"))
```

This code draws 100 real samples of size 50 from the original data set.  Then for each of those real samples, we:  

- Draw 1000 bootstrap samples, and for each one compute the bootstrap sample mean.  
- From that bootstrap sampling distribution of 1000 means, compute a confidence interval.  
- Check whether the confidence interval contains the right answer of 47.28.  

The code takes 10 or 15 seconds to run even with 8 compute cores, but the result is 100 samples leading to 100 confidence intervals, about 95 of which should contain the truth.  To check, let's make a plot using `geom_errorbar`, which we haven't specifically learned about yet, but which is tailor-made for showing confidence intervals:  

```{r bootstrap-coverage1, out.width="100%", fig.asp=0.5, fig.cap="The best self-diagnostic I know for assessing your understanding of the bootstrap."}
ggplot(one_hundred_intervals) + 
  geom_errorbar(aes(x=Sample, ymin=lower, ymax=upper, color=contains_truth)) + 
  geom_hline(aes(yintercept = 47.28), color='black') + 
  theme(legend.position="bottom")
```

Bingo!  Each of the vertical bars represents a single confidence interval from a single sample.  The true value is shown as a horizontal line at 47.28 degrees; you'll recall this number was based on our "temperature census," i.e. the full 1995--2011 data set from which we drew the samples.  The blue intervals contain that true value.  The red intervals don't.  I count 6 red intervals out of 100, or equivalently, 94 blue intervals out of 100.  That's pretty close to 95%.  

Incidentally, I've found over the years that Figure \@ref(fig:bootstrap-coverage1), with its 100 different confidence intervals, is an _excellent_ self-diagnostic.  If you understand this figure---I mean _really_ understand it, like how it was constructed, why it was constructed that way, and what it actually implies---then congratulations!  You should be proud of yourself, because to understand this figure, you have to understand a whole heck of a lot: 

- what we're measuring when we measure "statistical uncertainty."    
- what a sampling distribution is and why it's important.  
- what bootstrapping is and how it works.   
- how to get a confidence interval from a bootstrap sampling distribution.  
- what basis we might have for trusting a data scientist when they say "I'm 95% confident."    
- why bootstrapping seems, at least on the initial evidence, worthy of that trust.  

In other words, if you understand this figure, you've basically aced the last two lessons.^[To be clear, it's not important that you understand the micro-level details of the R code block I provided.  Nor is it important that you  could actually code this kind of thing yourself.   What is important is that you understand the high-level tasks that my code block is accomplishing.]  On the other hand, if you don't understand it, you have a deficit somewhere in need of a remedy.  I suggest revisiting the last two lessons, proceeding only when you feel comfortable with each of the bullet points above.  



### Example 2: sample minimum {-}

Just so that you don't come away from this lesson thinking that the bootstrap _always_ works, let's see a case where it doesn't.  

What was the lowest daily average temperature in Rapid City between 1995 and 2011?  Answer:

```{r}
min(~Temp, data=rapidcity)
```

-19 degrees F, or -28 C.  Let's call this our "population minimum," at least for the period 1995--2011.  As with the "population mean" of 47.28 F, there's no statistical uncertainty associated with this number.  

But now let's take a sample of 50 days, just as we did before, and try to use the minimum of that sample to estimate this population minimum.   Here's one sample of size 50, along with the corresponding minimum temperature in that sample:  

```{r}
rapidcity_sample = sample_n(rapidcity, 50)
min(~Temp, data=rapidcity_sample)
```

Well, that's not very close.  Then again, _of course_ our sample answer is wrong, compared with the right answer of -19.  We should expect to see that by now for pretty much any statistical summary.  So let's bootstrap our sample and calculate a confidence interval:

```{r, message=FALSE}
rapidcity_sample_boot_min = do(1000)*min(~Temp, data=resample(rapidcity_sample))
confint(rapidcity_sample_boot_min, level = 0.95)
```

It looks like our bootstrap confidence interval ranges from 18.8 to 25.2.  Wow.  That's a horrible miss, considering the right answer for the "population" minimum is -19.    

Did we just get unlucky?  After all, a 95% confidence interval _should_ miss 5% of the time.  Maybe we just experienced that 5%.

So let's try it again.  First we'll take an entirely new sample of size 50, and use the minimum of that new sample to again estimate the population minimum:

```{r}
rapidcity_sample2 = sample_n(rapidcity, 50)
min(~Temp, data=rapidcity_sample2)
```

Once again, we don't get the right answer of -19---but that's expected.  So let's bootstrap this second sample to get a confidence interval:  

```{r, message=FALSE}
rapidcity_sample_boot_min2 = do(1000)*min(~Temp, data=resample(rapidcity_sample2))
confint(rapidcity_sample_boot_min2, level = 0.95)
```

Missed again!  We're 0 for 2.  What's going on?  

Let's more carefully verify that this isn't just bad luck, by repeating our exercise of simulating 100 separate samples and calculating a bootstrap confidence interval for each one.  Except instead of bootstrapping the _sample mean_, we'll bootstrap the _sample minimum._  

Here's my modified code block to do just that.  I basically just copied and pasted from above, changing `mean` to `min`:    

```{r, echo=FALSE, message=FALSE}
set.rseed(20205685)
options(`mosaic:parallelMessage` = FALSE)
```

```{r, message=FALSE}
registerDoParallel(8)
another_one_hundred_intervals = foreach(iter = 1:100, .combine='rbind') %dopar% {
  rapidcity_sample = sample_n(rapidcity, 50)
  rapidcity_sample_boot = do(1000)*min(~Temp, data=resample(rapidcity_sample))
  confint(rapidcity_sample_boot, level = 0.95) %>% 
    mutate(Sample = iter)
}

another_one_hundred_intervals = another_one_hundred_intervals %>%
  mutate(contains_truth = ifelse(lower <= -19 & upper >= -19,
                                 yes="yes", no = "no"))
```

Let's now plot these 100 confidence intervals and check how many of them contained the true population minimum.

```{r bootstrap-coverage2, out.width="100%", fig.asp=0.5, fig.cap="This is about as bad as bootstrapping ges."}
ggplot(another_one_hundred_intervals) + 
  geom_errorbar(aes(x=Sample, ymin=lower, ymax=upper, color=contains_truth)) + 
  geom_hline(aes(yintercept = -19), color='black') +
  theme(legend.position="bottom")
```

Holy cow.  Of our 100 different 95% bootstrap confidence intervals, _just one_ of them actually contained the true value of -19 F.  That's not just bad luck.  Something has gone horribly wrong.  But what?  

[What's wrong is that the lowest temperature in a sample of 50 days is a __really naive estimate__ for the lowest temperature we expect to see over a 16-year (or 5,844 day) period.  The only way your sample minimum can ever equal the population minimum is if, by sheer dumb luck, you _just happen to sample_ the very coldest day overall.  With a sample size of 50, and a population size of 5,844, the chances that this happens are about 1 in 117.  For the other 116 out of 117 samples, the sample minimum is _necessarily_ larger than the population minimum.  No matter how many times you bootstrap such a sample, you are _never_ going to hit -19 F, and your confidence interval will fail to contain the true population minimum.  So as an estimator of the population minimum, the sample minimum exhibits a massive, fundamental asymmetry: it's _never_ too low, it's _nearly always_ too high, and _very rarely_ it's exactly right.  This asymmetric behavior is quite different from that of the sample mean: half the time the sample mean is too low and half the time it's too high, so that on average it's about right.]{.spoiler}

The moral of the story is: the bootstrap doesn't always work.  In particular, it doesn't work for the min (or max) of a sample!  


### Closing advice {-}  

So we've seen one example where the bootstrap works great, and another example where it works horribly.  Where does that leave us?  

In turns out that our second example, where we tried to bootstrap the sample minimum to get a confidence interval for the population minimum, is a bit pathological.  That is to say: it's not representative of most common data-science situations.

In fact, you can successfully bootstrap most common summary statistics most of the time, assuming your observations can plausibly be interpreted as independent samples from some larger reference population.  By "safely," I mean that you can quote bootstrap confidence intervals in the knowledge that you're likely to cover the true value the stated percentage of the time.  These "safe" statistics include almost all the stuff we've been calculating in these lessons so far:  

- means and medians  
- standard deviations and inter-quartile ranges   
- correlations  
- quantiles of a distribution that aren't too extreme (e.g. the 25th percentile is OK, but the 99th percentile might not work well unless your sample size is huge)    
- regression coefficients from ordinary least squares (i.e. from `lm` in R)  
- group-wise differences of any of these statistics    

I recognize that the phrase "most common summary statistics most of the time" contains not just one but two weasel words in it.  That's because it's hard to give generic, non-weasely advice about the bootstrap, and about statistical inference more generally, without the discussion veering off into some very deep, very scratchy mathematical weeds.   All I'll say about the math is that our [best people](https://www.routledge.com/An-Introduction-to-the-Bootstrap/Efron-Tibshirani/p/book/9780412042317) are [working on it](https://www.cambridge.org/asia/catalogue/catalogue.asp?isbn=9780521574716).  

However, I can give you some other examples of where the bootstrap breaks, and where you therefore need more advanced inference techniques not covered in these lessons.  These include the following situations:  

- your sample is very small, say less than 20 or maybe 30.      
- you want to estimate the extreme quantiles of a distribution (of which the min and max are the most extreme possible, as you've seen).    
- your data points are highly correlated cross space or time.  
- you're fitting a very fancy machine-learning model, with a cool name like "lasso" or "deep neural network."  
- the underlying population distribution is very heavy-tailed.  

Honestly, this is the kind of stuff you'd learn about in a more advanced statistics course.  Your best best in these cases is to actually take such a course, or else to work with a professional statistician.  Of course, if you'd like to dig in more yourself, the following book has a chapter (Ch.9) devoted to "When Bootstrapping Fails Along with Remedies for Failures":

>M. R. Chernick, Bootstrap methods: A guide for practitioners and researchers, 2nd ed. Hoboken N.J.: Wiley-Interscience, 2008.


## Study questions: the bootstrap {-}

```{exercise}
Return to the `NHANES_sleep` data.  Use bootstrapping the answer the following questions.  

(A) How does average sleep time differ for those with and without at least one depresssed day per month?  Calculate a 95% confidence interval for the difference in the mean of `SleepHrsNight` between the two groups.  Is the difference statistically significant at the 5% level?  In your opinion, is the difference practically significant?  
(B) How does the frequency of depression differ by gender?  Calculate a 95% confidence interval for the difference in the proportion of `DepressedAny = TRUE` between the two groups.  Is the difference statistically significant at the 5% level?  In your opinion, is the difference practically significant?  
```

```{exercise}
Question here.
```

<!--chapter:end:09_bootstrap.Rmd-->

# p-values

```{r, echo=FALSE}
knitr::opts_chunk$set(cache = T, out.width="100%", fig.align='center')
```

Among professional football fans, the New England Patriots are a polarizing team. 

```{r, echo=FALSE, out.width="75%", fig.align='center', fig.cap = "The dark side of the Force is strong."}
knitr::include_graphics('images/EmperorBelichick.jpg')
```

The Patriots' fan base is hugely devoted, probably due to their long run of success over nearly two decades. Many others, however, dislike the Patriots for their highly publicized cheating episodes, whether for deflating footballs or clandestinely filming the practice sessions of their opponents. This feeling is so common among football fans that sports websites often run articles with titles like "[11 reasons why people hate the Patriots.](http://www.foxsports.com/nfl/gallery/main-reasons-people-hate-new-england-patriots-092116)"  Despite---or perhaps because of---their success, the Patriots always seems to be dogged by scandal and ill will.

But could even the _Patriots_ cheat at the pre-game coin toss, which decides who starts the game with the ball?  

Believe it or not, many people think so.  That's because, for a stretch of 25 games spanning the 2014-15 NFL seasons, the Patriots won 19 out of 25 coin tosses, for a suspiciously high 76\% winning percentage.  Sports writers had a field day with this weird fact, [publishing headlines like this one:](https://www.cbssports.com/nfl/news/patriots-have-no-need-for-probability-win-coin-flip-at-impossible-clip/) 

```{r, echo=FALSE, out.width="100%", fig.align='center'}
knitr::include_graphics('images/patriots_probability.png')
```

But to the Patriots' detractors, this fact was more than weird.  It was infuriating, and clear evidence that something fishy was going on.  As one TV commentator remarked when this unusual fact was brought to his attention: "This just proves that either God or the devil is a Patriots fan, and it sure can't be God."

But before turning to fraud or the Force as an explanation, let's take a closer look at the evidence.  Just how likely is it that one team could win the pre-game coin toss at least 19 out of 25 times, assuming that there's no cheating going on?

This question calls for something called a __hypothesis test,__ which is another core tool of statistical inference.  The innocent explanation here is that the Patriots just got lucky; in the parlance of data science, that's called a _hypothesis._  The goal of our test is to check whether this hypothesis seems capable of explaining the data, or whether instead we need a new hypothesis (i.e. a different explanation for the data).  To do that, we'll calculate a number called a __p-value.__  

In this lesson, you'll learn:  

- the four steps common to all hypothesis testing problems.    
- how to conduct a hypothesis test via simulation.  
- what a p-value is, and why it confuses people so much.  


## Example 1: did the Patriots cheat?  
  
So let's return to our burning question: just how likely is it that one team could win the pre-game coin toss at least 19 out of 25 times, assuming that there's no cheating going on?  Of course we'd "expect" any given team to win only about half of its pre-game coin tosses, or roughly 12 or 13 out of 25.  But that's hardly enough evidence to open a cheating investigation into the Patriots.  We have to account for the possibility that they just got lucky.

One way to get a feel for the inherent randomness of coin-flipping is to visit [random.org](http://www.random.org/coins/), which has a cute feature where you can "flip" a state quarter of your choice, as many times as you like.  Since there was no quarter available for the Patriots' actual home state of Massachusetts, I chose the next-best "New England" option and flipped a Connecticut state quarter a bunch of times.  It didn't take me too long to notice a few lucky streaks.  For example, here's a stretch with 12 out of 15 heads:

```{r, echo=FALSE, fig.cap="A lucky streak with 12 heads out of 15 flips."}
knitr::include_graphics('images/connecticutt_quarters.png')
```

So these lucky streaks clearly do happen.  How likely, then, is a lucky streak with at least 19 wins out of 25 coin tosses?  

Rather than wasting the day away on random.org, we'll answer this question more systematically, using the Monte Carlo method---which, you'll recall, entails writing a computer program that simulates a random process (in this case a stretch of 25 coin flips).

Please load the `tidyverse` and `mosaic` libraries, and start by running the command `rflip(25)`, like in the code block below:  

```{r, echo=FALSE, message=FALSE}
set.rseed(438752872)
options(`mosaic:parallelMessage` = FALSE)
```

```{r, message=FALSE}
library(tidyverse)
library(mosaic)

rflip(25)
```

This simulates 25 coin flips.  Here we'll identify the "H" outcome with the event "Patriots win the pre-game coin toss," and the "T" outcome with the event "Patriots lose the pre-game coin toss."  This isn't just a metaphor; it turns out that the guy who called the pre-game coin toss for the Patriots [actually did call heads every single game for six years in a row,](https://www.patspulpit.com/2017/2/7/14522972/patriots-captain-matthew-slater-has-called-the-same-side-for-every-coin-toss-for-6-years) which only intensified speculation that the Patriots knew something about the coin that the rest of the NFL did not.

So in this simulation, it looks like the Patriots won 12 out of 25 coin tosses---about what we'd expect, considering that half of 25 is 12.5.  That's far shy of the 19 coin tosses the Patriots won in real life.  Then again, one simulation doesn't establish a very good baseline for understanding how "lucky" a 19-out-of-25 streak might be.

So let's simulate that same stretch of 25 coin flips, but 10,000 different times.   To do this we'll use `do` and `nflip`, which just counts "H" outcomes (as opposed to `rflip`, which prints out the individual H's and T's to the screen):  

```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r}
patriots_sim = do(10000)*nflip(25)
```

The object we created, `patriots_sim`, is a data frame with a single column, called nflip.  Each entry represents the number of "H" outcomes (i.e. notional Patriots wins) in a single simulation of 25 coin flips:  

```{r}
head(patriots_sim)
```

Let's make a histogram of all 10,000 of these outcomes:

```{r}
ggplot(patriots_sim) + 
  geom_histogram(aes(x=nflip), binwidth=1)
```

Just eyeballing the histogram, 19 wins or more seems pretty unlikely.  How unlikely?  According to our simulation, it happened only...

```{r}
sum(patriots_sim >= 19)
```

...58 times out of 10,000.  Clearly 19 wins is an unusual, although not impossible, result: based on the Monte Carlo simulation, we estimate its probability to be about 58/10000 $\approx$ 0.006.      


So did the Patriots win 19 out of 25 coin tosses by chance?  Well, nobody knows for sure---you can examine the evidence and decide for yourself.  

But despite the small probability of such an extreme result, it's hard to believe that the Patriots cheated on the coin toss, for a few reasons.  First, how could they?  The coin toss would be extremely hard to manipulate, even if you were inclined to do so.  Moreover, the Patriots are just _one_ team, and this is just _one_ 25-game stretch.  But the NFL has 32 teams, so the probability that _at least one_ of them would go on an unusual coin-toss winning streak over _at least one_ 25-game stretch over a long time period is a lot larger than the number we've calculated.  Finally, after this 25-game stretch, the Patriots reverted back to a more typical coin-toss winning percentage, closer to 50\%.  I conclude that the 25-game stretch was probably just luck.^[Out of curiosity, I actually dug deeper and ran a more involved simulation consisting of all NFL games over a ten-year period.  In about 23% of those simulations, at least one team went on a lucky streak where they won at least 19 out of 25 coin flips.  So overall, there's not much evidence to suggest that the Patriots were cheating at the coin toss.]


## The four steps of hypothesis testing  

But unless you're a hard-core NFL conspiracy theorist, let me encourage you to forget the Patriots for a moment and focus instead on the _process_ we've used to reason through this question.  This simple example has all the major elements of hypothesis testing, which is the subject of this lesson.  

(1) We have a _null hypothesis_, or a "hypothesis of no effect."  Here our null hypothesis is that the pre-game coin toss in the Patriots' games was truly random.  
(2) We use a _test statistic_, or a numerical summary used to measure the strength of evidence against the null hypothesis.  Here our test statistic is the number of Patriots' coin-toss wins out of 25: higher numbers entail stronger evidence against the null hypothesis.  
(3) We have a way of calculating the _probability distribution_ of the test statistic, assuming that the null hypothesis is true.  Here, we ran 10,000 Monte Carlo simulations (with each simulation having 25 coin flips), assuming an unbiased coin.  
(4) We have an _assessment_: we used the distribution in step (3) to assess whether the null hypothesis seemed capable of explaining the observed test statistic.  

To perform our assessment in step (4), we calculated a number (0.006) that represented the probability that the Patriots would go on a lucky streak with at least 19 wins out of 25 coin tosses.  This number of 0.006 is referred to as a __p-value__.  Here's the formal definition of this term.  


```{definition p-value}
A __p-value__ is the probability of observing a test statistic as extreme as, or more extreme than, the test statistic actually observed, given that the null hypothesis is true.
```

<br>

This sounds a bit abstract, but it's easier to understand from a picture:  

```{r patriots-pvalue, echo=FALSE, warning=FALSE, fig.cap="The p-value represents a tail area of the probability distribution for our test statistic, assuming the null hypothesis is true."}
ggplot(patriots_sim) + 
  geom_histogram(aes(x=nflip, fill={nflip < 19}), binwidth=1) + 
  theme(legend.position = "none") + 
  geom_vline(aes(xintercept = 18.5)) + 
  annotate(geo='text', x=20.5, y=90, label=expression("P(>= 19 wins) = 0.006"), size=3)
```

Remember, this histogram shows the results of our simulation, where we assumed that the null hypothesis was true (i.e. 50% chance of winning the coin flip).  The vertical bar shows our observed test statistic (19 wins).  The p-value represents the _tail area_ of this distribution: specifically, the area at or beyond the observed value of the test statistic of 19.  This area of $p = 0.006$ is shaded in red.  The smaller this number, the harder it is for our null hypothesis to explain the data.  

## Example 2: a disease cluster?

Let's see a second example to help us understand p-values better.

Between 1990 and 2011, there were 2,876 "cancer cluster" investigations conducted in the U.S.  These investigations are triggered when citizens believe they have anomalously high rates of cancer in their community, and they ask the local health department to conduct a formal investigation.  Many times there is no initially obvious hypothesis for why there migth be a cluster.  Other times, citizens may already suspect a possible cause.

Here we'll look at a famous such investigation surrounding nuclear power plants in Illinois during the period 1996-2005.  The basic facts of the investigation are straightforward.   Among households __less than 10 miles__ from a nuclear plant:  

- there were 80,515 children ages 0-14.
- there were 47 cases of childhood leukemia.
- Thus the rate of leukemia was 0.00058, or 5.8 cases per 10,000 children.  

In comparison, among households __more than 30 miles__ from a nuclear plant:

- there were 1,819,636 children ages 0-14.
- there were 851 cases of childhood leukemia.
- Thus the rate of leukemia was 0.00046 (4.7 cases per 10,000 children).

In epidemiology, the ratio of these two rates is referred to as the _incidence ratio_.  Here the incidence ratio was 5.8/4.7 = 1.23, implying that the rate of leukemia was 23% higher among childen living within 10 miles of a nuclear power plant, compared with the baseline risk among children living more than 30 miles away.  Based on this fact, citizens called for a formal cancer-cluster investigation.  

But let's remember what we've learned so far.  Whenever randomness is involved, you have to rule out blind luck before you can say you've found an anomaly.  Cancer is linked to a mutation of your genes.  This is clear example of a chance event.  Is it possible that Illinois children living near power plants experienced this chance event at an unluckily high rate, without there being a systematic reason for it?  In other words, is it plausible that, over the long run, children near nuclear plants actually _do_ experience leukemia at the same background rate as other children, and that the apparent anomaly in the 1996-2005 data can be explained purely by chance?

Let's run a hypothesis test to find out.  We'll follow the same four steps we followed in the Patriots example.

__Step 1:__ Our null hypothesis is children near nuclear power plants in Illinois experience leukemia at the background rate of 4.7 per 10,000, on average over the long run.

__Step 2:__ Our test statistic is the number of leukemia cases.  Higher numbers of cases imply stronger evidence against the null hypothesis.  In our data, 47 of 80,515 children living near nuclear plants had leukemia between 1996 and 2005.

__Step 3:__ we must calculate the probability distribution of the test statistic, assuming that the null hypothesis is true. This distribution provides context for the observed data. It allows us to check if the observed data looks plausible under this distribution, or instead whether the null hypothesis looks too implausible to be believed.

To do this, we'll use the same strategy of Monte Carlo simulation.  Specifically, we'll simulate 80,515 flips of a metaphorical biased "coin" that comes up heads 4.7 times per 10,000 flips (prob = 0.00047), on average:  

- 80,515 is the number of children living within 10 miles of a nuclear plant.  
- 0.00047 is the base rate of leukemia among Illinois children living __more than 30 miles__ from a nuclear plant.  Remember, our null hypothesis assumes that all children experience the same average rate of leukemia, regardless of their proximity to a nuclear power plant.  

In our metaphor, each occurrence of "heads" corresponds to a case of leukemia.  

I suggest that you run this following statement 4 or 5 times to see the variation you get:

```{r}
nflip(n=80515, prob=0.00047)
```

Each time you run this statement, you get a single number.  This number represents the simulated number of cancer cases among children within 10 miles of a nuclear plant, assuming that the null hypothesis is true.   

Let's now repeat this simulation 10,000 times and store the result.

```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r}
sim_cancer = do(10000)*nflip(n=80515, prob=0.00047)
```

As before, the result is a data frame with one column called "nflip":

```{r}
head(sim_cancer)
```

Let's now visualize the distribution.  Remember that our observed test statistic was 47 cancer cases. Can we eyeball how much of the distribution is 47 or larger?

```{r, sim-cancer-pvalue}
ggplot(sim_cancer) + 
  geom_histogram(aes(x=nflip), binwidth=1)
```

Maybe 10%  Of course, if we don't trust our eyeballs, we can ask R!  And that brings us to __step 4__: calculate a p-value.  How many simulations yielded 47 cases of cancer or more, just by chance?

```{r}
sum(sim_cancer >= 47)
```

That's 850 out of 10,000, or about 8.5%.  So our p-value is $p = 0.085$.  In other words, such a "cancer cluster" could easily have happened by chance.  This is a bit smaller than the probability that you'll get heads three times in three coin flips (which is 0.125).    

Here is the p-value shown visually, as a tail area of the simulated distribution of test statistics:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
ggplot(sim_cancer) + 
  geom_histogram(aes(x=nflip, fill={nflip < 47}), binwidth=1) + 
  theme(legend.position = "none") + 
  geom_vline(aes(xintercept = 46.5)) + 
  annotate(geo='text', x=58.5, y=70, label=expression("P(>= 47 cases) = 0.085"), size=3)
```

Based on this p-value, the Illinois Department of Public Health reached a similar conclusion.  They published a study on their investigation, writing:

> The objective of this study was to examine childhood cancer incidence in proximity to nuclear power plants in Illinois. Cancer cases diagnosed among Illinois children 0 to 14 years old from 1986 through 2005 were included in the study... The results show that children living within 10 miles of any nuclear power plant did not have significant increase in incidence for leukemia, lymphomas, or other cancer sites. Neither did the children living 10 to 20 miles or 20 to 30 miles from any nuclear power plants. This study did not find any significant childhood cancer excess among children living near nuclear plants and did not observe any dose-response patterns.^["Childhood Cancer Incidence in Proximity to Nuclear Power Plants In Illinois." Illinois Department of Public Health Division of Epidemiologic Studies, November 2012.]  

In summary, I hope this second example brings home some of the key terms and ideas from this lesson.  

- Null hypothesis: the hypothesis of no effect, i.e. that blind luck can account for an apparent "anomaly" or "effect" that we observe in the data.  
- Test statistic: a numerical summary of the data that measures the evidence against the null hypothesis.  
- p-value: the probability of observing a test statistic at least as extreme as what we actually observed, just by chance, if the null hypothesis were true.   


In hypothesis testing, the hard part is usually calculating the probability distribution of the test statistic, assuming that the null hypothesis is true.  In both examples here, we did this by simulating coin flips.  On future data sets, we'll have to work a bit harder, but we'll cover those details if and when we need them.  


## Interpreting p-values

Let's come back to the final number that we calculate in a hypothesis test: a p-value.  

Using a p-value as a measure of statistical significance has both advantages and disadvantages.  The main advantage is that the p-value gives us a continuous measure of evidence against the null hypothesis.  The smaller the p-value, the more unlikely it is that we would have seen our data under the null hypothesis, and therefore the stronger ("more significant") the evidence that the null hypothesis is false.

The main disadvantage is that the p-value is really, really hard to interpret correctly.  I mean, just look at the definition:

>A __p-value__ is the probability of observing a test statistic as extreme as, or more extreme than, the test statistic actually observed, given that the null hypothesis is true.

That's pretty convoluted!  For example, a p-value is used to _decide_ whether the null hypothesis is true, and yet the definition _assumes_ that the null hypothesis is true, which seems kinda weird.  Moreover, in real life, we've always observed some particular value of a test statistic.  But the p-value asks us to compute the probability of our test statistic, along with _all possible test statistics that are more extreme than ours_.  This is also kinda weird, since we never actually observed those "more extreme" test statistics---nor, presumably, did the null hypothesis predict that we _would_ observe them.  Of course, there _is_ a logic behind these definitional choices, and this logic explains why p-values have been important historically.  But it's a very subtle kind of logic, one based on a combination of thought experiment ("What if the null hypothesis were true?") and mathematical argument not worth going into here ("What, mathematically speaking, can we say about this p-value thing we've just defined?").  

The point is this: there are three levels at which you can understand a p-value.

(1) Knowing, at a qualitative level, that a p-value measures the strength of evidence against some null hypothesis, with smaller values implying stronger evidence.  This "level 1" understanding is attainable for anyone.  
(2) Knowing what, specifically, the formal definition of a p-value actually _says_ (and does not say).   If you work with data, you should simply memorize the formal definition to the point of being able to state it on demand.  You should also make sure you understand how that definition corresponds to a picture like Figure \@ref(fig:patriots-pvalue).   This "level 2" understanding is also attainable for anyone, with a bit of work and attention to detail.    
(3) Feeling deep in your soul what a p-value actually _means_, substantively speaking.  It's very natural for you to want to attain this.  But I'm convinced this "level 3"  understanding is basically impossible, or at best possible only for Jedis, and my advice to you is to not even try unless you pursue a Ph.D in statistics or the philosophy of science.  (And you certainly shouldn't feel bad about it if you never get there.  P-values really are _very_ hard understand.)  

Speaking personally: I wrote my [Ph.D thesis](https://dukespace.lib.duke.edu/dspace/handle/10161/1249) on statistical hypothesis tests, and while I have a firm level-2 understanding of p-values, I'm pretty sure I fall short of a level-3 understanding.  So, apparently, do most scientists; here's a link to [a 538 story](https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/) and funny-if-you're-a-nerd video of a bunch of scientists trying to explain what a p-value means.   Of those scientists featured in the video, maybe one person has a level-2 understanding of p-values.  Most seem stuck at either level 0 ("Ummmm....") or level 1.  Nobody comes anywhere close to level 3.  

#### OK, so now what? {-}

To avoid injuring themselves by thinking too hard about what a p-value actually means, many people just shrug their shoulders and ask a simple question about any particular p-value they happen to encounter: is it less than some magic threshold?  Of course, there is no such thing as a genuinely magic threshold where results become important, any more than there is such a thing as a magic height at which you can suddenly play professional basketball.  But it many fields, the threshold of $p \leq 0.05$ is often taken as an symbolically important boundary that demarcates "statistically significant" ($p \leq 0.05$) from "statistically insignificant" ($p > 0.05$) results.  You pretty much cannot publish a result in a medical journal unless your p-value is 0.05 or smaller.  You've read my thoughts on [Statistical vs. practical significance] before, so you can probably imagine how silly I think this is.  Moreover, while there are some legitimate reasons^[If you are interested in these reasons, you should read up on the Neyman--Pearson school of hypothesis testing.] for thinking in these terms, in practice, the $p \leq 0.05$ criterion can feel pretty arbitrary.  After all, $p=0.049$ and $p=0.051$ are nearly identical in terms of the amount of evidence they provide against a null hypothesis.

Because of how counter-intuitive p-values are, people make mistakes with them all the time, even (perhaps especially) people with Ph.D's quoting p-values in original research papers.   Here is some advice about a few common misinterpretations:

- The p-value is _not_ the probability that the null hypothesis is true, given that we have observed our statistic.
- The p-value is _not_ the probability of having observed our statistic, given that the null hypothesis is true.  Rather, it is the probability of having observed our statistic, _or any more extreme statistic_, given that the null hypothesis is true.
- The p-value is _not_ the probability that your procedure will falsely reject the null hypothesis, given that the null hypothesis is true.  To get a guarantee of this sort, you have to set up a pre-specified "rejection region" for your p-value (like $p \leq 0.05$), in which case the size of that rejection region---and not the observed p-value itself---can be interpreted as the probability that your procedure will reject the null hypothesis, given that the null hypothesis is true.^[As above: if you're interested, read about the Neyman--Pearson approach to testing.]  

So here's my summary advice for this lesson.

First, be broadly conversant with p-values.  People will throw them at you and expect you to know what they mean (even if, as seems quite likely, the people doing the throwing don't even understand what they mean).  Moreover, many common "off the shelf" statistical analyses produce p-values as part of their standard summary output.  Therefore, you should reach a level-2 understanding of what p-values are.   Know the common misinterpretations and why they're wrong.  

Second, know that most data scientists carry around in their head a subjective, not-very-scientific scale for interpreting p-values.  I won't claim to know anyone else's subjective scale, but here's mine, based purely on my own experience:   

- p $\approx$ 0.05: Maybe you've found something, but it could easily be nothing.  I'd really like to see more data.  
- p $\approx$ 0.01: You've piqued my interest.  It could still be nothing, but it's looking less likely.
- p $\approx$ 0.001, or $10^{-3}$: That's pretty small.  I _suppose_ it could still be nothing, but there's probably something here in need of an explanation.  
- $10^{-6} < p < 10^{-3}$: This region between "one in a thousand" and "one in a million" is clearly very small, but I honestly struggle to find the words to say how small in subjective terms.  
- p < $10^{-6}$: Wow, that's so small that even [particle physicists](https://blogs.scientificamerican.com/observations/five-sigmawhats-that/) would take an interest in it, and they're harder to impress than anyone.  

Third, don't split hairs with p-values.  You should pay basically just as much attention to an effect with a p-value of 0.051 as you do to an effect with a p-value of 0.049.  

Fourth, pay attention to the context in which a p-value was calculated.  It matters whether we're talking about the p-value for a single test, or the p-value that happened to be the smallest out of a bunch of tests.  Think of it this way: it would be remarkable if _you_ won the lottery tomorrow, but it would not be remarkable for _someone_ to win the lottery tomorrow, because there are an awful lot of lottery contestants.  In hypothesis-testing terms, the "number of contestants" should affect how we interpret a p-value.  The online cartoon xkcd [gets this exactly right](https://xkcd.com/882/):

```{r, echo=FALSE, out.width = "60%", fig.align="center"}
knitr::include_graphics("images/xkcd_significant.png")
```

Finally, you should appreciate that, while p-values might be interesting and useful for some purposes, they are still fundamentally impoverished, because they measure _only_ statistical significance, entirely ignoring any questions of practical significance.  In many circumstances, a much better question to ask than "what's the p-value?" is "what is a plausible range for the size of the effect I'm trying to measure?"  This question can be answered only with a confidence interval, not a p-value.   



<!--chapter:end:10_p_values.Rmd-->

# Large-sample inference

```{r, echo=FALSE}
knitr::opts_chunk$set(cache = T, fig.align='center')
```

Up to now, we've answered every question about statistical uncertainty by running a Monte Carlo simulation, of which the bootstrap is a special case.  This depends, obviously, on having a pretty fast computer, so that we can `do()` things repeatedly and not wait too long.  So you might ask: what on earth did people do before there were computers fast enough to make this possible?  How did they ever measure their statistical uncertainty?  

The answer is: people did __math.__  Lots and lots of math, spanning at least three centuries (18th to 20th).  And in this lesson, we're going to summarize what people learned from all that math.  


#### Did someone say "math"? {-}

If you enjoy math, feel free to skip this section.  But if you don't enjoy math, you might have some concerns.  Let me try to address those concerns head-on, with the following FAQ.

(1) __"Is this math hard?"__  Some of it is accessible to anyone with a high-school math education, and that's the part that gets taught in a high-school AP Statistics course.  But yes, [certain _other_ parts](https://www.cs.toronto.edu/~yuvalf/CLT.pdf) of this [math](http://cameron.econ.ucdavis.edu/e240a/asymptotic.pdf) actually are pretty hard.  And if you don't dig into this math, there will _always_ be parts of the high-school version that you just have to accept [because someone tells you to do it that way.](https://shoichimidorikawa.github.io/Lec/ProbDistr/t-e.pdf)  I'd say that overall, this math is harder than a college course on differential equations, but not as hard as, for example, a PhD course on quantum mechanics.^[For those that know this math, I'm thinking of, for example, the proof of the Central Limit Theorem involving characteristic functions, or the proof of the asymptotic normality of the OLS estimator.]


(2) __"Holy smokes.  That sounds terrifying.  Am I going to have to learn do this math myself?"__ Emphatically not, unless you decide to take your studies a lot further than these lessons can carry you.  

(3) __"Whew.  OK, so am I going to have to _understand_ this math?"__  It depends what you mean by "understand."   I _will not_ ask you to actually stare at mathematical derivations and understand what's going on.  I _will_ summarize the practical consequences of this math and ask that you understand them.  

(4) __"Do I even need this math to do statistical inference?"__  Usually not, at least for the problems we consider in this book.  Remember, not every data-science situation you'll encounter even requires statistical inference.  (See "[The truth about statistical uncertainty].") And when statistical inference _is_ required, you typically don't need any math at all---just what we've learned so far (mainly the bootstrap).    

(5) __"Seriously?  So you're telling me we're going to talk about some hard math from a long time ago, rendered largely unnecessary by computers.  Why am I still here?"__  Let me congratulate you on your keen sense of the opportunity cost of your time.  (I wish you were in charge of most meetings I attend.)  Yours is such an excellent question that it deserves a much longer answer.  


#### Why bother? {-}

There's one overwhelmingly important reason why it's important, even for beginners, to understand this historically older, more mathematical approach to statistical inference: _because a lot of people out there still use it._  

Why, you ask, do they still use this older approach?  (I mean, haven't they read my FAQ?!)  That's an interesting question, but it's a question for another day.  The fact is, folks _do_ use this older approach, many with good reasons.  And when they communicate their results to you, using terms like "z test" or "t interval," they'll expect you to understand them.  As a basic matter of statistical literacy, it is important that you do.

And you will!  If you understood the lesson on "[The bootstrap]", these folks describing their "z test" or "t interval" won't be talking about anything fundamentally unfamiliar to you, despite their exotic vocabulary.  In fact, you stand a decent chance of understanding what they're saying better than they understand it themselves.  It does take a bit of time and effort to learn a few new terms for a few old ideas (hence this lesson).  But at the end of the day, everyone's just talking about [Sampling distributions].  

To convince you of this, let me show you the following table.  In the left column are a bunch of phrases associated with the older, more mathematical way of approaching statistical inference.  In the right column are some corresponding translations in terms of the bootstrap.  If you understand what the right column is referring to, you're going to find this lesson a breeze.  It'll be just like memorizing how to say "please," "thank you," and "where's the bathroom?" in the local language of a foreign country you're about to visit.  


<br>

Table: (\#tab:translation-table) How to translate between two different approaches to statistical inference.

| If someone uses a...  | That's basically the same thing as...|
|:-------------------|:--------------------------------------|
| one-sample t-test (or "t inverval")  | Bootstrapping a sample mean and making a confidence interval (and possibly checking whether that confidence interval includes some specific value, like 0).        |
| one-sample test of proportion (or "z interval")  | Bootstrapping a proportion and making a confidence interval (and possibly checking whether that confidence interval includes some specific value, like 0.5).                                     |
| Two-sample t-test (or z-test)       | Bootstrapping the difference in means between two groups (`diffmean`) and making a confidence interval for the difference (and possibly checking whether that confidence interval contains zero).    |
| Two-sample test of proportions | Bootstrapping the difference in proportions between two groups (`diffprop`) and making a confidence interval for the difference, (and possibly checking whether that confidence interval contains 0). |
| Regression summary table       | Bootstrapping a regression model (`lm`) and making confidence intervals for the regression coefficients (and possibly checking whether those confidence intervals contain zero).    |

<br>

The rest of this lesson is about understanding Table \@ref(tab:translation-table).  

Let me also add four other reasons why you'd actually _want_ to understand this table, quite apart from the practical need to understand what other people are talking about when they use the terms in the left column.  

(1) __It's a time-saver.__  Hey, life is short.  I don't know about you, but sometimes I get bored waiting for my computer to `do()` something 10,000 times in order to produce a confidence interval.  It turns out that, if you understand the correspondences in the table above, you can get a perfectly fine confidence interval in no time at all, without actually bootstrapping.  This is pretty useful, especially if you're just exploring a data set and want a quick answer without having to twiddle your thumbs while R is thinking.  

(2) __It works on bigger data sets.__  A related problem is that for some very large data sets, bootstrapping might take _so_ much time that it's no longer just a question of boredom, but of whether you'll get an answer _at all_ within some reasonable time frame.  

(3) __It offers insight.__  To give one example: the math behind Table \@ref(tab:translation-table) helps you understand a very fundamental relationship between sample size and statistical uncertainty, called _de Moivre's equation_.  

(4) __It prepares you to do more.__  We've already talked about how [Bootstrapping usually, but not always, works].  If you decide to pursue further studies---in statistics, machine learning, computer science, economics, or any number of fields---eventually you will encounter the limits of the simple bootstrapping approach, and you'll need to rely on some math (or, at the very least, on the results of some other people's math).   This lesson will give you the proper foundation to understand what's going on if and when that happens.  Of course, my primary goal here is to teach you some practical data-science skills, not to run you through some mathematical boot camp for advanced studies in statistics.  Future-proofing your stats education is mainly just a nice side benefit.  

Let's dive right in.  


## The Central Limit Theorem {#central-limit-theorem}

### The normal distribution {-}

The first piece of the mathematical puzzle here is something called the normal distribution.  It helps us give a formal mathematical description of a phenomenon that you may have noticed yourself: that all sampling distributions basically look the same.

To dramatize this point, here's a cut-and-paste montage of nine different sampling distributions from eight different problems we've seen so far in these lessons:

```{r bunch-of-sampling-distributions, echo=FALSE, out.width="100%", fig.cap="Nine sampling distributions that all look pretty much the same."}
knitr::include_graphics("images/bunch_of_sampling_distributions.png")
```

I mean, just look at them.  It's like that famous first line of _Anna Karenina_, where all happy families are the same.  Without getting out your magnifying glass to inspect the scale of the $x$ axis in each plot, can you tell the difference between these sampling distributions?  I sure can't.  They all look the same!  More to the point, they all look like this:

<br>

```{r, echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("images/normal_cartoon.png")
```

<br>

Distributions that look like this have a name: we call them __normal distributions__ (or more colloquially, "bell curves," or more verbosely, ["Gaussian distributions"](https://en.wikipedia.org/wiki/List_of_things_named_after_Carl_Friedrich_Gauss)).  Normal distributions are "normal" in the same sense that blue jeans are normal, which is to say they're ubiquitous---as evidenced by the fact that all the sampling distributions in Figure \@ref(fig:bunch-of-sampling-distributions) look normal.  This is emphatically _not_ a coincidence, but rather a consequence of a very important result in mathematics called the Central Limit Theorem, or CLT for short.  We'll see a more precise statement of this theorem soon, but here's the basic idea.

> __Central Limit Theorem, simplified version__: sampling distributions based on averages from a large number of independent samples basically all look the same: like a normal distribution.

Since the normal distribution seems so... well... _central_ to all this business, let's try to understand it a bit more.  A normal distribution, written as $N(\mu, \sigma)$, can be described using two numbers: 

- the mean $\mu$, which determines where the peak of the distribution is centered.  
- the standard deviation $\sigma$, which determines how spread out the distribution is.  

The nine sampling distributions shown in Figure \@ref(fig:bunch-of-sampling-distributions) all have different means and standard deviations, but it's hard to tell because they're all shown on different scales/aspect ratios (i.e. different sets of $x$ and $y$ axes).  Below are three different normal distributions, all shown on the _same_ scale, so you can appreciate their variety.

<!-- ``` -->
<!-- curve(dnorm(x, 0, 1), n= 10001, from=-10, to = 10, -->
<!--       xlab='', ylab='', axes=FALSE, ylim=c(0, 0.8)) -->
<!-- curve(dnorm(x, 2, 3), n= 10001, add=TRUE, col='red') -->
<!-- curve(dnorm(x, -3, 0.5), n= 10001, add=TRUE, col='blue') -->
<!-- axis(1, at = seq(-10, 10, by=1), cex.axis=0.8) -->
<!-- ``` -->

```{r, echo=FALSE, out.width="100%", fig.align = 'center'}
knitr::include_graphics("images/normal_examples.png")
```

As you can see, a normal distribution can be centered anywhere (depending on $\mu$).  And it can be tall and skinny, short and broad, or anywhere in between (depending on $\sigma$).  If we think that some random quantity $X$ follows a normal distribution, we write $X \sim N(\mu, \sigma)$ as shorthand, where the tilde sign ($\sim$) means "is distributed as."  

Here are some useful facts about normal random variables---or more specifically, about the central areas under the curve, between the extremes of the left and right tail.  If $X$ has a $\mbox{N}(\mu, \sigma)$ distribution, then the chance that $X$ will be within $1 \sigma$ of its mean is about $68\%$; the chance that it will be within $2\sigma$ of its mean is about $95\%$; and the chance that it will be within $3\sigma$ of its mean is about $99.7\%$.  Said in equations:

\begin{eqnarray*}
P(\mu - 1\sigma < X < \mu + 1\sigma) &\approx& 0.68 \\
P(\mu - 2\sigma < X < \mu + 2\sigma) &\approx& 0.95 \\ 
P(\mu - 3\sigma < X < \mu + 3\sigma) &\approx& 0.997 \, .
\end{eqnarray*}

And said in a picture, with fastidiously accurate numbers:

<br>

```{r normal-6895, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap="A useful fact worth remembering about the normal distribution."}
knitr::include_graphics("images/normal6895.png")
```

<br>

### de Moivre's equation {-}

But our discussion of the Central Limit Theorem so far is incomplete.  Suppose we take a bunch of samples $X_1, \ldots, X_N$ from some wider population whose mean is $\mu$, and we calculate $\bar{X}_N$, the mean of the sample.  Can we say anything a bit more specific about the statistical fluctuations in $\bar{X}_N$, beyond the fact that those fluctuations will be approximately normal?  Specifically, can we say how spread out those fluctuations might be around the population mean of $\mu$?

It turns out that the answer is: yes, quite precisely.

This leads us to the second big piece of our mathematical puzzle: something called __de Moivre's equation__, also known as the "square root rule."^[To be clear, this is not the same thing as de Moivre's [formula for complex exponentials.](https://en.wikipedia.org/wiki/De_Moivre%27s_formula)]   The Central Limit Theorem tells us that, with enough data points, the distribution of an average looks approximately normal; de Moivre's equation tells us precisely how narrow or wide that normal distribution will be, as a function of two things:

(1) The variability of a single data point.  
(2) The number of data points you're averaging.  

This equation was discovered by a Swiss mathematician named [Abraham de Moivre](https://en.wikipedia.org/wiki/Abraham_de_Moivre) in 1730.  In my opinion, it represents one of the most under-rated triumphs of human reasoning in history.  Most educated people, for example, have heard of Einstein's equation: $e = mc^2$.  De Moivre's equation is just as profound; it represents an equally universally truth, and is equally useful for making accurate scientific predictions.  Yet very few people outside statistics and data science know it. 

De Moivre's equation concerns a situation we've approached before, using the bootstrap: one where we take an average of many samples, and we want to understand [how much statistical uncertainty we have about that average](#bootstrap-sample-mean).  Specifically, the equation establishes an inverse relationship between the variability of the sample average and the square root of the sample size.  It goes like this:

$$
\mbox{Variability of an average} = \frac{\mbox{Variability of a single data point}}{\sqrt{\mbox{Sample size}}}
$$

Data scientists usually use the Greek letter $\sigma$ (sigma) to denote the variability of a single data point, and the letter N to denote the sample size.  Also, you should remember our more technical term for the variability (or statistical fluctuations) of an average: the "standard error."  This terminology allows us to express de Moivre's equation a little more compactly:

$$
\mbox{Standard error of the mean} = \frac{\sigma}{\sqrt{N}} 
$$

This equation looks simple, but its consequences are profound.

First, de Moivre's equation places a fundamental limit on how fast we can learn about something subject to statistical fluctuations.  Naively, you might think that statistical uncertainty goes down in proportion to how much data you have, i.e. your sample size.  But de Moivre's equation tells us otherwise: it tells us that statistical uncertainty goes down in proportion to the __square root__ of your sample size.  So, for example:

- if you collect 4 times as much data as you had before, you're not 4 times as certain as you were before: de Moivre's equation says that your standard error will go down by a factor of $1/\sqrt{4} = 1/2$.  That is, you'll have half as much uncertainty as you had before.
- if you collect 100 times as much data as you had before, you're not 100 times as certain as you were before: de Moivre's equation says that your standard error will go down by a factor of $1/\sqrt{100} = 1/10$.  That is, you'll have 10% as much uncertainty as you had before.  


Second, de Moivre's equation also allows us to state a more precise version of the Central Limit Theorem, as follows.

```{theorem, name = "Central Limit Theorem, more precise version"}
Suppose we take $N$ independent samples from some wider population, and we compute the average of the samples, denoted $\bar{X}_N$.  Let $\mu$ be the mean of the population, and let $\sigma$ be the standard deviation of a single observation from the population.  If $N$ is sufficiently large, then the statistical fluctuations in $\bar{X}_N$ can be well approximated by a normal distribution, with mean $\mu$ and standard deviation $\sigma/\sqrt{N}$: 

$$
\bar{X}_N \approx N \left( \mu, \frac{\sigma}{\sqrt{N}} \right)
$$
```

<br>

Three notes here:  

(1) The Central Limit Theorem holds regardless of the shape of the original population distribution.^[OK, not _entirely_ regardless.  It is easy for mathematicians to find counterexamples, because that's what mathematicians do.  A more accurate statement is that the "basic" Central Limit Theorem holds whenever your population distribution has a finite mean and variance.  This covers the vast majority of all common data-science situations.]  In particular, the population itself _does not have to be normally distributed,_ and indeed can be crazily non-normal.  It doesn't matter.  As long as the sample size is large enough, the statistical fluctuations in the sample mean will look approximately normal.  We'll see examples of this below.  
(2) The requirement that $N$ be "sufficiently large" is what gives this lesson (and this whole approach to statistics) its name: __large-sample inference.__  The basic idea behind all this math is that, by studying what happens when we collect lots of data, we can learn some things that allow us to make inferential statements that hold __approximately__ across a wide variety of situations, even if we don't necessarily have lots of data ourselves.
(3) You might notice two remaining weasel words in this supposedly more precise version of the Central Limit Theorem: "sufficiently large" and "well approximated."  What, precisely, do those mean?  Alas, it's hard to say without going into some very detailed math, specifically a result called the [Berry--Esseen theorem](https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem).  But here's a rough guideline: if the population you're sampling from isn't too skewed or crazy looking, then 30 samples is generally "sufficiently large" for the approximation to be really quite good.  


### The CLT in action {-}

The Central Limit Theorem and de Moivre's equation can feel a bit abstract.  It's certainly not obvious or intuitive why either mathematical result is true.  In a course covering probability or mathematical statistics, you would actually _prove_ that they're true, but those kinds of mathematical proofs are not really our focus in these lessons.  

Instead, we're going to take a different approach.  In my opinion, the best way to make the Central Limit Theorem and de Moivre's equation feel concrete is to mess around with some Monte Carlo simulations until they both start to make sense.   [This web app](https://gallery.shinyapps.io/CLT_mean/) allows you to explore the CLT to your heart's content, and I highly recommend it!  But we'll look at two specific examples here.  


#### Example 1: FedEx packages {-}

In our first example, we'll try to reason our way through a deceptively simple question: what's the average weight of packages that a FedEx driver delivers in a single day, and how does that average fluctuate from truck to truck (or from day to day for a single truck)?  

To FedEx, understanding fluctuations in average package weight for a delivery truck is very much _not_ an academic question.  In fact, I once heard a really interesting conference presentation from a data scientist at FedEx who discussed this very issue.  Her reasoning was roughly the following:

- Every pound of package weight you're carrying around requires more fuel, so you need to make sure a given truck is adequately fueled to complete its delivery run.  
- But you don't want to _over-fuel_ a truck either.  Unnecessary fuel means you're carrying unnecessary weight, and therefore _burning_ more fuel than you need to.  
- So there's really an "ideal" amount of fuel to carry: not too much, but enough to cover typical day-to-day fluctuations in package weight.  You only save a small amount _per truck, per day_ by getting this right.  But over many trucks and many days, it can add up.  Considering that 3-5% of FedEx's roughly \$80 billion in revenue is spent on fuel in any given year, we're talking about real money here.^[You might ask, why doesn't FedEx just weigh each truck and fuel it accordingly?  Someone actually asked this question in the conference presentation I attended.  The answer was, basically, that time costs more than fuel.  Can you imagine how long it would take to line up every FedEx truck and weigh it as it leaves the warehouse?  It's far better, time-wise, to have a rule of thumb that covers expected statistical fluctuations in package weight.]  

With that in mind, let's now return to our question: how does the average weight of packages delivered by a single FedEx truck vary from one truck to the next?

We can think of a single truck's daily load of packages as a random sample from the population of all FedEx packages.  So the two relevant questions are: (1) What the does "the population of all FedEx packages" look like?  And (2) What is the "sample size," i.e. how many packages does a FedEx truck carry in a single day?  Since I don't work for FedEx, I'll have to make some assumptions.

As far as what the population looks like, here are some basic facts I remember from that presentation by the FedEx data scientist:

- the average weight of a FedEx package was 6.1 pounds
- the standard deviation in weight was 5.6 pounds
- the distribution of weights was _very_ skewed, where most packages were pretty light but a handful of them were very heavy.

Since I don't have access to FedEx's raw data, I simulated a notional population with 100,000 imaginary packages having these statistical properties.  You can find them in [fedex_sim.csv](data/fedex_sim.csv).  Go ahead and load this (simulated) data set into R studio.  (You'll also need `tidyverse` and `mosaic`.) 

```{r, echo=FALSE}
fedex_sim = read.csv('data/fedex_sim.csv')
```

```{r, message=FALSE}
library(tidyverse)
library(mosaic)
```

This data set has a single column called `weight`, notionally measured in pounds.  If we make a histogram of this `weight` variable, you'll see what I mean when I say that the distribution of individual package weights is very skewed:

```{r fedex-weight-pop, fig.align='center', fig.cap="A simulated distribution of FedEx package weights."}
ggplot(fedex_sim) + 
  geom_histogram(aes(x=weight), binwidth=0.5)
```

So let's treat this distribution as a decent stand-in for "the weight distribution of all FedEx packages."   While it's surely wrong in the particulars, it's probably pretty similar in shape to the _real_ distribution, which none of us actually know unless we work for FedEx.  And as you can see from `favstats`, the distribution matches the mean (6.1 pounds) and standard deviation (5.1 pounds) that I recall hearing:

```{r}
favstats(~weight, data=fedex_sim)
```

The next question is: how many packages per day does a typical FedEx truck deliver?  I'm not sure, but according to [some guy on the Internet who claims to have worked for a shipping company](https://www.quora.com/How-many-packages-does-a-FedEX-driver-deliver-in-a-day/answer/William-Travis-4?ch=10&share=7c86cb0f) (and who might also have [fought at the Alamo](https://en.wikipedia.org/wiki/William_B._Travis)), the answer is "it depends."  Our hero claims that a driver on an urban express route might deliver 300 packages, while a driver on a more rural route might deliver only 60.  These numbers sound reasonable to me, and frankly I wouldn't know any better, so let's run with them.  

We'll start on the low end, with 60 packages.  Let's say there are five different trucks in your general neighborhood today, each delivering 60 packages.  Each truck represents a sample of size 60 from the population, which we can simulate like this:

```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r, message=FALSE}
do(5)*mean(~weight, data=sample_n(fedex_sim, 60))
```

You might get the sense, even from just 5 samples, that the average package weight for a single truck is usually pretty close to the population average of 6.1 pounds.

But what about if we look across, say, 5000 trucks, which is about the number of FedEx trucks making deliveries in Texas on a single day?^[In 2020 FedEx claimed a fleet of 78,000 trucks.  Texas represents about 9% of the US population, and so probably about 9% of the FedEx trucks.  That's about 7000 trucks.  But probably not all are driving on any given day, so I'm using 5000 as a round, ballpark figure.]  Let's take 5000 different samples of size 60, representing 5000 different trucks, and make a histogram of each truck's mean package weight.

```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r, fedex-histogram60, out.width = "100%", fig.asp = 0.6, fig.cap = "The sampling distribution of the mean package weight for a FedEx truck, assuming it carries 60 packages.", message=FALSE}
# Monte Carlo simulation: 5000 samples of size n = 60
sim_n60 = do(5000)*mean(~weight, data=sample_n(fedex_sim, 60))

# histogram of the 5000 different means
ggplot(sim_n60) + 
  geom_histogram(aes(x=mean), binwidth=0.1)
```

Even though the _population distribution_ in Figure \@ref(fig:fedex-weight-pop) looks wildly non-normal, the _sampling distribution of the mean_ is really quite close to a normal distribution centered around the population mean of 6.1.  This is just as the Central Limit Theorem would predict. 

Moreover, de Moivre's equation tells us that the standard error of the mean of 60 packages is:

$$
\mbox{std. err.} = \frac{\sigma}{\sqrt{N}} = \frac{5.6}{\sqrt{60}} \approx 0.72
$$

This predicts how wide the histogram in Figure \@ref(fig:fedex-histogram60) should be.  So let's check! To dramatize how accurate de Moivre's equation is, let's superimpose a $N(6.1, 0.72)$ distribution on top of this histogram:

```{r, out.width = "100%", fig.asp = 0.6, echo=FALSE}
# histogram of the 5000 different means
mydens1 = function(x) 0.1*5000*dnorm(x, 6.1, 5.6/sqrt(60))

ggplot(sim_n60) + 
  geom_histogram(aes(x=mean), binwidth=0.1) + 
  stat_function(fun = mydens1, col='blue', size=2)
```

We've learned that the prediction of the Central Limit Theorem, together with de Moivre's equation, is pretty good, even with just 60 samples from a very non-normal population. 

Now what happens if we assume that a FedEx truck carries 300 packages?  We can simulate this by changing the sample size in our simulation, as in the code block below:  

```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r, message=FALSE}
# Monte Carlo simulation: 5000 samples of size n = 300
sim_n300 = do(5000)*mean(~weight, data=sample_n(fedex_sim, 300))
```

For this larger sample size de Moivre's equation predicts that the standard error of the sample mean should be 

$$
\mbox{std. err.} = \frac{\sigma}{\sqrt{N}} = \frac{5.6}{\sqrt{300}} \approx 0.32
$$

So let's plot our simulated sampling distribution, together with the prediction from de Moivre's equation of a $N(6.1, 0.32)$ distribution.  As you can see, the predicted normal distribution is near-exact fit to the true sampling distribution:

```{r, fedex-histogram300, out.width = "100%", fig.asp = 0.6, fig.cap = "The sampling distribution of the mean package weight for a FedEx truck, assuming it carries 300 packages, together with the prediction from the Central Limit Theorem and de Moivre's equation.", echo=FALSE, message=FALSE}
mydens2 = function(x) 0.1*5000*dnorm(x, 6.1, 5.6/sqrt(300))

ggplot(sim_n300) + 
  geom_histogram(aes(x=mean), binwidth=0.1) + 
  stat_function(fun = mydens2, col='red', size=2) 
```


Below you can see these two sampling distributions plotted on the same scale, allowing you to appreciate their differences in shape.  

```{r, echo=FALSE, message=FALSE, out.width="100%", fig.asp = 0.5}
D = rbind(mutate(sim_n60, sample_size = "60"), mutate(sim_n300, sample_size = "300"))

ggplot(D) + 
  geom_histogram(aes(x=mean, fill=sample_size), alpha=0.4, binwidth=0.1, position="identity") + 
  stat_function(fun = mydens1, col='blue', size=0.5) + 
  stat_function(fun = mydens2, col='red', size=0.5) + 
  labs(fill = "Sample size")
```

Since a sample of 300 has 5 times as much data as a sample of 60, de Moivre's equation says that the red histogram should be narrower than the blue histogram by a factor of $1/\sqrt{5} \approx 0.45$.  Just eyeballing the figure, this looks about right.  


#### Example 2: class GPA {-}

In our second example, we'll use the Central Limit Theorem to examine a question that I certainly think about from time to time: what kind of statistical fluctuations should we expect in the class GPA for a single section of a college statistics course?  

To begin, I grabbed some data on students' final grades from many, many sections of a course called STA 371 that I taught for a long time at the University of Texas.   These grades are on traditional plus/minus letter-grade scale, which I then [converted to GPA points](https://onestop.utexas.edu/student-records/grades/), where an A is 4.0, an A- is 3.67, a B+ is 3.33, and so on.  I also asked my colleagues for the grade distribution (although obviously not individual students' grades) from their sections, and I summarized all this information into a single table.  You can see the results in [grade_distribution.csv](data/grade_distribution.csv), which looks like this:


```{r, echo=FALSE, out.width="30%", out.extra='style="float:right; padding:10px"'}
grade_distribution = read.csv('data/grade_distribution.csv')
grade_distribution %>% knitr::kable() %>%
  kableExtra::kable_styling(full_width = F)
```

The column labeled `n` represents the total number of students who received that letter grade across all sections.  We can think of this as a population distribution of GPA for all students who take the course.  Here's a bar graph of this distribution:

```{r, grade-distribution-barplot, out.width = "80%", fig.asp = 0.6, fig.cap="The distribution of student grades across many, many sections of a single college statistics course, where 4.0=A, 3.67=A-, etc."}
ggplot(grade_distribution) + 
  geom_col(aes(x=factor(GPA), y = n)) + 
  labs(x="Student GPA")
```

This is obviously _nothing_ like a normal distribution.  It's highly skewed, and it can only take on a discrete set of values.  But as we'll soon see---and as the Central Limit Theorem would predict---it turns out that the average GPA for an individual section of this course _does_ look very close to normal.  

To give us something to sample from, I simulated a hypothetical population of 5000 students from this grade distribution.  You can find this simulated population in [grade_population.csv](data/grade_population.csv).  Go ahead and import this into RStudio.  Here are the first six lines of the file, representing the first six students in our population:  

```{r, echo=FALSE}
grade_population = read.csv('data/grade_population.csv')
```

```{r}
head(grade_population)
```

We can compute an overall population GPA for this course by taking the average of the `GPA` column, like this:

```{r}
mean(~GPA, data=grade_population)
```

As you can see, the overall GPA across lots and lots of sections is very close to 3.3, which is actually what our dean's office "suggests" that we, as instructors of individual courses, aim for.  

But what about the class GPA for a single section of, say, 45 students?   Obviously it won't be _exactly_ 3.3, but how close can we expect it to be, assuming that students in a specific section are a random sample of all students?  We can simulate a single section as follows, by randomly sampling 45 students from our entire "population":

```{r}
# Construct a random sample
single_section = sample_n(grade_population, size = 45)

# mean GPA for all 45 students in the simulated section
mean(~GPA, data=single_section)
```

In this particular simulated section, the class GPA was close to, but not exactly equal to, the "population" GPA of 3.3.  

So now let's repeat this process 10000 times to get a sampling distribution for the class GPA in a section of 45:

```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r, message=FALSE}
gpa_sim = do(10000)*mean(~GPA, data = sample_n(grade_population, size = 45))
```

Here's a histogram of this sampling distribution:  

```{r, gpa-histogram, out.width = "100%", fig.asp = 0.6, fig.cap = "The sampling distribution of course GPA in a section of 45 students.", message=FALSE}
ggplot(gpa_sim) + 
  geom_histogram(aes(x=mean), binwidth=0.02)
```

As the CLT would predict, this looks quite close to normal, despite the striking non-normality in Figure \@ref(fig:grade-distribution-barplot).  Moreover, the population standard deviation is...

```{r}
sd(~GPA, data=grade_population)
```

about 0.687.  Therefore, since our sample size is n=45, de Moivre's equation predicts that the standard error of this sampling distribution should be:

$$
\mbox{std. err.} = \frac{\sigma}{\sqrt{N}} = \frac{0.687}{\sqrt{45}} \approx 0.1
$$

So let's superimpose a normal distribution with mean 3.3 and standard deviation 0.1 on top of this histogram:

```{r, out.width = "100%", fig.asp = 0.6, echo=FALSE, message=FALSE}
mydens3 = function(x) 0.02*10000*dnorm(x, 3.3, 0.1)

ggplot(gpa_sim) + 
  geom_histogram(aes(x=mean), binwidth=0.02) + 
  stat_function(fun = mydens3, col='blue', size=2) 
```

The prediction of the Central Limit Theorem, together with de Moivre's equation, accords pretty closely with reality.  

__Take-home lesson.__  I was surprised when I put this example together and saw the sampling distribution in Figure \@ref(fig:gpa-histogram) for the first time.  It was wider than I (naively) expected.  I don't think this kind of "typical" variability from one section of a course to the next is widely appreciated by either faculty or students.  Moreover, I think ignorance of this variability has some important practical consequences.  For example, there are a few major teaching awards in my college for which faculty are eligible _only_ if their course GPA is _very_ close to 3.3---much closer than this sampling distribution would suggest is reasonable.  For another thing, every faculty member who teaches big undergraduate classes has it ingrained in their minds what the "expected" GPA for their course is.  But no one emphasizes to faculty how much natural variability to expect around this number.  Heck, I'm a statistics professor and I didn't know it until I actually ran the simulation!

Why does this matter?  Well, when I look back over the years, I see that the course GPA's in my own sections of this course are much more clustered around 3.3 than this distribution in Figure \@ref(fig:gpa-histogram) predicts that they should be.  And the only way this could happen is if we're actually punishing the very best students!  Here's why: on a practical level, the way most faculty arrive at a target GPA is by setting the curve on students' raw percentage grades in order to "curve them up" so that the course GPA comes close to 3.3.  In some years the curve is more aggressive, in other years less so.  But course GPA's that are _too_ tightly clustered around 3.3 are not just unwarranted, they're actively damaging.  For example, I _must_ have had at least one or two abnormally good sections over the years, i.e. sections that probably "should" have had a GPA of maybe 3.4-3.5.  These are sections drawn from the upper tail of Figure \@ref(fig:gpa-histogram).  But the students in these "better" sections didn't get the grades they deserved: because I didn't _appreciate_ that they were in abnormally good sections, I curved their grades less than I should have, because I was aiming for something spuriously close to 3.3.  (Of course, the opposite is also true; I must also have had some sections that probably "should" have had a course GPA of 3.1-3.2, and I curved their raw grades more than I should have.)   

Anyway, when I saw Figure \@ref(fig:gpa-histogram), I e-mailed the dean about it.  I'm not sure if anything will come of it, but the exercise was certainly instructive for me---and, I hope, for you.  


## Confidence intervals for a mean {#confidence-intervals-mean}

Let's now turn to the main question we're trying to answer in this lesson: how can we use the Central Limit Theorem, combined with de Moivre's equation, to get confidence interval without bootstrapping? 

You might remember the "confidence interval rule of thumb" from our lesson on [The bootstrap]: 

> __Confidence interval rule of thumb:__ a 95% confidence interval tends to be about two standard errors to either side of your best guess.  A 68% confidence interval tends to be about one standard error to either side of your best guess.

These numbers of 68% and 95% actually come from the normal distribution (recall Figure \@ref(fig:normal-6895)), and the underlying justification for this rule of thumb _is_ the Central Limit Theorem.  The logic here, roughly, is as follows.

- The Central Limit Theorem says that statistical fluctuations in the sample mean $\bar{X}_N$ can be described by a normal distribution, centered around the population mean $\mu$.    
- de Moivre's equation tells us the standard deviation of that normal distribution: $\sigma/\sqrt{n}$, where $\sigma$ is the standard deviation of a single data point, and $n$ is the sample size.  
- A normally distributed random variable is within 2 standard deviations of its mean 95% of the time.  Therefore, $\bar{X}_N$ should be within 2 standard errors of $\mu$ (the population mean) 95% of the time.  
- Thus if we quote the confidence interval $\bar{X}_N \pm 2 \cdot \sigma/\sqrt{n}$, we should capture $\mu$ in our interval 95% of the time.  

And that's basically it---a couple centuries' worth of math in a handful of bullet points.  Or even more succinctly: __just use de Moivre's equation and quote $\bar{X}_N \pm 2 \cdot \sigma/\sqrt{n}$ as your 95% confidence interval for the mean.__  The Central Limit implies that this confidence interval should be really close to the confidence interval you get from bootstrapping.  

The caveat, of course, is that such a confidence interval characterizes _only_ statistical uncertainty.  It doesn't have anything to say about non-statistical uncertainty, i.e. systematic biases in your measurement process.  So if your sample is biased, then all bets are off regarding whether your confidence interval contains the true value.  But this is true of pretty much any confidence interval, including those calculated by bootstrapping.  

With that caveat in mind, let's see some examples.  


### Example 1: sleep, revisited  {-}

```{r, echo=FALSE, message=FALSE}
NHANES_sleep = read.csv('data/NHANES_sleep.csv', header=T)
```

For our first example, let's revisit the [NHANES_sleep](data/NHANES_sleep.csv) data we saw in the lesson on bootstrapping.   Please import this data set into RStudio, and then load the `tidyverse` and `mosaic` libraries:

```{r, message=FALSE}
library(tidyverse)
library(mosaic)
```

You'll recall that, in our original analysis of this data, we looked at Americans' average number of sleep hours per night, based on the following distribution of results from the NHANES survey:  

```{r, eval=FALSE}
ggplot(NHANES_sleep) + 
  geom_histogram(aes(x = SleepHrsNight), binwidth=1)
```

```{r, echo=FALSE}
ggplot(NHANES_sleep) + 
  geom_histogram(aes(x = SleepHrsNight), binwidth=1) + 
  scale_x_continuous(breaks=1:13)
```

We took the mean of this data distribution, and we found that on average, Americans sleep 6.88 hours per night.  

```{r}
mean(~SleepHrsNight, data=NHANES_sleep)
```

Now let's turn to the question of statistical uncertainty.  How much should the sample mean of 1,991 people fluctuate around the _true_ population mean?  

de Moivre's equation makes a specific mathematical prediction: it says that the standard error of the sample mean is equal to the standard deviation of a single measurement ($\sigma$), divided by the square root of the sample size.  So as long as you know $N$ and can calculate $\sigma$ from the data, we can use de Moivre's equation to quantify our statistical uncertainty, all without ever running the bootstrap---which, of course, de Moivre couldn't feasibly do, since he sadly had never encountered a computer.  

Let's churn through the math.  Our sample size $N$ is...

```{r}
nrow(NHANES_sleep)
```

...1,991 survey respondents.  And we estimate that standard deviation $\sigma$ of the `SleepHrsNight` variable is:

```{r}
sd(~SleepHrsNight, data=NHANES_sleep)
```

... about 1.32 hours.  Therefore de Moivre's equation says that the standard error of our sample mean should be...

```{r}
1.32/sqrt(1991)
```

...about 0.0296 hours.

Remember, this number represents a specific mathematical prediction for the statistical uncertainty of the sample mean---that is, the typical error of the mean across many repeated samples.  Let's compare this prediction with the standard error we get when we bootstrap:  

```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r, message=FALSE}
boot_sleep = do(10000)*mean(~SleepHrsNight, data=resample(NHANES_sleep))

# calculate bootstrapped standard error
boot_sleep %>%
  summarize(std_err_sleep = sd(mean))
```

About 0.0295.  Clearly the answer from de Moivre's equation was really, really similar.  In fact, here's $\pm$ 1 bootstrap standard error to either side of the sample mean,  superimposed on the bootstrap sampling distribution:

```{r, echo=FALSE, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_sleep) + 
  geom_histogram(aes(x=mean)) + 
  geom_vline(xintercept=c(6.878955 - 0.0296, 6.878955 + 0.0296), color='red', size=2)
```

And here's $\pm$ 1 standard error calculated from de Moivre's equation, superimposed on the same sampling distribution:

```{r, echo=FALSE, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_sleep) + 
  geom_histogram(aes(x=mean)) + 
  geom_vline(xintercept=c(6.878955 - 0.0295, 6.878955 + 0.0295), color='blue', size=2)
```

I don't know about you but I definitely can't tell the difference.  

What about a 95% confidence interval?  Let's compare our bootstrapped confidence interval with what we get using de Moivre's equation---that is, going out two "de Moivre" standard errors from the sample mean.  Our bootstrapped confidence interval goes from about 6.82 to 6.94:

```{r}
confint(boot_sleep, level = 0.95)
```

And if we go use de Moivre's equation to go out two standard errors to either side of the sample mean of 6.88, we get a confidence interval of...

```{r}
6.88 - 2 * 1.32/sqrt(1991)
6.88 + 2 * 1.32/sqrt(1991)
```

... also 6.82 to 6.94.  Voila---a confidence interval that's basically identical to what you get when you bootstrap, except using math rather than computational muscle.  

Finally, let's remember our caveat: this confidence interval characterizes _only_ statistical uncertainty.  But in this case, since the NHANES study consists of a representative sample of all Americans, any biases in our measurement process are likely to be small.  It seems reasonable to assume that statistical uncertainty is what matters most here.  


### `t.test` shortcut {-}

In this particular case, it is both trivial and fast to compute a bootstrapped confidence interval.  However, I hope you think it's at least somewhat cool that de Moivre could have used his equation back in 1730 to tell you what bootstrap confidence interval you could expect to get 300 years later, with your fancy modern computer.  

However, it's also a little bit tedious to have to do these calculations "by hand," i.e. treating R as a calculator and manually type out the formula for de Moivre's equation, like this:

```{r}
6.88 - 2 * 1.32/sqrt(1991)
6.88 + 2 * 1.32/sqrt(1991)
```

Luckily, there's a shortcut, using a built-in R command called `t.test`.  It works basically just like `mean`, as follows:

```{r}
t.test(~SleepHrsNight, data=NHANES_sleep)
```

This command prints out a _bunch_ of extraneous information to the console.  You can safely ignore just about everything for now, except the part labeled "95% confidence interval," which is indeed the interval calculated using de Moivre's equation.

__Summary: inference for a mean.__  If you want to get a 95% confidence interval for a mean without bootstrapping, use `t.test`, and ignore everything in the output except for the part that says "confidence interval."  This is, in fact, the shortcut that most people use.  And now you understand the first row of Table \@ref(tab:translation-table).  This is often called the "t interval" for a mean.  


__Minor technical point.__  You might ask: why is the confidence interval calculated via `t.test` really similar, but not _exactly identical_, to the confidence interval we calculated by hand by going out 2 standard errors from the sample mean?  Here those two intervals are, out to 4 decimal places:  

- 2 standard errors by hand: (6.8208, 6.9392)
- using `t.test`: (6.8211, 6.9369)  

These two intervals are slightly different because going out 2 standard errors is just a rule of thumb that's easy to remember.  It gives you something really close, but not _exactly identical_, to a 95% confidence interval based on the Central Limit Theorem.  For a normal distribution, the exact number is closer to 1.96 (see Figure \@ref(fig:normal-6895)).  So instead of the rule of thumb based on 2 standard errors, `t.test` is using an exact figure, and is a bit narrower as a result.  

In fact, `t.test` is being even a little bit more clever.  It uses a number that's always pretty close to 2 but that depends on the sample size. For example, if you had only 10 samples, `t.test` would calculate a confidence interval by going out 2.26 standard errors from the sample mean.  If you had 30 samples, it would go out 2.04 standard errors.  Only if you had thousands of samples of more would it actually use 1.96, which is the mathematically "correct" number for a normal distribution.  
Is there any rhyme or reason to these numbers? Yes, there is---but the reasoning is very complicated.  It's based on something called the [t distribution](https://www.jmp.com/en_us/statistics-knowledge-portal/t-test/t-distribution.html) (hence the "t interval" and the function `t.test` in R).  There's a huge amount of detail here, but the short version is this:

- de Moivre's equation and the Central Limit Theorem assume that the population standard deviation $\sigma$ is known.  
- But $\sigma$ is almost never known.  We have to estimate it using the data---and this estimate, like all estimates, is subject to error.  
- When we use an _estimate_ of $\sigma$ rather than the true $\sigma$ in de Moivre's equation, it introduces extra uncertainty in our calculation.  
- The t distribution is like a version of the normal distribution that accounts in a precise mathematical way for this extra uncertainty.  

Lots of introductory statistics courses spend a whole lot of time on these details.  In these lessons, we're basically going to ignore them; in my opinion, they're just not that important until you reach a much more advanced level in your statistics education.  It's enough to understand that `t.test` is basically just computing a confidence interval based on the Central Limit Theorem and de Moivre's equation, with a minor correction that widens the confidence intervals to account for small sample sizes.  It's not important that you understand the precise details of how that "minor correction" is actually calculated.  


### Example 2: cheese {-}

Let's see a second example, using the [data on weekly cheese sales](data/kroger.csv) across 68 weeks at 11 different Kroger grocery stores that we examined back in the lesson on [Plots].  

```{r, echo=FALSE, message=FALSE}
kroger = read.csv('data/kroger.csv')
```

We'll start by filtering down this data set to consider only sales for the Dallas store: 

```{r}
kroger_dfw = filter(kroger, city == 'Dallas')
```

Here's a quick histogram of the data distribution for the `vol` variable, representing weekly sales of [American cheese](https://www.google.com/search?q=kraft+singles&hl=en&tbm=isch):

```{r, message=FALSE}
ggplot(kroger_dfw) + 
  geom_histogram(aes(x=vol))
```

This clearly isn't normally distributed, but having seen the examples of the previous section, that shouldn't faze us a bit in applying de Moivre's equation.


Let's calculate the mean weekly sales volume for the Dallas store across these 61 weeks, which we can think of as a good approximation to the long-run average of this store's "true" sales level.  This sample mean is...

```{r}
# mean for the sample...
mean(~vol, data=kroger_dfw)
```

... about 4356 packages of cheese per week.  

But what about statistical uncertainty?  Weekly sales of pretty much any consumer good are an inherently variable thing, even such a staple product as American cheese.  Maybe a couple weeks were a bit colder than average, depressing demand for patio fare like nachos.  Or maybe the Dallas area had a big power outage one week, implying that nobody could microwave any cheese.  These things happen.  And what if this particular store had a weekly target of 4500 packages?  Does this data set imply that they're _really, truly_ under-performing their target, or might they have been the victim of some unlucky weekly fluctuations?

With these statistical fluctuations in mind, let's use de Moivre's equation to calculate the standard error of this sample mean.  Our sample size is...

```{r}
nrow(kroger_dfw)
```

...61 weeks.  And our sample standard deviation is...

```{r}
sd(~vol, data=kroger_dfw)
```

... about 2354.  So de Moivre's equation says is that our standard error is $\sigma/\sqrt{n}$, or:

```{r}
2353.645/sqrt(61)
```

...about 300 packages of cheese.  Let's compare this with our bootstrapped standard error, which is...

```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r, message=FALSE}
boot_cheese = do(10000)*mean(~vol, data=resample(kroger_dfw))

# calculate bootstrapped standard error
boot_cheese %>%
  summarize(std_err_vol = sd(mean))
```

... about 300.  Again, that's nearly identical to the answer from de Moivre's equation.  For reference, here's $\pm$ 1 bootstrap standard error to either side of the sample mean, superimposed on the sampling distribution:  

```{r, echo=FALSE, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_cheese) + 
  geom_histogram(aes(x=mean)) + 
  geom_vline(xintercept=c(4356.508 - 301.3534, 4356.508 + 301.3534), color='red', size=2)
```

And here's $\pm$ 1 standard error calculated from de Moivre's equation, superimposed on the same sampling distribution:

```{r, echo=FALSE, message=FALSE, out.width="100%", fig.asp=0.7}
ggplot(boot_cheese) + 
  geom_histogram(aes(x=mean)) + 
  geom_vline(xintercept=c(4356.508 - 298.181, 4356.508 + 298.181), color='blue', size=2)
```

Again, pretty hard to tell apart.  No matter which standard error we use, it's totally plausible that the _real_ long run average sales figure for this store exceeds its hypothetical target of 4500 weekly units, and that the low sample mean just reflects natural variation.  This is a good example of where our statistical uncertainty comes not from sampling (like in a political poll), but rather from the intrinsic variability of some phenomenon---in this case, the economic and culinary phenomenon of how much cheese the people of Dallas happen to want to buy in any given week.  


What about a 95% confidence interval for the long-run average of this store's sales?  Let's compare our bootstrapped confidence interval with what we get using de Moivre's equation by going out "de Moivre" standard errors from the sample mean.  Our bootstrapped confidence interval is...

```{r}
confint(boot_cheese, level = 0.95)
```

... about 3780 to 4950.  And if we go use de Moivre's equation to go out two standard errors (that is, 2 times $2353.645/\sqrt{61}$) to either side of the sample mean of 4356.5, we get a confidence interval of...

```{r}
4356.5 - 2 * 2353.645/sqrt(61)
4356.5 + 2 * 2353.645/sqrt(61)
```

... about 3750 to 4960.  As with the previous example, our bootstrap confidence interval and our CLT-based confidence interval are very similar.  Moreover, the differences between the two confidence intervals are _much_ smaller (on the order of 10 packets of cheese) than than the statistical uncertainty associated with the sample mean itself (on the order of 1000 packets of cheese).  It basically doesn't matter which one you use.  

Finally, also remember that we can use `t.test` to compute this latter CLT-based confidence interval (a.k.a. "t-interval"), without taking the trouble to plug in numbers to de Moivre's equation ourselves:

```{r}
t.test(~vol, data=kroger_dfw)
```

Abraham de Moivre presumably never encountered American cheese.  I like to think that, being Swiss, he would have appreciated its similarity, at least in nacho-cheese form, to fondue.  (Although being _French_-Swiss, he might instead have found it [bizarre](https://www.youtube.com/watch?v=8xjfdGErgUc&ab_channel=BuzzFeedVideo)).  Either way, his equation is pretty good for describing statistical fluctuations in American cheese sales.  



## Beyond de Moivre's equation

### The basic recipe of large-sample inference {-}

In the previous section on the Central Limit Theorem, we focused exclusively on the sample mean.  We learned that, for large enough samples, the statistical fluctuations in the sample mean can be described by a normal distribution whose width is known quite precisely, via de Moivre's equation.  

It turns out that the sample mean isn't special: many other common data summaries also have approximately normal sampling distributions, regardless of whether the underlying population itself looks normal.  The upshot is that we can make approximate probability statements about most common estimators using the normal distribution, using the same basic logic from the section on [confidence intervals for a mean](#confidence-intervals-mean). This includes means, medians, proportions, differences of means, differences of proportions, standard deviations, correlations, regression coefficients, quantiles... it's quite a long list!  These estimators are all said to be _asymptotically normal_, assuming your data consists a random sample from some underlying population.


```{definition, label="asymptotic-normal", name = "Asymptotic normality"}
An estimator is said to be __asymptotically normal__ if its sampling distribution is approximately normal for large enough samples.  
```

<br>

For any asymptotically normal estimator $\hat{\theta}$, we can calculate a confidence interval using the same basic approach:

(1) Calculate the sample estimate $\hat{\theta}$.  
(2) Calculate the standard error of your estimate, $\mbox{se}(\hat{\theta})$, from the appropriate formula (like de Moivre's equation).  
(3) Form the confidence interval like this:  

```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("images/CLT.png")
```

In fact, the only thing that changes from one summary statistic to another is the formula for the standard error.  For a sample mean, we calculate the standard error using de Moivre's equation: $\mbox{se}(\bar{X}_N) = \sigma/\sqrt{n}$.  For any statistic other than the sample mean, we simply require a different formula for the standard error.  Luckily, over the last couple of centuries, _many_ of these "de-Moivre-like" formulas have been worked out for a huge range of situations, and they're programmed into every common statistical software package (including R). 

I'll issue a pre-emptive warning here: the details of these formulas can get a little tedious!  In practice, this traditional way of doing statistical inference involves a lot of time spent looking up these formulas, and a lot of "plug and chug," where you plug numbers into a formula and then just do the darn arithmetic.  (If you took AP Statistics in high school, this likely describes your experience for much of the course.)  I'll quote a few of these formulas briefly below, just to give you a sense for what they're like---but there's no need to memorize them.  Then we'll go through several examples that illustrate how these formulas compare with bootstrapped standard errors.  (Spoiler alert: they give nearly identical results.)  My advice is to not focus on the micro-details of the formulas.  Instead, focus on how this basic three-step recipe is applied each time. 


#### Sample mean {-}

Suppose we collect a random sample of data points $x_1, \ldots, x_N$ from some wider population with mean $\mu$, and we calculate their sample mean $\bar{x}$.  The standard error of this sample mean is:

$$
\mathrm{se}(\bar{x}) = \frac{\sigma}{\sqrt{N}}
$$

where $N$ is the sample size and $\sigma$ is the standard deviation, typically estimated by taking the sample standard deviation of $x_1, \ldots, x_N$.  This is __de Moivre's equation__, as we've already covered.  


#### Sample proportion {-}

Suppose we collect observe a sample of $N$ independent "yes-or-no" outcomes, all with common but unknown probability $p$ of being "yes."  We let $\hat{p}$ denote the proportion of "yes" outcomes in our sample.  The standard error of this sample proportion is:

$$
\mathrm{se}(\hat{p}) = \sqrt{\frac{\hat{p} \cdot (1-\hat{p})}{N}}
$$


#### The Pythagorean theorem  {-}

Suppose we collect independent random samples from two groups, and that we calculate the same summary statistic in each group, denoted $\hat{\theta}_1$ and $\hat{\theta}_2$, respectively.  We are interested in the difference between the two groups, $\hat{\theta}_1 - \hat{\theta}_2$.  The standard error of this difference can be calculated using the Pythagorean Theorem for standard errors:   

$$
\left[ \mathrm{se}(\hat{\theta}_1 - \hat{\theta}_2) \right]^2 =  \left[ \mathrm{se}(\hat{\theta}_1) \right]^2 +  \left[ \mathrm{se}(\hat{\theta}_2) \right]^2 
$$

Intuitively, it's like $\mathrm{se}(\hat{\theta}_1 - \hat{\theta}_2)$ is the hypotenuse of a right triangle, while $\mathrm{se}(\hat{\theta}_1)$ and $\mathrm{se}(\hat{\theta}_2)$ are the two legs of that triangle.  If you have that picture in your head, then it makes sense to use the Pythagorean theorem to calculate the "length of the hypotenuse," i.e. the standard error of the differences between the two groups.^[This analogy can be made precise using probability theory, but that's beyond the scope of these lessons.]  For example:  

- difference of means, $\bar{x}_1 - \bar{x}_2$.  According to the Pythagorean Theorem for standard errors, the standard error of this difference is:  
$$
\mathrm{se}(\bar{x}_1 - \bar{x}_2) = \sqrt{\frac{\sigma_1^2}{N_1} + \frac{\sigma^2_2}{N_2}} 
$$

- difference of proportions, $\hat{p}_1 - \hat{p}_2$.  According to the Pythagorean Theorem for standard errors, the standard error of this difference is:  
$$
\mathrm{se}(\hat{p}_1 - \hat{p}_2) = \sqrt{\frac{\hat{p_1} \cdot (1 - \hat{p_1})}{N_1}  +  \frac{\hat{p_2} \cdot (1 - \hat{p_2})}{N_2}} 
$$



<!-- #### Regression coefficients {-} -->

<!-- Suppose we fit a regression model by ordinary least squares. -->

<!-- \begin{eqnarray} -->
<!-- \mbox{se}(\hat{\beta_0}) &=& \hat{s} \cdot \sqrt{ \frac{1}{n}  + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2}  }  \\ -->
<!-- \mbox{se}(\hat{\beta_1}) &=& \hat{s} \cdot \sqrt{ \frac{1}{\sum_{i=1}^n (x_i - \bar{x})^2} } \, . -->
<!-- \end{eqnarray} -->

<!-- where $\hat{s}$ is the standard deviation of the model residuals and $\bar{x}$ is the sample mean of our predictor ($x$) variable. -->



### Example 1: sample proportion {-}

Let's now see some examples of these formulas in action.  

First, please download and import the data in [beer.csv](data/beer.csv), which contains a sample of beers (and ciders) available for purchase in Texas.  The first six lines of the file look like this: 

```{r, echo=FALSE}
beer = read.csv("data/beer.csv")
```

```{r}
head(beer)
```

Here we'll focus on the `IPA` column, which indicates whether a beer is a particular style called an [India Pale Ale](https://www.bonappetit.com/story/ipa-beer-styles).  Our question is simple: what proportion of unique beers available for sale in Texas are IPA's?

```{r}
prop(~IPA, data=beer)
```

A little over 20%.  What about a confidence interval?  Let's follow the three steps outlined in "[The basic recipe of large-sample inference]".  

__Step 1: summary statistics.__  The sample proportion of IPA's is $\hat{p} = 0.206$, based on a sample of size N=344.  

__Step 2: standard error.__ Our standard error formula for a proportion is:

$$
\mathrm{se}(\hat{p}) = \sqrt{\frac{\hat{p} \cdot (1 - \hat{p})}{N}} = \sqrt{\frac{0.206 \cdot (1 - 0.206)}{344}} = 0.022
$$


__Step 3: confidence interval.__ For a confidence interval of 95%, we go out two standard errors from our sample estimate of 0.206.  Therefore our 95% confidence interval is:  

$$
\begin{aligned}
&\phantom{\approx} 0.206 \pm 2 \cdot 0.022 \\
&\approx (0.16, \, 0.25)
\end{aligned}
$$

Our conclusion is that between 16% and 25% of unique beers on the wider market in Texas are IPA's.  

Let's compare this with a bootstrapped confidence interval, which we can calculate easily using the following code block:


```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r}
boot_IPAprop = do(10000)*prop(~IPA, data=resample(beer))
confint(boot_IPAprop)
```

This is nearly identical to what we get using the Central Limit Theorem.


#### `prop.test` shortcut {-}

There's also a handy shortcut to get a large-sample confidence interval for a proportion, using R's built-in `prop.test` function.  It works just like `prop`, `mean`, and `t.test`.  Again, you can safely ignore everything except the part labeled "confidence interval":  

```{r}
prop.test(~IPA, data=beer)
```

This avoids having to do any tedious standard-error calculations by hand. 


### Example 2: difference of means {-}

Let's now focus on the column labeled `ped`, which stands for price elasticity of demand.  This represents consumer price sensitivity for each beer in the sample. We'll compare price elasticity for IPA's versus non-IPA's:

```{r}
ggplot(beer) + 
  geom_boxplot(aes(x=IPA, y=ped))
```

It looks like IPA's have considerably more negative price elasticities, on average, implying great consumer price sensitivity for IPA's than non-IPA's.  We can calculate the average value of `ped` in each group as follows:

```{r}
mean(ped ~ IPA, data=beer)
```


One possible explanation for this difference is that consumers consider IPA's to be highly substitutable for one another.  So if one particular IPA seems expensive, consumers just buy a different one.  

Let's get a confidence interval for the difference between these two means, applying our three-step recipe.  Here we'll need to use both de Moivre's equation as well as the "Pythagorean theorem" for the difference in sample means between two groups.

__Step 1: summary statistics.__  To apply the correct formulas, we need the sample mean, sample standard deviation, and sample size in each group.  We'll get this via group/pipe/summarize:

```{r}
beer %>%
  group_by(IPA) %>%
  summarize(xbar = mean(ped),
            sigma = sd(ped),
            n = n())
```

We can also get R to calculate the difference between the two means, which is about 1.31% (i.e. more negative elasticity for IPA's):  

```{r}
diffmean(ped ~ IPA, data=beer)
```


__Step 2: standard error.__  Our formula for the standard error of the difference between two sample means $\bar{x}_1$ and $\bar{x}_2$ is:  

$$
\mathrm{se}(\bar{x}_1 - \bar{x}_2) = \sqrt{\frac{\sigma_1^2}{N_1} + \frac{\sigma^2_2}{N_2}} = \sqrt{\frac{0.606^2}{273} + \frac{0.623^2}{71}} = 0.082
$$

__Step 3: confidence interval.__  For a 95% interval, we go out standard errors from our difference in sample means, which is 1.31:  

$$
\begin{aligned}
&\phantom{\approx} 1.31  \pm 2 \cdot 0.082 \\
&\approx (1.15, \, 1.47)
\end{aligned}
$$

So there you have it.  What a slog.  This kind of "by-hand" calculation gets so tedious so quickly that I almost never bother---but I do want to show you the details for these examples so you see what's going on "under the hood."  It's the kind of thing that makes you sympathize with those scientists in years past who lacked computers and had to do _everything_ by hand.  

But since I _do_ have a computer, I typically just use `t.test`, which applies the correct formulas automatically to calculate a large-sample confidence interval for the difference of two means:

```{r}
t.test(ped ~ IPA, data=beer)
```

The output labeled "95% confidence interval" goes from 1.14 to 1.47, just like our "by hand" calculation.  This interval represents our statistical uncertainty about the difference in average elasticity for IPA's versus non-IPA's.  This confidence interval doesn't contain zero, so we have a statistically significant difference in elasticity: it appears IPA-beer consumers are more price-sensitive than non-IPA beer consumers.

You may notice that `t.test` quotes a p-value.  This p-value is calculated under the null hypothesis that the true difference in `ped` for IPA's and non-IPA's were zero.  The fact that this p-value is tiny means we're very confident of a non-zero difference in elasticity between the two groups.  

We can compare our large-sample confidence interval to what we get when we bootstrap the difference in means:

```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r}
boot_IPA_ped = do(10000)*diffmean(ped ~ IPA, data=resample(beer))
confint(boot_IPA_ped, level=0.95)
```

Other than the difference in sign, this confidence interval is basically identical to the large-sample confidence interval calculated using `t.test`.   (The difference in sign arises because `t.test` calculates the IPA mean minus the non-IPA mean, whereas our bootstrapped confidence interval is for the non-IPA mean minus the IPA mean.)  


### Example 3: difference of proportions {-}

For our third example, we'll consider the case of a difference in proportions between two groups.  Please download and import the data set in [recid.csv](data/recid.csv).  This contains data on the recividism status for a random sample of 432 prisoners released from Maryland state prisons:

```{r, echo=FALSE}
recid = read.csv('data/recid.csv')
```

```{r}
head(recid)
```

The `arrest` column indicates whether someone was re-arrested within 52 weeks after their release from prison.  We'll compare recidivism rates stratified by the `emp1` variable, which indicates whether someone was employed within 1 week after their release:  

```{r}
prop(arrest ~ emp1, data=recid, success=0)
```

By adding `success = 0` as an optional flag, we're telling `prop` which outcome corresponds to a "success" (1 = arrest, or 0 = no arrest).  We choose `success=0` because it would be odd to interpret re-arrest as a "success."  So here, it looks like 81.7% of those who were employed within a week of release avoided re-arrest, versus 72.3% for those who weren't.

To get a confidence interval for the population-wide difference in proportions, let's use `prop.test`, which applies the appropriate standard-error formulas:

```{r}
prop.test(arrest ~ emp1, data=recid, success=0)
```

This tells us that the 95% confidence interval for the difference in re-arrest rates was somewhere between -21.1% and +2.4%.  So there's a large amount of uncertainty here.  `prop.test` also provides a p-value under the null hypothesis that the true difference in proportions is 0.  This p-value is 0.1714, indicating that the difference is not statistically significant at any commonly accepted level (e.g. 0.05).  


Let's compare our large-sample confidence interval with a bootstrapped confidence interval:

```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r}
boot_recid = do(10000)*diffprop(arrest ~ emp1, data=resample(recid))
confint(boot_recid)
```

This isn't identical to our large-sample confidence interval from `prop.test`, but it's very similar: about -20% to about +2%.  Both confidence intervals are telling us basically the same thing: recidivism is probably lower among released prisoners who get jobs quickly, but there's a lot of uncertainty about how much lower.  

This, incidentally, provides another example of the difference between statistical and practical significance.  The difference between recidivism rates in the two groups here is not statistically significant at the 5% level.  But that doesn't prove that there's no difference!  Here, it just means the sample size is too small to rule out statistical fluctuations (particularly among prisoners who _did_ find work within a week of release, of whom there are only 60).  

It's worth re-iterating a lesson we learned in our discussion of [Statistical vs. practical significance], back when we learned about the bootstrap:  

> Confidence intervals are _almost always_ more useful than claims about statistical significance.  My advice to you is to entertain the full range of values encompassed by the confidence interval, rather than jumping straight to the needlessly binary question of whether that confidence interval contains zero.  

Think of it this way.  Can the data rule out a 0% difference in overall recividism rates between the employed and non-employed groups?  No.  But can the data rule out, for example, an 18% difference?  Also no---and 18% would be a really large difference, in practical terms.  (If a program to get released prisoners into work quickly reduced recidivism by 18%, that would be an incredible policy triumph and a major win for society.)  So here, we have an effect that is _not_ statistically significant, but that might actually be quite large.  Of course, it might also be small, or zero, or even negative (which admittedly seems implausible, at least to me).  We're just not sure, in light of the data.  We probably need a bigger study.  


### Example 4: regression model {-}


```{r, echo=FALSE, message=FALSE}
heartrate = read.csv("data/heartrate.csv", header=TRUE)
```

For our last example, let's return to the data in [heartrate.csv](data/heartrate.csv).  We looked at this data in our unit on [Fitting equations](#basic_regression), when we looked at the relationship between someone's maximum heart rate and their age:  

```{r, fig.asp = 0.6, out.width = "100%"}
ggplot(heartrate) + 
  geom_point(aes(x=age, y=hrmax))
```

We then used regression to learn that, on average, maximum heart rate declines about 0.7 beats per minute with each additional year of age:  

```{r}
lm_heart = lm(hrmax ~ age, data=heartrate)
coef(lm_heart)
```

In R, it's incredibly simple to get a confidence interval for regression coefficients based on the Central Limit Theorem: just call `confint` on your fitted model object, like this!^[You may notice that I never even gave you the formulas for the standard error of regression coefficients.  You can [look it up here](https://www.statisticshowto.com/find-standard-error-regression-slope/) if you really want to!]

```{r}
confint(lm_heart)
```

So our 95% large-sample confidence interval for the slope on age is about (-0.80, -0.58).  Let's compare this with a bootstrap confidence interval:


```{r, echo=FALSE, message=FALSE}
options(`mosaic:parallelMessage` = FALSE)
```

```{r}
boot_hr = do(10000)*lm(hrmax ~ age, data=resample(heartrate))
confint(boot_hr)
```

If you inspect the row labeled `age`, you'll see that our bootstrap confidence interval for this coefficient is incredibly close to our large-sample confidence interval: roughly (-0.81, -0.58).  


### Summary {-}  

We've now covered every row of Table \@ref(tab:translation-table).  Let's summarize what we've learned:  

- Most common statistical summaries (means, proportions, regression coefficients, etc) have sampling distributions that look approximately normal for large enough samples.  
- This is a consequence of a deeper principle called the Central Limit Theorem.
- We can use the Central Limit Theorem to calculate "large-sample" confidence intervals for these summaries.  We expect these confidence intervals to be pretty accurate unless the sample size is very small.  
- This whole approach requires some fairly tedious math, to calculate the standard error of the sampling distribution for a specific kind of estimator.    
- Luckily R has built-in routines for computing large-sample confidence intervals that do the tedious math for you.  
- In nearly every case, the large-sample confidence interval calculated using R's built-in routines looks very similar to the corresponding confidence interval calculated via bootstrapping.  

Here's a table that covers these built-in R functions, together with their bootstrapping equivalents.  In the table, `y` refers to an outcome variable, `group` to some grouping factor (like IPA's vs. non-IPA's), and `x` refers to a predictor variable in a regression model.  

| Situation | Built-in R function | Bootstrapping equivalent | 
|:------|:----------|:-------------|
| Estimating a mean       | `t.test(~y, data=D)`      | `do(10000)*mean(~y, data=resample(D))` |
| Estimating a proportion | `prop.test(~y, data=D)`      | `do(10000)*prop(~y, data=resample(D))` | 
| Comparing two means     | `t.test(y~group, data=D)`      | `do(10000)*mean(y~group, data=resample(D))` |
| Comparing two proportions     | `prop.test(y~group, data=D)`      | `do(10000)*prop(y~group, data=resample(D))` |
| Regression model     | `lm(y~x, data=D)`, then `confint`     | `do(10000)*lm(y~x, data=resample(D))` |

## Study questions: large-sample inference {-}  

(1) etc. 
(2) etc. 

<!--chapter:end:11_large_sample_inference.Rmd-->

# (PART\*) Part III: Models {-}

# Experiments



<!--chapter:end:12_experiments.Rmd-->

# Observational studies



<!--chapter:end:13_observational_studies.Rmd-->

# Grouped data



<!--chapter:end:14_grouped_data.Rmd-->

# Regression  



<!--chapter:end:15_multiple_regression.Rmd-->

# Prediction



<!--chapter:end:16_prediction.Rmd-->

# Probability models 



<!--chapter:end:17_probability_models.Rmd-->

`r if (knitr:::is_html_output()) '
# References {-}
'`

<!--chapter:end:99-references.Rmd-->

